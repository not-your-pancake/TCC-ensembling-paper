{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88462a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_bandarban = pd.read_csv('../../1980-2024-dataset/rangamati_historical_weather_1980_2024.csv')\n",
    "df_bandarban = df_bandarban.drop('district', axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc3ed58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df_bandarban.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Correlation test rangamati')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92fcf850",
   "metadata": {},
   "source": [
    "# temperature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca86df",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232dbc45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef72ebc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3407ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for temperature:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'RÂ² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "all_importances = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    all_importances.append(rf_model_kf.feature_importances_)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# 3. Calculate the average across all folds\n",
    "avg_importance = np.mean(all_importances, axis=0)\n",
    "std_importance = np.std(all_importances, axis=0) # Bonus: see how stable they are!\n",
    "\n",
    "# 4. Create the final DataFrame\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Average_Importance': avg_importance,\n",
    "    'Std_Dev': std_importance\n",
    "})\n",
    "\n",
    "# 5. Sort by the average\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nRandom Forest Feature Importances (Averaged over all folds):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe9a873",
   "metadata": {},
   "source": [
    "## rf-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd01a374",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = fourier_cols + lag_cols + rolling_cols\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396f34ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 5  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mae')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2da5f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# --- STEP 1: Selective Features for LSTM ---\n",
    "# We only give the LSTM the most important \"weather\" features to reduce noise\n",
    "# lstm_feature_cols = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'temp_lag_1',\n",
    "#     'temp_roll_7', # \n",
    "#     'temp_lag_2', # \n",
    "#     'temp_lag_3', # \n",
    "#     'atm_roll_7', # \n",
    "#     'atm_lag_2', # ~ 1.84 r2\n",
    "#     'atmospheric_pressure', # r2 -> ~ 1.15\n",
    "#     'atm_lag_1', # 89.78/90.78 ~ -1\n",
    "#     'day_of_year_sin_3', # 90.84/91.71 ~ -0.86\n",
    "#     'atm_lag_3', # 90.79/91.60 ~-0.80 [better]\n",
    "#     # 'day_of_year_cos_1', # 90.69/91.87 ~-1.18\n",
    "#     # 'day_of_year_sin_1' # 90.61/91.81 ~-1.20\n",
    "# ]\n",
    "\n",
    "lstm_feature_cols = FEATURES\n",
    "\n",
    "X_train_slim = X_train[lstm_feature_cols]\n",
    "X_test_slim = X_test[lstm_feature_cols]\n",
    "\n",
    "scaler_slim = StandardScaler()\n",
    "X_train_scaled = scaler_slim.fit_transform(X_train_slim)\n",
    "X_test_scaled = scaler_slim.transform(X_test_slim)\n",
    "\n",
    "# --- STEP 2: Create Sequences ---\n",
    "window_size = 7 # Try a full week\n",
    "X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "\n",
    "# --- STEP 3: Optimized LSTM ---\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "    LSTM(100, activation='tanh', return_sequences=True), # Return sequences for deeper learning\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2) \n",
    "])\n",
    "\n",
    "# Use a slightly slower learning rate to find the pattern\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber') # Huber loss is great for weather outliers\n",
    "lstm_model.fit(X_train_seq, y_train_res_seq, epochs=60, batch_size=64, verbose=0)\n",
    "\n",
    "# 1. Align the Test Data (Skip the first 7 days used for the window)\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# 2. Get the \"Base\" predictions from your Random Forest\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# 3. Get the \"Corrections\" from the LSTM\n",
    "# X_test_seq was created during your sequence step\n",
    "lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# 4. Combine them: Base + Correction\n",
    "final_uv = rf_t_pred + lstm_corrections[:, 0]\n",
    "\n",
    "rf_lstm_r2 = r2_score(y_test_aligned, final_uv)\n",
    "rf_lstm_mse = mean_squared_error(y_test_aligned, final_uv)\n",
    "rf_lstm_rmse = np.sqrt(rf_lstm_mse)\n",
    "\n",
    "# 5. Output the New Results\n",
    "print(\"--- HYBRID MODEL PERFORMANCE ---\")\n",
    "print(f\"Final temperature R2: {rf_lstm_r2:.4f}\")\n",
    "print(f\"Final temperature MSE: {rf_lstm_mse:.4f}\")\n",
    "print(f\"Final temperature RMSE: {rf_lstm_mse:.4f}\")\n",
    "# final r2 90.90/89.87 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68cb2740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Assuming X and y are your full datasets before the train/test split\n",
    "# X_full, y_full, rf_model, create_sequences need to be defined in your workspace\n",
    "\n",
    "TEMP = 'temperature(degree C)'\n",
    "X_full = df[FEATURES]\n",
    "y_full = df[TEMP]\n",
    "\n",
    "fold = 1\n",
    "lstmRf_hybrid_r2_scores = []\n",
    "lstmRf_hybrid_mse_scores = []\n",
    "lstmRf_hybrid_rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "\n",
    "    print(f\"--- Processing Fold {fold} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train_cv, X_test_cv = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    # --- STEP 1: Random Forest Base Model (Required for Residuals) ---\n",
    "\n",
    "    # Training the RF on the current fold's training set\n",
    "    rf_model.fit(X_train_cv, y_train_cv)\n",
    "    train_residuals = y_train_cv - rf_model.predict(X_train_cv)\n",
    "    \n",
    "    # --- STEP 2: Preprocessing for LSTM ---\n",
    "    # lstm_feature_cols = [\n",
    "    #     'minimum_temperature(degree C)',\n",
    "    # 'temp_lag_1',\n",
    "    # 'temp_roll_7', # \n",
    "    # 'temp_lag_2', # \n",
    "    # 'temp_lag_3', # \n",
    "    # 'atm_roll_7', # \n",
    "    # 'atm_lag_2', # ~ 1.84 r2\n",
    "    # 'atmospheric_pressure', # r2 -> ~ 1.15\n",
    "    # 'atm_lag_1', # 89.78/90.78 ~ -1\n",
    "    # 'day_of_year_sin_3', # 90.84/91.71 ~ -0.86\n",
    "    # 'atm_lag_3', # 90.79/91.60 ~-0.80 [better]\n",
    "    # ]\n",
    "\n",
    "    lstm_feature_cols = FEATURES\n",
    "\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_cv[lstm_feature_cols])\n",
    "    X_test_scaled = scaler.transform(X_test_cv[lstm_feature_cols])\n",
    "    \n",
    "    # --- STEP 3: Create Sequences ---\n",
    "    window_size = 5\n",
    "    X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals.values, window_size)\n",
    "    # We pass zeros for y_test as we only need the X sequences for prediction\n",
    "    X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "    \n",
    "    # --- STEP 4: Train LSTM ---\n",
    "    # Re-initialize the model each fold to avoid weight leakage\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "        LSTM(100, activation='tanh', return_sequences=True),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber')\n",
    "    lstm_model.fit(X_train_seq, y_train_res_seq, epochs=30, batch_size=64, verbose=0)\n",
    "    \n",
    "    # --- STEP 5: Hybrid Prediction & Evaluation ---\n",
    "    # Align target data (drop first 'window_size' rows)\n",
    "    y_test_aligned = y_test_cv.iloc[window_size:]\n",
    "    rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n",
    "    \n",
    "    lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "    lstmRf_hybrid_prediction = rf_base_pred + lstm_corrections[:, 0] # Adjust index if target is multi-output\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_mse_kf = mean_squared_error(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_rmse_kf = np.sqrt(lstmRf_hybrid_mse_kf)\n",
    "\n",
    "    lstmRf_hybrid_r2_scores.append(lstmRf_hybrid_r2_kf)\n",
    "    lstmRf_hybrid_mse_scores.append(lstmRf_hybrid_mse_kf)\n",
    "    lstmRf_hybrid_rmse_scores.append(lstmRf_hybrid_rmse_kf)\n",
    "\n",
    "    print(f\"Fold {fold} R2: {lstmRf_hybrid_r2_kf:.4f}\")\n",
    "    print(f\"Fold {fold} MSE: {lstmRf_hybrid_mse_kf:.4f}\")\n",
    "    print(f\"Fold {fold} RMSE: {lstmRf_hybrid_rmse_kf:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "rf_lstm_r2_cv = np.mean(lstmRf_hybrid_r2_scores)\n",
    "rf_lstm_mse_cv = np.mean(lstmRf_hybrid_mse_scores)\n",
    "rf_lstm_rmse_cv = np.mean(lstmRf_hybrid_rmse_scores)\n",
    "\n",
    "print(\"\\n--- FINAL CROSS-VALIDATION RESULTS ---\")\n",
    "print(f\"Mean R2: {rf_lstm_r2_cv:.4f} (+/- {np.std(lstmRf_hybrid_r2_scores):.4f})\")\n",
    "print(f\"Mean mse: {rf_lstm_mse_cv:.4f} (+/- {np.std(lstmRf_hybrid_mse_scores):.4f})\")\n",
    "print(f\"Mean rmse: {rf_lstm_rmse_cv:.4f} (+/- {np.std(lstmRf_hybrid_rmse_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcb534",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43111cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e66529",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ac4a64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "'''\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "xgb_model = {}\n",
    "y_preds_xgb = {}\n",
    "rmses_xgb = {}\n",
    "r2s_xgb = {}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Results for temperature:\")\n",
    "print(f'Mean Squared Error: {mse_xgb:.4f}')\n",
    "print(f'RMSE: {rmse_xgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_xgb:.4f}')\n",
    "\n",
    "\n",
    "# K-Fold cross-validation for XGBoost\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_xgb = []\n",
    "r2_list_xgb = [] # Added to track R2 across all folds\n",
    "mse_list_xgb = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        xgb_model_kf = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_xgb = xgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        all_importances.append(xgb_model_kf.feature_importances_)\n",
    "\n",
    "\n",
    "        mse_kf_xgb = mean_squared_error(y_test_kf, y_pred_kf_xgb)\n",
    "        rmse_kf_xgb = np.sqrt(mse_kf_xgb)\n",
    "        r2_kf_xgb = r2_score(y_test_kf, y_pred_kf_xgb)\n",
    "\n",
    "        mse_list_xgb.append(mse_kf_xgb)\n",
    "        rmse_list_xgb.append(rmse_kf_xgb)\n",
    "        r2_list_xgb.append(r2_kf_xgb)\n",
    "\n",
    "\n",
    "        average_rmse_xgb = np.mean(rmse_list_xgb)\n",
    "        average_r2_xgb = np.mean(r2_list_xgb)\n",
    "        average_mse_xgb = np.mean(mse_kf_xgb)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_xgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_xgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_xgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_xgb}\")\n",
    "\n",
    "# 3. Calculate the average across all folds\n",
    "avg_importance = np.mean(all_importances, axis=0)\n",
    "std_importance = np.std(all_importances, axis=0) # Bonus: see how stable they are!\n",
    "\n",
    "# 4. Create the final DataFrame\n",
    "feature_importance_df_rf = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Average_Importance': avg_importance,\n",
    "    'Std_Dev': std_importance\n",
    "})\n",
    "\n",
    "# 5. Sort by the average\n",
    "feature_importance_df_xgb = feature_importance_df_rf.sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "\n",
    "diff = (r2_xgb - average_r2_xgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(feature_importance_df_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd26bba0",
   "metadata": {},
   "source": [
    "## light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e10707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9154d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c28b5d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "''''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "lgb_model = {}\n",
    "y_preds_lgb = {}\n",
    "rmses_lgb = {}\n",
    "r2s_lgb = {}\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "        # n_estimators=800,\n",
    "        # learning_rate=0.01,\n",
    "        # max_depth=8,\n",
    "        # subsample=0.8,\n",
    "        # colsample_bytree=0.8,\n",
    "        # random_state=42,\n",
    "        # verbosity=-1\n",
    "\n",
    "        n_estimators=300,        # Reduced to prevent memorization as UV r 4k dataset\n",
    "        learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "        max_depth=6,             # Shallow trees are better for 4k rows\n",
    "        num_leaves=20,           # Controls complexity\n",
    "        min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "        subsample=0.7,           # More aggressive sampling for better generalization\n",
    "        colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "                              # Clean console\n",
    "    )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_lgb = np.sqrt(mse_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Results for temp:\")\n",
    "print(f'Mean Squared Error: {mse_lgb:.4f}')\n",
    "print(f'RMSE: {rmse_lgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_lgb:.4f}')\n",
    "\n",
    "\n",
    "    # --- 6) 5-fold CV R^2 ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "rmse_list_lgb = []\n",
    "r2_list_lgb = []\n",
    "mse_list_lgb = []\n",
    "\n",
    "\n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        lgb_model_kf = lgb.LGBMRegressor(\n",
    "            \n",
    "            n_estimators=300,        # Reduced to prevent memorization\n",
    "            learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "            max_depth=6,             # Shallow trees are better for 4k rows\n",
    "            num_leaves=20,           # Controls complexity\n",
    "            min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "            subsample=0.7,           # More aggressive sampling for better generalization\n",
    "            colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "            random_state=42,\n",
    "            verbosity=-1             # Clean console\n",
    "        )\n",
    "\n",
    "        lgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_lgb = lgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        all_importances.append(lgb_model_kf.feature_importances_)\n",
    "\n",
    "\n",
    "        mse_kf_lgb = mean_squared_error(y_test_kf, y_pred_kf_lgb)\n",
    "        rmse_kf_lgb = np.sqrt(mse_kf_lgb)\n",
    "        r2_kf_lgb = r2_score(y_test_kf, y_pred_kf_lgb)\n",
    "        \n",
    "        mse_list_lgb.append(mse_kf_lgb)\n",
    "        rmse_list_lgb.append(rmse_kf_lgb)\n",
    "        r2_list_lgb.append(r2_kf_lgb)\n",
    "\n",
    "        average_rmse_lgb = np.mean(rmse_list_lgb)\n",
    "        average_r2_lgb = np.mean(r2_list_lgb)\n",
    "        average_mse_lgb = np.mean(mse_kf_lgb)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_lgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_lgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_lgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lgb}\")\n",
    "\n",
    "# 3. Calculate the average across all folds\n",
    "avg_importance = np.mean(all_importances, axis=0)\n",
    "std_importance = np.std(all_importances, axis=0) # Bonus: see how stable they are!\n",
    "\n",
    "# 4. Create the final DataFrame\n",
    "feature_importance_df_lgb = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Average_Importance': avg_importance,\n",
    "    'Std_Dev': std_importance\n",
    "})\n",
    "\n",
    "# 5. Sort by the average\n",
    "feature_importance_df_lgb = feature_importance_df_lgb.sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "\n",
    "diff = (r2_lgb - average_r2_lgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(feature_importance_df_lgb)\n",
    "\n",
    "##### year wise analysis #####\n",
    "# 1980 to 2024 -> ~ 0.69 (44 years)\n",
    "# 2014 to 2024 -> ~ 0.82 (10 years)\n",
    "# 2017 to 2024 -> ~ 0.042 (7 years) **** yey ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
    "# 2018 to 2024 -> ~ 0.78 (6 years)\n",
    "# 2019 to 2024 -> ~ 1.15 (5 yrs)\n",
    "# 2021 to 2024 -> ~ 2.33 (3 yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "547d329b",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2daa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c32609",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640611ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "''''\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "cat_model = {}\n",
    "y_preds_cat = {}\n",
    "rmses_cat = {}\n",
    "r2s_cat = {}\n",
    "\n",
    "# loss_function='RMSE' is standard for regression\n",
    "cat_model = CatBoostRegressor(\n",
    "        iterations=800,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "        bootstrap_type='Bayesian',\n",
    "        bagging_temperature=1,\n",
    "        random_strength=1,\n",
    "        loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "mse_cat = mean_squared_error(y_test, y_pred)\n",
    "rmse_cat = np.sqrt(mse_cat)\n",
    "r2_cat = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"\\nLightGBM Results for temp :\")\n",
    "print(f'Mean Squared Error: {mse_cat:.4f}')\n",
    "print(f'RMSE: {rmse_cat:.4f}')\n",
    "print(f'RÂ² Score: {r2_cat:.4f}')\n",
    "\n",
    "\n",
    "    # 3. 5-Fold Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_cat = []\n",
    "r2_list_cat = []\n",
    "mse_list_cat = []\n",
    "    \n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        cat_model_kf = CatBoostRegressor(\n",
    "            iterations=800,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "            bootstrap_type='Bayesian',\n",
    "            bagging_temperature=1,\n",
    "            random_strength=1,\n",
    "            loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        cat_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_cat = cat_model_kf.predict(X_test_kf)\n",
    "\n",
    "\n",
    "        all_importances.append(cat_model_kf.feature_importances_)\n",
    "\n",
    "\n",
    "\n",
    "        mse_kf_cat = mean_squared_error(y_test_kf, y_pred_kf_cat)\n",
    "        rmse_kf_cat = np.sqrt(mse_kf_cat)\n",
    "        r2_kf_cat = r2_score(y_test_kf, y_pred_kf_cat)\n",
    "\n",
    "        mse_list_cat.append(mse_kf_cat)\n",
    "        rmse_list_cat.append(rmse_kf_cat)\n",
    "        r2_list_cat.append(r2_kf_cat)\n",
    "\n",
    "        average_rmse_cat = np.mean(rmse_list_cat)\n",
    "        average_r2_cat = np.mean(r2_list_cat)\n",
    "        average_mse_cat = np.mean(mse_list_cat)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_cat:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_cat:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_cat}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_cat}\")\n",
    "\n",
    "# 3. Calculate the average across all folds\n",
    "avg_importance = np.mean(all_importances, axis=0)\n",
    "std_importance = np.std(all_importances, axis=0) # Bonus: see how stable they are!\n",
    "\n",
    "# 4. Create the final DataFrame\n",
    "feature_importance_df_cat = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Average_Importance': avg_importance,\n",
    "    'Std_Dev': std_importance\n",
    "})\n",
    "\n",
    "# 5. Sort by the average\n",
    "feature_importance_df_cat = feature_importance_df_cat.sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "\n",
    "diff = (r2_cat - average_r2_cat)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nCatBoost Feature Importances:\")\n",
    "print(feature_importance_df_cat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1118c550",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712b9646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17df9faf",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "accd8617",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create an instance with specific parameters\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15,          # Wait 15 epochs for improvement before stopping\n",
    "    restore_best_weights=True  # Very important: keeps the best version of your model\n",
    ")\n",
    "\n",
    "# 1. Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for GRU: (samples, time_steps, features)\n",
    "# Here we use time_steps=1. If you want sequences, you'd need a sliding window function.\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split data (matching your non-shuffle 80/20 split)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train initial model\n",
    "gru_model = build_gru((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_gru = mean_squared_error(y_test_unscaled, y_pred)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_unscaled, y_pred)\n",
    "\n",
    "print(f\"\\nGRU Results for temperature :\")\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'RMSE: {rmse_gru:.4f}')\n",
    "print(f'RÂ² Score: {r2_gru:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild/Reset model for each fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_gru = np.mean(r2_list_gru)\n",
    "average_mse_gru = np.mean(mse_list_gru)\n",
    "average_rmse_gru = np.mean(rmse_list_gru)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_gru:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_gru:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_gru:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_gru}\")\n",
    "\n",
    "diff = (r2_gru - np.mean(r2_list_gru))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f8f3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def calculate_permutation_importance_gru(model, X_val, y_val, scaler_y):\n",
    "    \"\"\"\n",
    "    Calculates importance for 3D GRU inputs by shuffling the feature dimension.\n",
    "    \"\"\"\n",
    "    # Baseline prediction\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_inv = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_inv, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importances = []\n",
    "    num_features = X_val.shape[2] # Number of columns in X\n",
    "    \n",
    "    for i in range(num_features):\n",
    "        # Work on a copy to avoid corrupting the original data\n",
    "        X_permuted = X_val.copy()\n",
    "        \n",
    "        # Shuffle the i-th feature across all samples\n",
    "        # Note: we shuffle the sample order for that specific feature\n",
    "        shuffled_feature = X_permuted[:, :, i]\n",
    "        np.random.shuffle(shuffled_feature)\n",
    "        X_permuted[:, :, i] = shuffled_feature\n",
    "        \n",
    "        # Predict and calculate new error\n",
    "        shuffled_preds = model.predict(X_permuted, verbose=0)\n",
    "        shuffled_mse = mean_squared_error(y_val_inv, scaler_y.inverse_transform(shuffled_preds))\n",
    "        \n",
    "        # Importance is the delta in MSE\n",
    "        importances.append(max(0, shuffled_mse - baseline_mse))\n",
    "        \n",
    "    return np.array(importances)\n",
    "\n",
    "# --- Cross-Validation Loop ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "all_fold_importances = [] # Store importances from each fold\n",
    "\n",
    "for fold, (train_index, test_index) in enumerate(tscv.split(X_reshaped)):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild and train model for this fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Calculate Standard Metrics\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf, verbose=0)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "    # --- Permutation Importance for this fold ---\n",
    "    print(f\"Processing Fold {fold+1} Importance...\")\n",
    "    fold_importance = calculate_permutation_importance_gru(gru_kf, X_test_kf, y_test_kf, scaler_y)\n",
    "    all_fold_importances.append(fold_importance)\n",
    "\n",
    "# --- Aggregate Results ---\n",
    "avg_importance = np.mean(all_fold_importances, axis=0)\n",
    "std_importance = np.std(all_fold_importances, axis=0)\n",
    "\n",
    "# Create Final Importance DataFrame\n",
    "# Assuming 'X' is your original dataframe to get feature names\n",
    "feature_importance_gru = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Average_Importance': avg_importance,\n",
    "    'Std_Dev': std_importance\n",
    "}).sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "# Final Reporting\n",
    "print(\"\\n--- GRU Final Results ---\")\n",
    "print(f\"Average RMSE from CV: {np.mean(rmse_list_gru):.4f}\")\n",
    "print(f\"Average RÂ² from CV: {np.mean(r2_list_gru):.4f}\")\n",
    "print(\"\\nGRU Permutation Feature Importances (Averaged):\")\n",
    "print(feature_importance_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468229c8",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dff6a2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb32e1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e0f3379",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# --- 1. Scaling and Reshaping (Fix for image_d61d81.png) ---\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Scale 2D data first\n",
    "X_scaled = scaler_X.fit_transform(X) \n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Now reshape to 3D for LSTM\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "\n",
    "# Split matching your CatBoost logic (shuffle=False)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
    "    return model\n",
    "\n",
    "\n",
    "# Initial Training\n",
    "lstm_model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_actual, y_pred_lstm)\n",
    "\n",
    "print(f\"\\nLSTM Results for temp :\")\n",
    "print(f'Mean Squared Error: {mse_lstm:.4f}')    \n",
    "print(f'RMSE: {rmse_lstm:.4f}')\n",
    "print(f'RÂ² Score: {r2_lstm:.4f}')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# --- 2. Permutation Logic ---\n",
    "def get_permutation_importance_fold(model, X_val, y_val, scaler_y):\n",
    "    # Baseline\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    # Fix for image_d6383f: Ensure scaler_y is used only after fit_transform is called\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val.reshape(-1, 1))\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds.reshape(-1, 1)))\n",
    "    \n",
    "    fold_imps = []\n",
    "    for i in range(X_val.shape[2]):\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        \n",
    "        perm_preds = model.predict(X_permuted, verbose=0)\n",
    "        perm_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(perm_preds.reshape(-1, 1)))\n",
    "        fold_imps.append(max(0, perm_mse - baseline_mse))\n",
    "    return fold_imps\n",
    "\n",
    "# --- 3. CV Loop (Fix for image_d462c2.png) ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_lstm, r2_list_lstm, mse_list_lstm, all_fold_importances = [], [], [], []\n",
    "\n",
    "print(\"Starting Cross-Validation...\")\n",
    "\n",
    "# Important: split on X_reshaped (the NumPy array) to avoid KeyErrors\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    # NumPy indexing works perfectly here\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    lstm_kf = build_lstm((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    lstm_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Prediction and Evaluation\n",
    "    y_pred_kf_scaled = lstm_kf.predict(X_test_kf, verbose=0)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled.reshape(-1, 1))\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf.reshape(-1, 1))\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_lstm.append(np.sqrt(mse_kf))\n",
    "    mse_list_lstm.append(mse_kf)\n",
    "    r2_list_lstm.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "    \n",
    "    fold_importance = get_permutation_importance_fold(lstm_kf, X_test_kf, y_test_kf, scaler_y)\n",
    "    all_fold_importances.append(fold_importance)\n",
    "\n",
    "# --- 4. Aggregates ---\n",
    "lstm_importance_df = pd.DataFrame({\n",
    "    'Feature': FEATURES,\n",
    "    'Average_Importance': np.mean(all_fold_importances, axis=0),\n",
    "    'Std_Dev': np.std(all_fold_importances, axis=0)\n",
    "}).sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "average_r2_lstm = np.mean(r2_list_lstm)\n",
    "average_mse_lstm = np.mean(mse_list_lstm)\n",
    "average_rmse_lstm = np.mean(rmse_list_lstm)\n",
    " \n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from LSTM CV: {average_rmse_lstm:.4f}\")\n",
    "print(f\"Average RÂ² from LSTM CV: { average_r2_lstm:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_lstm:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lstm}\")\n",
    "\n",
    "\n",
    "print(\"\\nTop Importances:\")\n",
    "print(lstm_importance_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66afd1f2",
   "metadata": {},
   "source": [
    "## ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7c22e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a329665b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45702a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X_ann = StandardScaler()\n",
    "scaler_y_ann = StandardScaler()\n",
    "\n",
    "X_scaled_ann = scaler_X_ann.fit_transform(X)\n",
    "y_scaled_ann = scaler_y_ann.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split matching your CatBoost logic (80/20, shuffle=False)\n",
    "split_idx = int(len(X_scaled_ann) * 0.8)\n",
    "X_train_ann, X_test_ann = X_scaled_ann[:split_idx], X_scaled_ann[split_idx:]\n",
    "y_train_ann, y_test_ann = y_scaled_ann[:split_idx], y_scaled_ann[split_idx:]\n",
    "\n",
    "# build model \n",
    "\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Linear output for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "ann_model = build_ann(X_train_ann.shape[1])\n",
    "ann_model.fit(X_train_ann, y_train_ann, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled_ann = ann_model.predict(X_test_ann)\n",
    "y_pred_ann = scaler_y_ann.inverse_transform(y_pred_scaled_ann)\n",
    "y_test_actual = scaler_y_ann.inverse_transform(y_test_ann)\n",
    "\n",
    "mse_ann = mean_squared_error(y_test_actual, y_pred_ann)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "r2_ann = r2_score(y_test_actual, y_pred_ann)\n",
    "\n",
    "print(f\"\\nANN Results for temperature :\")\n",
    "print(f'Mean Squared Error: {mse_ann:.4f}')\n",
    "print(f'RMSE: {rmse_ann:.4f}')\n",
    "print(f'RÂ² Score: {r2_ann:.4f}')\n",
    "\n",
    "# CV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_ann = []\n",
    "r2_list_ann = []\n",
    "mse_list_ann = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled_ann):\n",
    "    X_train_kf, X_test_kf = X_scaled_ann[train_index], X_scaled_ann[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled_ann[train_index], y_scaled_ann[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    ann_kf = build_ann(X_train_kf.shape[1])\n",
    "    ann_kf.fit(X_train_kf, y_train_kf, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = ann_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y_ann.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y_ann.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_ann.append(np.sqrt(mse_kf))\n",
    "    mse_list_ann.append(mse_kf)\n",
    "    r2_list_ann.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_ann = np.mean(r2_list_ann)\n",
    "average_mse_ann = np.mean(mse_list_ann)\n",
    "average_rmse_ann = np.mean(rmse_list_ann)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from ANN CV: {average_rmse_ann:.4f}\")\n",
    "print(f\"Average RÂ² from ANN CV: {average_r2_ann:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_ann:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_ann}\")\n",
    "\n",
    "# importance\n",
    "def calculate_ann_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    for i in range(X_val.shape[1]): # Iterate through 2D features\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        importance_results.append(max(0, permuted_mse - baseline_mse))\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance_results}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "ann_importance_df = calculate_ann_permutation_importance(ann_model, X_test_ann, y_test_ann, scaler_y_ann, FEATURES)\n",
    "\n",
    "diff = (r2_ann - np.mean(r2_list_ann))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nANN Permutation Feature Importances:\")\n",
    "print(ann_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c39b0a9",
   "metadata": {},
   "source": [
    "## cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434cf0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point', 'atmospheric_pressure','max_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'feels_lag_{lag}'] = df['max_temperature(degree C)'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'atm_lag_{lag}', f'feels_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['feels_roll_7'] = df['max_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'atm_roll_7', 'feels_roll_7']\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879e34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES =  fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6863786f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23708d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# 1. Prepare 3D Data (Samples, Time Steps, Features)\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_hybrid(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. CNN Stage: Extracts spatial/local patterns from the window\n",
    "        # Reducing filters to 32 is often better for ~4k rows to prevent noise capture\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100,1)),\n",
    "        BatchNormalization(), # Stabilizes learning and speeds up convergence\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2), # Standard regularization\n",
    "\n",
    "        # 2. LSTM Stage: Learns temporal dependencies\n",
    "        # tanh is the standard and most stable activation for LSTM\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        # 3. Fully Connected Stage\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1) # Output for UV prediction\n",
    "    ])\n",
    "    \n",
    "    # Using a slightly lower learning rate (0.0005) helps with smaller datasets\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Data\n",
    "window = 30\n",
    "data_values = df['temperature(degree C)'].values.reshape(-1, 1)\n",
    "X, y = create_sequences(data_values, window)\n",
    "\n",
    "# --- BASE PERFORMANCE ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "model = build_hybrid((window, 1))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, y_pred)\n",
    "base_rmse = np.sqrt(base_mse)\n",
    "base_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Base Results: R2: {base_r2:.4f}, MSE: {base_mse:.4f}, RMSE: {base_rmse:.4f}\")\n",
    "\n",
    "# --- 5-FOLD CROSS VALIDATION ---\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_r2, cv_mse, cv_rmse = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    m = build_hybrid((window, 1))\n",
    "    m.fit(X[train_idx], y[train_idx], epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    p = m.predict(X[test_idx])\n",
    "    cv_r2.append(r2_score(y[test_idx], p))\n",
    "    cv_mse.append(mean_squared_error(y[test_idx], p))\n",
    "    cv_rmse.append(np.sqrt(cv_mse[-1]))\n",
    "\n",
    "\n",
    "cnn_lstm_r2_cv = np.mean(cv_r2)\n",
    "cnn_lstm_mse_cv = np.mean(cv_mse)\n",
    "cnn_lstm_rmse_cv = np.mean(cv_rmse)\n",
    "\n",
    "print(f\"5-Fold CV Average: R2: {cnn_lstm_r2_cv:.4f}, MSE: {cnn_lstm_mse_cv:.4f}, RMSE: {cnn_lstm_rmse_cv:.4f}\")\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1777d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# from sklearn.metrics import mean_squared_error, r2_score\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "# from keras.optimizers import Adam\n",
    "\n",
    "# # 1. DATA PREPARATION (Multivariate)\n",
    "# window = 30\n",
    "# # Combine all features and the target into one array\n",
    "# data_all = df[FEATURES + ['temperature(degree C)']].values\n",
    "\n",
    "# def create_sequences_multivariate(data, window_size):\n",
    "#     X, y = [], []\n",
    "#     for i in range(len(data) - window_size):\n",
    "#         # All columns except the last one are features\n",
    "#         X.append(data[i:(i + window_size), :-1]) \n",
    "#         # The last column is the target (temperature)\n",
    "#         y.append(data[i + window_size, -1]) \n",
    "#     return np.array(X), np.array(y)\n",
    "\n",
    "# X, y = create_sequences_multivariate(data_all, window)\n",
    "\n",
    "# # Split into Train and Test\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# # 2. UPDATED HYBRID MODEL\n",
    "# def build_hybrid_multivariate(input_shape):\n",
    "#     model = Sequential([\n",
    "#         # input_shape will be (30, 18)\n",
    "#         Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "#         BatchNormalization(),\n",
    "#         MaxPooling1D(pool_size=2),\n",
    "#         Dropout(0.2),\n",
    "\n",
    "#         LSTM(64, activation='tanh', return_sequences=False), \n",
    "#         Dropout(0.2),\n",
    "\n",
    "#         Dense(32, activation='relu'),\n",
    "#         Dropout(0.1),\n",
    "#         Dense(1)\n",
    "#     ])\n",
    "    \n",
    "#     optimizer = Adam(learning_rate=0.0005)\n",
    "#     model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "#     return model\n",
    "\n",
    "# # Initialize and train\n",
    "# model = build_hybrid_multivariate((window, len(FEATURES)))\n",
    "# model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# # 3. PERMUTATION IMPORTANCE WITH STD DEV\n",
    "# def get_permutation_importance(model, X_val, y_val, feature_names):\n",
    "#     base_preds = model.predict(X_val, verbose=0)\n",
    "#     base_rmse = np.sqrt(mean_squared_error(y_val, base_preds))\n",
    "    \n",
    "#     importance_results = []\n",
    "\n",
    "#     for i, col_name in enumerate(feature_names):\n",
    "#         scores = []\n",
    "#         # Run 5 times to get Standard Deviation\n",
    "#         for run in range(5):\n",
    "#             X_shuffled = X_val.copy()\n",
    "#             # Shuffle only the specific feature across all samples and time steps\n",
    "#             # We shuffle the 'depth' slice corresponding to feature i\n",
    "#             shuffled_feature = np.random.permutation(X_shuffled[:, :, i].flatten())\n",
    "#             X_shuffled[:, :, i] = shuffled_feature.reshape(X_shuffled.shape[0], X_shuffled.shape[1])\n",
    "            \n",
    "#             shuff_preds = model.predict(X_shuffled, verbose=0)\n",
    "#             shuff_rmse = np.sqrt(mean_squared_error(y_val, shuff_preds))\n",
    "#             scores.append(shuff_rmse - base_rmse)\n",
    "        \n",
    "#         importance_results.append({\n",
    "#             'Feature': col_name,\n",
    "#             'Average_Importance': np.mean(scores),\n",
    "#             'Std_Dev': np.std(scores)\n",
    "#         })\n",
    "#         print(f\"Computed importance for: {col_name}\")\n",
    "\n",
    "#     return pd.DataFrame(importance_results).sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "# # Run the importance check\n",
    "# importance_df_hybrid = get_permutation_importance(model, X_test, y_test, FEATURES)\n",
    "\n",
    "# print(\"\\n--- Hybrid Model Feature Importance (Averaged) ---\")\n",
    "# print(importance_df_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a463637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  # Added for normalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# 1. SCALING AND DATA PREPARATION\n",
    "window = 30\n",
    "# Initialize scalers for features (X) and target (y)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Scale features and target separately\n",
    "scaled_features = scaler_X.fit_transform(df[FEATURES])\n",
    "scaled_target = scaler_y.fit_transform(df[['temperature(degree C)']])\n",
    "\n",
    "# Combine scaled data back for sequence creation\n",
    "data_all_scaled = np.hstack((scaled_features, scaled_target))\n",
    "\n",
    "def create_sequences_multivariate(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size), :-1]) \n",
    "        y.append(data[i + window_size, -1]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences_multivariate(data_all_scaled, window)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 2. UPDATED HYBRID MODEL (No changes needed to architecture)\n",
    "def build_hybrid_multivariate(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_hybrid_multivariate((window, len(FEATURES)))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# 3. PERFORMANCE EVALUATION (With Inverse Scaling)\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "# Convert predictions and actuals back to original Celsius values\n",
    "y_pred_rescaled = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "mse_final = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "r2_final = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "\n",
    "print(f\"\\nRescaled Results: R2: {r2_final:.4f}, RMSE: {np.sqrt(mse_final):.4f}\")\n",
    "\n",
    "# 4. PERMUTATION IMPORTANCE (Standard logic remains)\n",
    "def get_permutation_importance(model, X_val, y_val, feature_names):\n",
    "    base_preds = model.predict(X_val, verbose=0)\n",
    "    base_rmse = np.sqrt(mean_squared_error(y_val, base_preds))\n",
    "    importance_results = []\n",
    "\n",
    "    for i, col_name in enumerate(feature_names):\n",
    "        scores = []\n",
    "        for run in range(5):\n",
    "            X_shuffled = X_val.copy()\n",
    "            shuffled_feature = np.random.permutation(X_shuffled[:, :, i].flatten())\n",
    "            X_shuffled[:, :, i] = shuffled_feature.reshape(X_shuffled.shape[0], X_shuffled.shape[1])\n",
    "            shuff_preds = model.predict(X_shuffled, verbose=0)\n",
    "            shuff_rmse = np.sqrt(mean_squared_error(y_val, shuff_preds))\n",
    "            scores.append(shuff_rmse - base_rmse)\n",
    "        \n",
    "        importance_results.append({\n",
    "            'Feature': col_name,\n",
    "            'Average_Importance': np.mean(scores),\n",
    "            'Std_Dev': np.std(scores)\n",
    "        })\n",
    "        print(f\"Computed importance for: {col_name}\")\n",
    "\n",
    "    return pd.DataFrame(importance_results).sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "importance_df_hybrid = get_permutation_importance(model, X_test, y_test, FEATURES)\n",
    "print(\"\\n--- Hybrid Model Feature Importance ---\")\n",
    "print(importance_df_hybrid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc6e1d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def predict_next_7_days_scaled(model, df, features, scaler_X, scaler_y, window_size=30):\n",
    "    # 1. Get the last window of data and scale it\n",
    "    # We use the already fitted scaler_X\n",
    "    last_raw_data = df[features].tail(window_size).values\n",
    "    current_batch = scaler_X.transform(last_raw_data) \n",
    "    current_batch = current_batch.reshape((1, window_size, len(features)))\n",
    "    \n",
    "    future_predictions_scaled = []\n",
    "    \n",
    "    # 2. Recursive Prediction Loop\n",
    "    for i in range(7):\n",
    "        # Predict (this gives the negative/scaled number)\n",
    "        next_pred_scaled = model.predict(current_batch, verbose=0)[0, 0]\n",
    "        future_predictions_scaled.append(next_pred_scaled)\n",
    "        \n",
    "        # 3. Update the window\n",
    "        # Create a new row for the next step\n",
    "        # Note: For a true forecast, we'd need to update lags/fourier manually.\n",
    "        # For this block, we shift the window and carry over the last feature state.\n",
    "        new_row = current_batch[:, -1:, :].copy()\n",
    "        \n",
    "        # Shift: Remove oldest day, append the newest \"predicted\" state\n",
    "        next_step = np.append(current_batch[:, 1:, :], new_row, axis=1)\n",
    "        current_batch = next_step\n",
    "\n",
    "    # 4. INVERSE TRANSFORM: The magic step that fixes the \"lol\" values\n",
    "    future_predictions_scaled = np.array(future_predictions_scaled).reshape(-1, 1)\n",
    "    final_temperatures = scaler_y.inverse_transform(future_predictions_scaled)\n",
    "    \n",
    "    # 5. Create the DataFrame\n",
    "    last_date = pd.to_datetime(df['date']).max()\n",
    "    forecast_dates = pd.date_range(start=last_date + pd.Timedelta(days=1), periods=7)\n",
    "    \n",
    "    forecast_df = pd.DataFrame({\n",
    "        'Date': forecast_dates,\n",
    "        'Predicted_Temperature(C)': final_temperatures.flatten()\n",
    "    })\n",
    "    \n",
    "    return forecast_df\n",
    "\n",
    "# --- EXECUTION ---\n",
    "# Make sure model, scaler_X, and scaler_y are defined from your previous part 2 block\n",
    "forecast_results = predict_next_7_days_scaled(model, df, FEATURES, scaler_X, scaler_y)\n",
    "\n",
    "print(\"\\n--- Final Corrected Temperature Forecast ---\")\n",
    "print(forecast_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265050d3",
   "metadata": {},
   "source": [
    "## model chart temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c0f4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance chart \n",
    "MODEL_NAMES = [\"Random Forest\", \"RF-LSTM hybrid\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"GRU\", \"LSTM\", \"ANN\", \"CNN-LSTM hybrid\"]\n",
    "\n",
    "R_SQUARED_VALUES = [r2_rf, rf_lstm_r2, r2_xgb, r2_lgb, r2_cat, r2_gru, r2_lstm, r2_ann, base_r2 ]\n",
    "R2CV = [average_r2_rf, rf_lstm_r2_cv, average_r2_xgb, average_r2_lgb, average_r2_cat, average_r2_gru, average_r2_lstm, average_r2_ann, cnn_lstm_r2_cv ]\n",
    "\n",
    "R2_DIFF = [\n",
    "    (r2_rf - average_r2_rf), \n",
    "    (rf_lstm_r2 - rf_lstm_r2_cv), \n",
    "    (r2_xgb - average_r2_xgb), \n",
    "    (r2_lgb - average_r2_lgb), \n",
    "    (r2_cat - average_r2_cat), \n",
    "    (r2_gru - average_r2_gru), \n",
    "    (r2_lstm - average_r2_lstm), \n",
    "    (r2_ann - average_r2_ann), \n",
    "    (base_r2 - cnn_lstm_r2_cv)\n",
    "]\n",
    "\n",
    "MSE_VALUES = [mse_rf, rf_lstm_mse, mse_xgb, mse_lgb, mse_cat, mse_gru, mse_lstm, mse_ann, base_mse ]\n",
    "MSE_CV = [average_mse_rf, rf_lstm_mse_cv, average_mse_xgb, average_mse_lgb, average_mse_cat, average_mse_gru, average_mse_lstm, average_mse_ann, cnn_lstm_mse_cv ]\n",
    "\n",
    "RMSE_VALUES = [rmse_rf, rf_lstm_rmse, rmse_xgb, rmse_lgb, rmse_cat, rmse_gru, rmse_lstm, rmse_ann , base_rmse ]\n",
    "RMSE_CV = [average_rmse_rf, rf_lstm_rmse_cv, average_rmse_xgb, average_rmse_lgb, average_rmse_cat, average_rmse_gru, average_rmse_lstm, average_rmse_ann, cnn_lstm_rmse_cv]\n",
    "\n",
    "data = {\n",
    "    \"Model\": MODEL_NAMES,\n",
    "    \"R^2\": R_SQUARED_VALUES,\n",
    "    \"CVR2\": R2CV,\n",
    "    \"R2 DIFF\": R2_DIFF,\n",
    "    \"MSE\": MSE_VALUES,\n",
    "    \"MSE CV\": MSE_CV,\n",
    "    \"RMSE\": RMSE_VALUES,\n",
    "    \"RMSE CV\": RMSE_CV\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(data).sort_values(by=[\"R2 DIFF\",\"R^2\"], ascending= [False, True])\n",
    "\n",
    "df_performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262e3409",
   "metadata": {},
   "source": [
    "## best model temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95b145b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define weights for your 'Best Logical Model' criteria\n",
    "# We want high R^2, low RMSE, and low R2 DIFF (stability)\n",
    "weights = {\n",
    "    'R^2': 0.4,       # Predictive power\n",
    "    'MSE': 0.3,\n",
    "    'RMSE': 0.3,     # Magnitude of error\n",
    "    'R2 DIFF': 0.3    # Robustness/Generalization\n",
    "}\n",
    "\n",
    "# 2. Create a Ranking Score (Lower is better)\n",
    "# .rank(ascending=False) means highest value gets rank 1\n",
    "# .rank(ascending=True) means lowest value gets rank 1\n",
    "df_performance['Score'] = (\n",
    "    df_performance['R^2'].rank(ascending=False) * weights['R^2'] +\n",
    "    df_performance['MSE'].rank(ascending=True) * weights['MSE'] +\n",
    "    df_performance['RMSE'].rank(ascending=True) * weights['RMSE']  +\n",
    "    df_performance['R2 DIFF'].rank(ascending=True) * weights['R2 DIFF']\n",
    ")\n",
    "\n",
    "# 3. Extract the winner\n",
    "best_logical_model = df_performance.loc[df_performance['Score'].idxmin()]\n",
    "\n",
    "print(f\"The Best Logical Model is: {best_logical_model['Model']}\")\n",
    "print(f\"--- Reason: Balanced score across R^2 ({best_logical_model['R^2']:.4f}) \"\n",
    "      f\"and Stability (DIFF: {best_logical_model['R2 DIFF']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd247e",
   "metadata": {},
   "source": [
    "# humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcdd900",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'date' not in df_bandarban.columns:\n",
    "    df_bandarban['date'] = pd.to_datetime(df_bandarban[['year', 'month', 'day']])\n",
    "\n",
    "# df_bandarban = df_bandarban[\n",
    "#     (df_bandarban['date'] >= '2024-01-01') & (df_bandarban['date'] <= '2024-12-31')\n",
    "# ]\n",
    "\n",
    "min_date = df_bandarban['date'].min()\n",
    "max_date = df_bandarban['date'].max()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_bandarban['date'], df_bandarban['humidity'], linewidth=0.2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Humidity (%)')\n",
    "plt.title('Humidity (rangamati)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9d0a4b",
   "metadata": {},
   "source": [
    "# feateare engineroah  humidity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a56530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# df = df_bandarban.copy()\n",
    "# if 'date' not in df.columns:\n",
    "#     df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# # 'dew_point','solar_radiation','UV','atmospheric_pressure','precipitation\t' [collected based on the corrilation matrix]\n",
    "\n",
    "# # adding lagging for 3 days\n",
    "# lags = [7, 8, 9]\n",
    "# lag_cols = []\n",
    "\n",
    "# for lag in lags:\n",
    "#     df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "#     df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "#     df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "#     df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "#     df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "#     lag_cols.extend([f'dew_lag_{lag}', f'solar_lag_{lag}', f'UV_lag_{lag}', f'atm_lag_{lag}', f'ppt_lag_{lag}'])\n",
    "\n",
    "\n",
    "# # 7-day Rolling Average\n",
    "# df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "# df['UV_roll_7'] = df['UV'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "# df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "# df['ppt_roll_7'] = df['precipitation'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "# rolling_cols = ['dew_roll_7', 'solar_roll_7', 'UV_roll_7','atm_roll_7', 'ppt_roll_7']\n",
    "\n",
    "\n",
    "# df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "# df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "# def add_fourier_features(df,col,period,n_terms=3):\n",
    "#     for n in range(1, n_terms + 1):\n",
    "#         df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "#         df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "#     return df\n",
    "\n",
    "# df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "# fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e910615",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# FEATURES = fourier_cols + lag_cols + rolling_cols \n",
    "\n",
    "# X = df[FEATURES]\n",
    "# y = df['humidity']\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444f5cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_bandarban = pd.read_csv('../../1980-2024-dataset/rangamati_historical_weather_1980_2024.csv')\n",
    "df_bandarban = df_bandarban.drop('district', axis =1 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49a4cc6c",
   "metadata": {},
   "source": [
    "1. Addressing the Temporal Gaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c080ad60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 1. Addressing the Temporal Gaps\n",
    "\n",
    "# # 1. Ensure a continuous daily index\n",
    "# df_bandarban['date'] = pd.to_datetime(df_bandarban[['year', 'month', 'day']])\n",
    "# df_bandarban = df_bandarban.sort_values('date').set_index('date')\n",
    "\n",
    "# # Create a full date range from 1980 to 2024\n",
    "# full_range = pd.date_range(start=df_bandarban.index.min(), end=df_bandarban.index.max(), freq='D')\n",
    "# df_clean = df_bandarban.reindex(full_range)\n",
    "\n",
    "# # 2. Interpolate small gaps (up to 7 days)\n",
    "# # Large gaps (like 2007) will remain NaN so we can drop them properly later\n",
    "\n",
    "# df = df_clean.interpolate(method='linear', limit=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c237c26",
   "metadata": {},
   "source": [
    "2. Removing Outliers (Sensor Noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6112859e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 3. Detect and handle outliers using a rolling 3-sigma window\n",
    "# rolling_mean = df['humidity'].rolling(window=30, center=True).mean()\n",
    "# rolling_std = df['humidity'].rolling(window=30, center=True).std()\n",
    "\n",
    "# # Replace values outside 3 standard deviations with the rolling mean\n",
    "# outlier_mask = (df['humidity'] - rolling_mean).abs() > (3 * rolling_std)\n",
    "# df.loc[outlier_mask, 'humidity'] = rolling_mean[outlier_mask]\n",
    "\n",
    "# # Drop the remaining large gaps (the 2007 period) to keep the sequences \"real\"\n",
    "# df = df.dropna().reset_index().rename(columns={'index': 'date'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e02bf0",
   "metadata": {},
   "source": [
    "3. Stationarity Check (ADF Test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499ba406",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# result = adfuller(df['humidity'].dropna())\n",
    "# print(f'ADF Statistic: {result[0]}')\n",
    "# print(f'p-value: {result[1]}')\n",
    "\n",
    "# if result[1] > 0.05:\n",
    "#     print(\"Series is non-stationary. We should consider differencing.\")\n",
    "# else:\n",
    "#     print(\"Series is stationary. Raw values are likely safe for the CNN-LSTM.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04331d8",
   "metadata": {},
   "source": [
    "Fragmanet two"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90bdb940",
   "metadata": {},
   "source": [
    "1. Physics-Based Feature: Vapor Pressure Deficit (VPD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd63a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# # Saturated Vapor Pressure (es) - how much moisture the air COULD hold\n",
    "# df['es'] = 0.61078 * np.exp((17.27 * df['temperature(degree C)']) / (df['temperature(degree C)'] + 237.3))\n",
    "\n",
    "# # Actual Vapor Pressure (ea) - based on the dew point\n",
    "# df['ea'] = 0.61078 * np.exp((17.27 * df['dew_point']) / (df['dew_point'] + 237.3))\n",
    "\n",
    "# # VPD is the deficit. A high VPD means the air is very 'thirsty' (low humidity).\n",
    "# df['VPD'] = df['es'] - df['ea']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d18ee51",
   "metadata": {},
   "source": [
    "2. Interaction & Thermal Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dd08fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Solar-Thermal Interaction: Combines the two strongest negative correlations\n",
    "# df['solar_thermal_idx'] = df['solar_radiation'] * df['temperature(degree C)']\n",
    "\n",
    "# # Diurnal Temperature Range: Large swings often correlate with drier air masses\n",
    "# df['temp_range'] = df['max_temperature(degree C)'] - df['minimum_temperature(degree C)']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88155c7",
   "metadata": {},
   "source": [
    "3. Multi-Step Target Engineering (The 7-Day Lead)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faf6e70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create 7 target columns: target_1 (tomorrow) through target_7 (next week)\n",
    "# target_cols = []\n",
    "# for i in range(1, 8):\n",
    "#     col_name = f'target_d{i}'\n",
    "#     df[col_name] = df['humidity'].shift(-i)\n",
    "#     target_cols.append(col_name)\n",
    "\n",
    "# # Drop the last 7 rows where we have no future 'ground truth' to train on\n",
    "# df = df.dropna().reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd0c7128",
   "metadata": {},
   "source": [
    "4. cyclic encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f40882",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# import numpy as np\n",
    "\n",
    "# df = df_bandarban.copy()\n",
    "# if 'date' not in df.columns:\n",
    "#     df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# # 'dew_point','solar_radiation','UV','atmospheric_pressure','precipitation\t' [collected based on the corrilation matrix]\n",
    "\n",
    "# # adding lagging for 3 days\n",
    "# lags = [7, 8, 9]\n",
    "# lag_cols = []\n",
    "\n",
    "# for lag in lags:\n",
    "#     df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "#     df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "#     df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "#     df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "#     df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "#     lag_cols.extend([f'dew_lag_{lag}', f'solar_lag_{lag}', f'UV_lag_{lag}', f'atm_lag_{lag}', f'ppt_lag_{lag}'])\n",
    "\n",
    "\n",
    "# # 7-day Rolling Average\n",
    "# df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "# df['UV_roll_7'] = df['UV'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "# df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "# df['ppt_roll_7'] = df['precipitation'].transform(lambda x: x.rolling(window =7).mean().shift(7))\n",
    "\n",
    "\n",
    "# rolling_cols = ['dew_roll_7', 'solar_roll_7', 'UV_roll_7','atm_roll_7', 'ppt_roll_7']\n",
    "\n",
    "\n",
    "# df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "# df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "# df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "# df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "# def add_fourier_features(df,col,period,n_terms=3):\n",
    "#     for n in range(1, n_terms + 1):\n",
    "#         df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "#         df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "#     return df\n",
    "\n",
    "# df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "# fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f82003d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# 1. Initialize and handle dates\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "# 2. CALC PHYSICS FEATURES FIRST (So they exist in 'df')\n",
    "# Saturated Vapor Pressure (es) and Actual Vapor Pressure (ea)\n",
    "df['es'] = 0.61078 * np.exp((17.27 * df['temperature(degree C)']) / (df['temperature(degree C)'] + 237.3))\n",
    "df['ea'] = 0.61078 * np.exp((17.27 * df['dew_point']) / (df['dew_point'] + 237.3))\n",
    "df['VPD'] = df['es'] - df['ea']\n",
    "\n",
    "# Solar-Thermal Interaction & Temp Range\n",
    "df['solar_thermal_idx'] = df['solar_radiation'] * df['temperature(degree C)']\n",
    "df['temp_range'] = df['max_temperature(degree C)'] - df['minimum_temperature(degree C)']\n",
    "\n",
    "# 3. Create Multi-Output Targets (7-day lead)\n",
    "target_cols = []\n",
    "for i in range(1, 8):\n",
    "    col_name = f'target_d{i}'\n",
    "    df[col_name] = df['humidity'].shift(-i)\n",
    "    target_cols.append(col_name)\n",
    "\n",
    "# 4. Adding Lags (7, 8, 9)\n",
    "lags = [7, 8, 9]\n",
    "lag_cols = []\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "    df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'solar_lag_{lag}', f'UV_lag_{lag}', f'atm_lag_{lag}', f'ppt_lag_{lag}'])\n",
    "\n",
    "# 5. 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['UV_roll_7'] = df['UV'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "df['ppt_roll_7'] = df['precipitation'].transform(lambda x: x.rolling(window=7).mean().shift(7))\n",
    "rolling_cols = ['dew_roll_7', 'solar_roll_7', 'UV_roll_7','atm_roll_7', 'ppt_roll_7']\n",
    "\n",
    "# 6. Cyclical/Fourier Features\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "def add_fourier_features(df, col, period, n_terms=3):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df[col] / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df[col] / period)\n",
    "    return df\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n",
    "# 7. Cleanup and Split\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "physics_features = ['VPD', 'solar_thermal_idx', 'temp_range', 'atmospheric_pressure']\n",
    "FEATURES = fourier_cols + physics_features + lag_cols + rolling_cols\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df[target_cols] # Use the 7-day lead targets here!\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623069f",
   "metadata": {},
   "source": [
    "# RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87d0458c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 400 ,\n",
    "        random_state=42,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for humidity:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'RÂ² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=400,\n",
    "        random_state=42,\n",
    "        max_depth=None,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "43b304c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Day      RMSE        R2\n",
      "0    1  5.357979  0.628911\n",
      "1    2  6.246608  0.495856\n",
      "2    3  6.674352  0.424818\n",
      "3    4  6.786424  0.405290\n",
      "4    5  6.764942  0.409056\n",
      "5    6  6.817802  0.400126\n",
      "6    7  6.908039  0.384456\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Calculate metrics for each day specifically\n",
    "individual_metrics = []\n",
    "\n",
    "for i in range(7):\n",
    "    # i=0 is Day 1, i=6 is Day 7\n",
    "    actual = y_test.iloc[:, i]\n",
    "    predicted = y_pred[:, i]\n",
    "    \n",
    "    mse = mean_squared_error(actual, predicted)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(actual, predicted)\n",
    "    \n",
    "    individual_metrics.append({\n",
    "        'Day': i + 1,\n",
    "        'RMSE': rmse,\n",
    "        'R2': r2\n",
    "    })\n",
    "\n",
    "metrics_df = pd.DataFrame(individual_metrics)\n",
    "\n",
    "# 2. Plotting the Decay of Accuracy\n",
    "fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('Forecast Horizon (Days)')\n",
    "ax1.set_ylabel('RMSE (Lower is Better)', color=color)\n",
    "ax1.plot(metrics_df['Day'], metrics_df['RMSE'], marker='o', color=color, linewidth=2, label='RMSE')\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('RÂ² Score (Higher is Better)', color=color)\n",
    "ax2.plot(metrics_df['Day'], metrics_df['R2'], marker='s', color=color, linewidth=2, label='RÂ²')\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "plt.title('Model Performance Decay over 7-Day Horizon')\n",
    "fig.tight_layout()\n",
    "plt.grid(alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7018f1",
   "metadata": {},
   "source": [
    "## rf-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2832f616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "Windowed UV R2: 0.4190\n",
      "Windowed UV MSE: 45.1500\n",
      "Windowed UV RMSE: 6.7194\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 7  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(7) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mse')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "64d2c39e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n",
      "--- HYBRID MODEL PERFORMANCE ---\n",
      "Final humidity R2: 0.4216\n",
      "Final humidity MSE: 44.9505\n",
      "Final humidity RMSE: 44.9505\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# \n",
    "\n",
    "# ]\n",
    "\n",
    "lstm_feature_cols = FEATURES # change korai lagbe eita naile noise add hobe (therory)\n",
    "\n",
    "X_train_slim = X_train[lstm_feature_cols]\n",
    "X_test_slim = X_test[lstm_feature_cols]\n",
    "\n",
    "scaler_slim = StandardScaler()\n",
    "X_train_scaled = scaler_slim.fit_transform(X_train_slim)\n",
    "X_test_scaled = scaler_slim.transform(X_test_slim)\n",
    "\n",
    "# --- STEP 2: Create Sequences ---\n",
    "window_size = 7 # Try a full week\n",
    "X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "\n",
    "# --- STEP 3: Optimized LSTM ---\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "    LSTM(100, activation='tanh', return_sequences=True), # Return sequences for deeper learning\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(7) \n",
    "])\n",
    "\n",
    "# Use a slightly slower learning rate to find the pattern\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber') # Huber loss is great for weather outliers\n",
    "lstm_model.fit(X_train_seq, y_train_res_seq, epochs=60, batch_size=64, verbose=0)\n",
    "\n",
    "# 1. Align the Test Data (Skip the first 7 days used for the window)\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# 2. Get the \"Base\" predictions from your Random Forest\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# 3. Get the \"Corrections\" from the LSTM\n",
    "# X_test_seq was created during your sequence step\n",
    "lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# 4. Combine them: Base + Correction\n",
    "final_uv = rf_t_pred + lstm_corrections\n",
    "\n",
    "rf_lstm_r2 = r2_score(y_test_aligned, final_uv)\n",
    "rf_lstm_mse = mean_squared_error(y_test_aligned, final_uv)\n",
    "rf_lstm_rmse = np.sqrt(rf_lstm_mse)\n",
    "\n",
    "# 5. Output the New Results\n",
    "print(\"--- HYBRID MODEL PERFORMANCE ---\")\n",
    "print(f\"Final humidity R2: {rf_lstm_r2:.4f}\")\n",
    "print(f\"Final humidity MSE: {rf_lstm_mse:.4f}\")\n",
    "print(f\"Final humidity RMSE: {rf_lstm_mse:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "69176add",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Fold 1 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (660,7) (660,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[104]\u001b[39m\u001b[32m, line 71\u001b[39m\n\u001b[32m     68\u001b[39m rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n\u001b[32m     70\u001b[39m lstm_corrections = lstm_model.predict(X_test_seq)\n\u001b[32m---> \u001b[39m\u001b[32m71\u001b[39m lstmRf_hybrid_prediction = \u001b[43mrf_base_pred\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlstm_corrections\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;66;03m# Adjust index if target is multi-output\u001b[39;00m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# Calculate Metrics\u001b[39;00m\n\u001b[32m     74\u001b[39m lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (660,7) (660,) "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Assuming X and y are your full datasets before the train/test split\n",
    "# X_full, y_full, rf_model, create_sequences need to be defined in your workspace\n",
    "\n",
    "\n",
    "X_full = df[FEATURES]\n",
    "y_full = df[target_cols]\n",
    "\n",
    "fold = 1\n",
    "lstmRf_hybrid_r2_scores = []\n",
    "lstmRf_hybrid_mse_scores = []\n",
    "lstmRf_hybrid_rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "\n",
    "    print(f\"--- Processing Fold {fold} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train_cv, X_test_cv = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    # --- STEP 1: Random Forest Base Model (Required for Residuals) ---\n",
    "\n",
    "    # Training the RF on the current fold's training set\n",
    "    rf_model.fit(X_train_cv, y_train_cv)\n",
    "    train_residuals = y_train_cv - rf_model.predict(X_train_cv)\n",
    "    \n",
    "    # [\n",
    "    # ]\n",
    "\n",
    "    lstm_feature_cols = FEATURES # change korai lagbe eita naile noise add hobe (therory)\n",
    "\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_cv[lstm_feature_cols])\n",
    "    X_test_scaled = scaler.transform(X_test_cv[lstm_feature_cols])\n",
    "    \n",
    "    # --- STEP 3: Create Sequences ---\n",
    "    window_size = 7\n",
    "    X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals.values, window_size)\n",
    "    # We pass zeros for y_test as we only need the X sequences for prediction\n",
    "    X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "    \n",
    "    # --- STEP 4: Train LSTM ---\n",
    "    # Re-initialize the model each fold to avoid weight leakage\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "        LSTM(100, activation='tanh', return_sequences=True),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(7) \n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber')\n",
    "    lstm_model.fit(X_train_seq, y_train_res_seq, epochs=30, batch_size=64, verbose=0)\n",
    "    \n",
    "    # --- STEP 5: Hybrid Prediction & Evaluation ---\n",
    "    # Align target data (drop first 'window_size' rows)\n",
    "    y_test_aligned = y_test_cv.iloc[window_size:]\n",
    "    rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n",
    "    \n",
    "    lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "    lstmRf_hybrid_prediction = rf_base_pred + lstm_corrections[:, 0] # Adjust index if target is multi-output\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_mse_kf = mean_squared_error(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_rmse_kf = np.sqrt(lstmRf_hybrid_mse_kf)\n",
    "\n",
    "    lstmRf_hybrid_r2_scores.append(lstmRf_hybrid_r2_kf)\n",
    "    lstmRf_hybrid_mse_scores.append(lstmRf_hybrid_mse_kf)\n",
    "    lstmRf_hybrid_rmse_scores.append(lstmRf_hybrid_rmse_kf)\n",
    "\n",
    "    print(f\"Fold {fold} R2: {lstmRf_hybrid_r2_kf:.4f}\")\n",
    "    print(f\"Fold {fold} MSE: {lstmRf_hybrid_mse_kf:.4f}\")\n",
    "    print(f\"Fold {fold} RMSE: {lstmRf_hybrid_rmse_kf:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "rf_lstm_r2_cv = np.mean(lstmRf_hybrid_r2_scores)\n",
    "rf_lstm_mse_cv = np.mean(lstmRf_hybrid_mse_scores)\n",
    "rf_lstm_rmse_cv = np.mean(lstmRf_hybrid_rmse_scores)\n",
    "\n",
    "print(\"\\n--- FINAL CROSS-VALIDATION RESULTS ---\")\n",
    "print(f\"Mean R2: {rf_lstm_r2_cv:.4f} (+/- {np.std(lstmRf_hybrid_r2_scores):.4f})\")\n",
    "print(f\"Mean mse: {rf_lstm_mse_cv:.4f} (+/- {np.std(lstmRf_hybrid_mse_scores):.4f})\")\n",
    "print(f\"Mean rmse: {rf_lstm_rmse_cv:.4f} (+/- {np.std(lstmRf_hybrid_rmse_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed514bf",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de833e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "targets = y\n",
    "xgb_model = {}\n",
    "y_preds_xgb = {}\n",
    "rmses_xgb = {}\n",
    "r2s_xgb = {}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Results for humidity:\")\n",
    "print(f'Mean Squared Error: {mse_xgb:.4f}')\n",
    "print(f'RMSE: {rmse_xgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_xgb:.4f}')\n",
    "\n",
    "\n",
    "# K-Fold cross-validation for XGBoost\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_xgb = []\n",
    "r2_list_xgb = [] # Added to track R2 across all folds\n",
    "mse_list_xgb = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        xgb_model_kf = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_xgb = xgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "\n",
    "        mse_kf_xgb = mean_squared_error(y_test_kf, y_pred_kf_xgb)\n",
    "        rmse_kf_xgb = np.sqrt(mse_kf_xgb)\n",
    "        r2_kf_xgb = r2_score(y_test_kf, y_pred_kf_xgb)\n",
    "\n",
    "        mse_list_xgb.append(mse_kf_xgb)\n",
    "        rmse_list_xgb.append(rmse_kf_xgb)\n",
    "        r2_list_xgb.append(r2_kf_xgb)\n",
    "\n",
    "\n",
    "        average_rmse_xgb = np.mean(rmse_list_xgb)\n",
    "        average_r2_xgb = np.mean(r2_list_xgb)\n",
    "        average_mse_xgb = np.mean(mse_kf_xgb)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_xgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_xgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_xgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_xgb}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "importance = xgb_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_xgb - average_r2_xgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(feature_importance_df_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "448edf40",
   "metadata": {},
   "source": [
    "## light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159368be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "targets = y\n",
    "lgb_model = {}\n",
    "y_preds_lgb = {}\n",
    "rmses_lgb = {}\n",
    "r2s_lgb = {}\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "        # n_estimators=800,\n",
    "        # learning_rate=0.01,\n",
    "        # max_depth=8,\n",
    "        # subsample=0.8,\n",
    "        # colsample_bytree=0.8,\n",
    "        # random_state=42,\n",
    "        # verbosity=-1\n",
    "\n",
    "        n_estimators=300,        # Reduced to prevent memorization as UV r 4k dataset\n",
    "        learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "        max_depth=6,             # Shallow trees are better for 4k rows\n",
    "        num_leaves=20,           # Controls complexity\n",
    "        min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "        subsample=0.7,           # More aggressive sampling for better generalization\n",
    "        colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "                              # Clean console\n",
    "    )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_lgb = np.sqrt(mse_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Results for humidity:\")\n",
    "print(f'Mean Squared Error: {mse_lgb:.4f}')\n",
    "print(f'RMSE: {rmse_lgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_lgb:.4f}')\n",
    "\n",
    "\n",
    "    # --- 6) 5-fold CV R^2 ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "rmse_list_lgb = []\n",
    "r2_list_lgb = []\n",
    "mse_list_lgb = []\n",
    "\n",
    "\n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        lgb_model_kf = lgb.LGBMRegressor(\n",
    "            \n",
    "            n_estimators=300,        # Reduced to prevent memorization\n",
    "            learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "            max_depth=6,             # Shallow trees are better for 4k rows\n",
    "            num_leaves=20,           # Controls complexity\n",
    "            min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "            subsample=0.7,           # More aggressive sampling for better generalization\n",
    "            colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "            random_state=42,\n",
    "            verbosity=-1             # Clean console\n",
    "        )\n",
    "\n",
    "        lgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_lgb = lgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_lgb = mean_squared_error(y_test_kf, y_pred_kf_lgb)\n",
    "        rmse_kf_lgb = np.sqrt(mse_kf_lgb)\n",
    "        r2_kf_lgb = r2_score(y_test_kf, y_pred_kf_lgb)\n",
    "        \n",
    "        mse_list_lgb.append(mse_kf_lgb)\n",
    "        rmse_list_lgb.append(rmse_kf_lgb)\n",
    "        r2_list_lgb.append(r2_kf_lgb)\n",
    "\n",
    "        average_rmse_lgb = np.mean(rmse_list_lgb)\n",
    "        average_r2_lgb = np.mean(r2_list_lgb)\n",
    "        average_mse_lgb = np.mean(mse_kf_lgb)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_lgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_lgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_lgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lgb}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = lgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_lgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_lgb = feature_importance_df_lgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_lgb - average_r2_lgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(feature_importance_df_lgb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f82ee741",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0377e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "targets = y\n",
    "cat_model = {}\n",
    "y_preds_cat = {}\n",
    "rmses_cat = {}\n",
    "r2s_cat = {}\n",
    "\n",
    "# loss_function='RMSE' is standard for regression\n",
    "cat_model = CatBoostRegressor(\n",
    "        iterations=800,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "        bootstrap_type='Bayesian',\n",
    "        bagging_temperature=1,\n",
    "        random_strength=1,\n",
    "        loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "mse_cat = mean_squared_error(y_test, y_pred)\n",
    "rmse_cat = np.sqrt(mse_cat)\n",
    "r2_cat = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"\\nLightGBM Results for temperature :\")\n",
    "print(f'Mean Squared Error: {mse_cat:.4f}')\n",
    "print(f'RMSE: {rmse_cat:.4f}')\n",
    "print(f'RÂ² Score: {r2_cat:.4f}')\n",
    "\n",
    "\n",
    "    # 3. 5-Fold Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_cat = []\n",
    "r2_list_cat = []\n",
    "mse_list_cat = []\n",
    "    \n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        cat_model_kf = CatBoostRegressor(\n",
    "            iterations=800,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "            bootstrap_type='Bayesian',\n",
    "            bagging_temperature=1,\n",
    "            random_strength=1,\n",
    "            loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        cat_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_cat = cat_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_cat = mean_squared_error(y_test_kf, y_pred_kf_cat)\n",
    "        rmse_kf_cat = np.sqrt(mse_kf_cat)\n",
    "        r2_kf_cat = r2_score(y_test_kf, y_pred_kf_cat)\n",
    "\n",
    "        mse_list_cat.append(mse_kf_cat)\n",
    "        rmse_list_cat.append(rmse_kf_cat)\n",
    "        r2_list_cat.append(r2_kf_cat)\n",
    "\n",
    "        average_rmse_cat = np.mean(rmse_list_cat)\n",
    "        average_r2_cat = np.mean(r2_list_cat)\n",
    "        average_mse_cat = np.mean(mse_list_cat)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_cat:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_cat:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_cat}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_cat}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = cat_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_cat = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_cat = feature_importance_df_cat.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_cat - average_r2_cat)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nCatBoost Feature Importances:\")\n",
    "print(feature_importance_df_cat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5543364b",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8733a2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create an instance with specific parameters\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15,          # Wait 15 epochs for improvement before stopping\n",
    "    restore_best_weights=True  # Very important: keeps the best version of your model\n",
    ")\n",
    "\n",
    "# 1. Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y) \n",
    "\n",
    "# 2. Reshape for GRU: (samples, time_steps, features)\n",
    "# Here we use time_steps=1. If you want sequences, you'd need a sliding window function.\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split data (matching your non-shuffle 80/20 split)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(7) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train initial model\n",
    "gru_model = build_gru((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_gru = mean_squared_error(y_test_unscaled, y_pred)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_unscaled, y_pred)\n",
    "\n",
    "print(f\"\\nGRU Results for humidity :\")\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'RMSE: {rmse_gru:.4f}')\n",
    "print(f'RÂ² Score: {r2_gru:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild/Reset model for each fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_gru = np.mean(r2_list_gru)\n",
    "average_mse_gru = np.mean(mse_list_gru)\n",
    "average_rmse_gru = np.mean(rmse_list_gru)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_gru:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_gru:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_gru:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_gru}\")\n",
    "\n",
    "diff = (r2_gru - np.mean(r2_list_gru))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28be70a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculates importance by measuring how much the MSE increases \n",
    "    when a single feature is randomly shuffled.\n",
    "    \"\"\"\n",
    "    # Baseline prediction\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    baseline_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                     scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X_val.shape[2]):  # Iterate through each feature\n",
    "        save = X_val[:, :, i].copy()\n",
    "        \n",
    "        # Shuffle the current feature across all samples\n",
    "        np.random.shuffle(X_val[:, :, i])\n",
    "        \n",
    "        # Predict with shuffled feature\n",
    "        shuffled_preds = model.predict(X_val, verbose=0)\n",
    "        shuffled_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                         scaler_y.inverse_transform(shuffled_preds))\n",
    "        \n",
    "        # Importance is the increase in error\n",
    "        importances.append(max(0, shuffled_mse - baseline_mse))\n",
    "        \n",
    "        # Restore the original feature values\n",
    "        X_val[:, :, i] = save\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    return importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Execute ---\n",
    "# Note: Use your X_test and y_test from the previous step\n",
    "feature_importance_gru = calculate_permutation_importance(\n",
    "    gru_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGRU Permutation Feature Importances:\")\n",
    "print(feature_importance_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd4a4536",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7a691f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for LSTM: [samples, time_steps, features]\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split matching your CatBoost logic (shuffle=False)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "lstm_model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_actual, y_pred_lstm)\n",
    "\n",
    "print(f\"\\nLSTM Results for humidity :\")\n",
    "print(f'Mean Squared Error: {mse_lstm:.4f}')    \n",
    "print(f'RMSE: {rmse_lstm:.4f}')\n",
    "print(f'RÂ² Score: {r2_lstm:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_lstm = []\n",
    "r2_list_lstm = []\n",
    "mse_list_lstm = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    lstm_kf = build_lstm((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    lstm_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = lstm_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_lstm.append(np.sqrt(mse_kf))\n",
    "    mse_list_lstm.append(mse_kf)\n",
    "    r2_list_lstm.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "\n",
    "average_r2_lstm = np.mean(r2_list_lstm)\n",
    "average_mse_lstm = np.mean(mse_list_lstm)\n",
    "average_rmse_lstm = np.mean(rmse_list_lstm)\n",
    " \n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from LSTM CV: {average_rmse_lstm:.4f}\")\n",
    "print(f\"Average RÂ² from LSTM CV: { average_r2_lstm:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_lstm:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lstm}\")\n",
    "\n",
    "def calculate_lstm_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Computes permutation importance for a trained LSTM model.\n",
    "    \"\"\"\n",
    "    # 1. Get baseline score (Inverse scale to get real-world MSE)\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    \n",
    "    # Iterate through each feature index\n",
    "    for i in range(X_val.shape[2]):\n",
    "        # Create a copy to avoid permanent shuffling\n",
    "        X_permuted = X_val.copy()\n",
    "        \n",
    "        # 2. Shuffle the specific feature across all samples\n",
    "        # Shuffling happens across the 'samples' dimension for the i-th feature\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        \n",
    "        # 3. Predict with the permuted feature\n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        # 4. Importance = Increase in Error (shuffled error - baseline error)\n",
    "        importance = max(0, permuted_mse - baseline_mse)\n",
    "        importance_results.append(importance)\n",
    "\n",
    "    # Organize into a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names, \n",
    "        'Importance': importance_results\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# --- Execution ---\n",
    "# Using the X_test and y_test from your LSTM training\n",
    "lstm_importance_df = calculate_lstm_permutation_importance(\n",
    "    lstm_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "diff = (r2_lstm - np.mean(r2_list_lstm))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLSTM Permutation Feature Importances:\")\n",
    "print(lstm_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01fbad63",
   "metadata": {},
   "source": [
    "## ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac9aad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X_ann = StandardScaler()\n",
    "scaler_y_ann = StandardScaler()\n",
    "\n",
    "X_scaled_ann = scaler_X_ann.fit_transform(X)\n",
    "y_scaled_ann = scaler_y_ann.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split matching your CatBoost logic (80/20, shuffle=False)\n",
    "split_idx = int(len(X_scaled_ann) * 0.8)\n",
    "X_train_ann, X_test_ann = X_scaled_ann[:split_idx], X_scaled_ann[split_idx:]\n",
    "y_train_ann, y_test_ann = y_scaled_ann[:split_idx], y_scaled_ann[split_idx:]\n",
    "\n",
    "# build model \n",
    "\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Linear output for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "ann_model = build_ann(X_train_ann.shape[1])\n",
    "ann_model.fit(X_train_ann, y_train_ann, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled_ann = ann_model.predict(X_test_ann)\n",
    "y_pred_ann = scaler_y_ann.inverse_transform(y_pred_scaled_ann)\n",
    "y_test_actual = scaler_y_ann.inverse_transform(y_test_ann)\n",
    "\n",
    "mse_ann = mean_squared_error(y_test_actual, y_pred_ann)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "r2_ann = r2_score(y_test_actual, y_pred_ann)\n",
    "\n",
    "print(f\"\\nANN Results for humidity :\")\n",
    "print(f'Mean Squared Error: {mse_ann:.4f}')\n",
    "print(f'RMSE: {rmse_ann:.4f}')\n",
    "print(f'RÂ² Score: {r2_ann:.4f}')\n",
    "\n",
    "# CV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_ann = []\n",
    "r2_list_ann = []\n",
    "mse_list_ann = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled_ann):\n",
    "    X_train_kf, X_test_kf = X_scaled_ann[train_index], X_scaled_ann[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled_ann[train_index], y_scaled_ann[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    ann_kf = build_ann(X_train_kf.shape[1])\n",
    "    ann_kf.fit(X_train_kf, y_train_kf, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = ann_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y_ann.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y_ann.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_ann.append(np.sqrt(mse_kf))\n",
    "    mse_list_ann.append(mse_kf)\n",
    "    r2_list_ann.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_ann = np.mean(r2_list_ann)\n",
    "average_mse_ann = np.mean(mse_list_ann)\n",
    "average_rmse_ann = np.mean(rmse_list_ann)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from ANN CV: {average_rmse_ann:.4f}\")\n",
    "print(f\"Average RÂ² from ANN CV: {average_r2_ann:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_ann:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_ann}\")\n",
    "\n",
    "# importance\n",
    "def calculate_ann_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    for i in range(X_val.shape[1]): # Iterate through 2D features\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        importance_results.append(max(0, permuted_mse - baseline_mse))\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance_results}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "ann_importance_df = calculate_ann_permutation_importance(ann_model, X_test_ann, y_test_ann, scaler_y_ann, FEATURES)\n",
    "\n",
    "diff = (r2_ann - np.mean(r2_list_ann))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nANN Permutation Feature Importances:\")\n",
    "print(ann_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65427126",
   "metadata": {},
   "source": [
    "## cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca2ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# 1. Prepare 3D Data (Samples, Time Steps, Features)\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_hybrid(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. CNN Stage: Extracts spatial/local patterns from the window\n",
    "        # Reducing filters to 32 is often better for ~4k rows to prevent noise capture\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100,1)),\n",
    "        BatchNormalization(), # Stabilizes learning and speeds up convergence\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2), # Standard regularization\n",
    "\n",
    "        # 2. LSTM Stage: Learns temporal dependencies\n",
    "        # tanh is the standard and most stable activation for LSTM\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        # 3. Fully Connected Stage\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1) # Output for UV prediction\n",
    "    ])\n",
    "    \n",
    "    # Using a slightly lower learning rate (0.0005) helps with smaller datasets\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Data\n",
    "window = 30\n",
    "data_values = df['humidity'].values.reshape(-1, 1)\n",
    "X, y = create_sequences(data_values, window)\n",
    "\n",
    "# --- BASE PERFORMANCE ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "model = build_hybrid((window, 1))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, y_pred)\n",
    "base_rmse = np.sqrt(base_mse)\n",
    "base_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Base Results: R2: {base_r2:.4f}, MSE: {base_mse:.4f}, RMSE: {base_rmse:.4f}\")\n",
    "\n",
    "# --- 5-FOLD CROSS VALIDATION ---\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_r2, cv_mse, cv_rmse = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    m = build_hybrid((window, 1))\n",
    "    m.fit(X[train_idx], y[train_idx], epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    p = m.predict(X[test_idx])\n",
    "    cv_r2.append(r2_score(y[test_idx], p))\n",
    "    cv_mse.append(mean_squared_error(y[test_idx], p))\n",
    "    cv_rmse.append(np.sqrt(cv_mse[-1]))\n",
    "\n",
    "\n",
    "cnn_lstm_r2_cv = np.mean(cv_r2)\n",
    "cnn_lstm_mse_cv = np.mean(cv_mse)\n",
    "cnn_lstm_rmse_cv = np.mean(cv_rmse)\n",
    "\n",
    "print(f\"5-Fold CV Average: R2: {cnn_lstm_r2_cv:.4f}, MSE: {cnn_lstm_mse_cv:.4f}, RMSE: {cnn_lstm_rmse_cv:.4f}\")\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80334495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler  # Added for normalization\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout, BatchNormalization\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "# 1. SCALING AND DATA PREPARATION\n",
    "window = 30\n",
    "# Initialize scalers for features (X) and target (y)\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "# Scale features and target separately\n",
    "scaled_features = scaler_X.fit_transform(df[FEATURES])\n",
    "scaled_target = scaler_y.fit_transform(df[['humidity']])  \n",
    "\n",
    "# Combine scaled data back for sequence creation\n",
    "data_all_scaled = np.hstack((scaled_features, scaled_target))\n",
    "\n",
    "def create_sequences_multivariate(data, window_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size), :-1]) \n",
    "        y.append(data[i + window_size, -1]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "X, y = create_sequences_multivariate(data_all_scaled, window)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "\n",
    "# 2. UPDATED HYBRID MODEL (No changes needed to architecture)\n",
    "def build_hybrid_multivariate(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2),\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "model = build_hybrid_multivariate((window, len(FEATURES)))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# 3. PERFORMANCE EVALUATION (With Inverse Scaling)\n",
    "y_pred_scaled = model.predict(X_test)\n",
    "# Convert predictions and actuals back to original Celsius values\n",
    "y_pred_rescaled = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_rescaled = scaler_y.inverse_transform(y_test.reshape(-1, 1))\n",
    "\n",
    "mse_final = mean_squared_error(y_test_rescaled, y_pred_rescaled)\n",
    "r2_final = r2_score(y_test_rescaled, y_pred_rescaled)\n",
    "\n",
    "print(f\"\\nRescaled Results: R2: {r2_final:.4f}, RMSE: {np.sqrt(mse_final):.4f}\")\n",
    "\n",
    "# 4. PERMUTATION IMPORTANCE (Standard logic remains)\n",
    "def get_permutation_importance(model, X_val, y_val, feature_names):\n",
    "    base_preds = model.predict(X_val, verbose=0)\n",
    "    base_rmse = np.sqrt(mean_squared_error(y_val, base_preds))\n",
    "    importance_results = []\n",
    "\n",
    "    for i, col_name in enumerate(feature_names):\n",
    "        scores = []\n",
    "        for run in range(5):\n",
    "            X_shuffled = X_val.copy()\n",
    "            shuffled_feature = np.random.permutation(X_shuffled[:, :, i].flatten())\n",
    "            X_shuffled[:, :, i] = shuffled_feature.reshape(X_shuffled.shape[0], X_shuffled.shape[1])\n",
    "            shuff_preds = model.predict(X_shuffled, verbose=0)\n",
    "            shuff_rmse = np.sqrt(mean_squared_error(y_val, shuff_preds))\n",
    "            scores.append(shuff_rmse - base_rmse)\n",
    "        \n",
    "        importance_results.append({\n",
    "            'Feature': col_name,\n",
    "            'Average_Importance': np.mean(scores),\n",
    "            'Std_Dev': np.std(scores)\n",
    "        })\n",
    "        print(f\"Computed importance for: {col_name}\")\n",
    "\n",
    "    return pd.DataFrame(importance_results).sort_values(by='Average_Importance', ascending=False)\n",
    "\n",
    "importance_df_hybrid = get_permutation_importance(model, X_test, y_test, FEATURES)\n",
    "print(\"\\n--- Hybrid Model Feature Importance ---\")\n",
    "print(importance_df_hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f37b88",
   "metadata": {},
   "source": [
    "## model chart humidity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d107466",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance chart \n",
    "MODEL_NAMES = [\"Random Forest\", \"RF-LSTM hybrid\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"GRU\", \"LSTM\", \"ANN\", \"CNN-LSTM hybrid\"]\n",
    "\n",
    "R_SQUARED_VALUES = [r2_rf, rf_lstm_r2, r2_xgb, r2_lgb, r2_cat, r2_gru, r2_lstm, r2_ann, base_r2 ]\n",
    "R2CV = [average_r2_rf, rf_lstm_r2_cv, average_r2_xgb, average_r2_lgb, average_r2_cat, average_r2_gru, average_r2_lstm, average_r2_ann, cnn_lstm_r2_cv ]\n",
    "\n",
    "R2_DIFF = [\n",
    "    (r2_rf - average_r2_rf), \n",
    "    (rf_lstm_r2 - rf_lstm_r2_cv), \n",
    "    (r2_xgb - average_r2_xgb), \n",
    "    (r2_lgb - average_r2_lgb), \n",
    "    (r2_cat - average_r2_cat), \n",
    "    (r2_gru - average_r2_gru), \n",
    "    (r2_lstm - average_r2_lstm), \n",
    "    (r2_ann - average_r2_ann), \n",
    "    (base_r2 - cnn_lstm_r2_cv)\n",
    "]\n",
    "\n",
    "MSE_VALUES = [mse_rf, rf_lstm_mse, mse_xgb, mse_lgb, mse_cat, mse_gru, mse_lstm, mse_ann, base_mse ]\n",
    "MSE_CV = [average_mse_rf, rf_lstm_mse_cv, average_mse_xgb, average_mse_lgb, average_mse_cat, average_mse_gru, average_mse_lstm, average_mse_ann, cnn_lstm_mse_cv ]\n",
    "\n",
    "RMSE_VALUES = [rmse_rf, rf_lstm_rmse, rmse_xgb, rmse_lgb, rmse_cat, rmse_gru, rmse_lstm, rmse_ann , base_rmse ]\n",
    "RMSE_CV = [average_rmse_rf, rf_lstm_rmse_cv, average_rmse_xgb, average_rmse_lgb, average_rmse_cat, average_rmse_gru, average_rmse_lstm, average_rmse_ann, cnn_lstm_rmse_cv]\n",
    "\n",
    "data = {\n",
    "    \"Model\": MODEL_NAMES,\n",
    "    \"R^2\": R_SQUARED_VALUES,\n",
    "    \"CVR2\": R2CV,\n",
    "    \"R2 DIFF\": R2_DIFF,\n",
    "    \"MSE\": MSE_VALUES,\n",
    "    \"MSE CV\": MSE_CV,\n",
    "    \"RMSE\": RMSE_VALUES,\n",
    "    \"RMSE CV\": RMSE_CV\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(data).sort_values(by=[\"R2 DIFF\",\"R^2\"], ascending= [True, True])\n",
    "\n",
    "print (df_performance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494f1da0",
   "metadata": {},
   "source": [
    "## best model humidty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3407f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define weights for your 'Best Logical Model' criteria\n",
    "# We want high R^2, low RMSE, and low R2 DIFF (stability)\n",
    "weights = {\n",
    "    'R^2': 0.4,       # Predictive power\n",
    "    'MSE': 0.3,\n",
    "    'RMSE': 0.3,     # Magnitude of error\n",
    "    'R2 DIFF': 0.3    # Robustness/Generalization\n",
    "}\n",
    "\n",
    "# 2. Create a Ranking Score (Lower is better)\n",
    "# .rank(ascending=False) means highest value gets rank 1\n",
    "# .rank(ascending=True) means lowest value gets rank 1\n",
    "df_performance['Score'] = (\n",
    "    df_performance['R^2'].rank(ascending=False) * weights['R^2'] +\n",
    "    df_performance['MSE'].rank(ascending=True) * weights['MSE'] +\n",
    "    df_performance['RMSE'].rank(ascending=True) * weights['RMSE']  +\n",
    "    df_performance['R2 DIFF'].rank(ascending=True) * weights['R2 DIFF']\n",
    ")\n",
    "\n",
    "# 3. Extract the winner\n",
    "best_logical_model = df_performance.loc[df_performance['Score'].idxmin()]\n",
    "\n",
    "print(f\"The Best Logical Model is: {best_logical_model['Model']}\")\n",
    "print(f\"--- Reason: Balanced score across R^2 ({best_logical_model['R^2']:.4f}) \"\n",
    "      f\"and Stability (DIFF: {best_logical_model['R2 DIFF']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20fc89f2",
   "metadata": {},
   "source": [
    "# dewpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86de59ac",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_bandarban.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['hum_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'hum_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42df9db4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1204ae49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5e186d7",
   "metadata": {},
   "source": [
    "## rf-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6694a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c185bac9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c887bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac8caae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19014a03",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60b6e1ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec6ebb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866e6e25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2d3f024b",
   "metadata": {},
   "source": [
    "## light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7420e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2b9dc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53745ecc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f3bb7c5a",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785e912e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85415d8f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a05ccd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3685bd56",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f66e6ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18bbcc2e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c116d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf12f31d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4bf986a4",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7657afbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856d8fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e506ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "08327780",
   "metadata": {},
   "source": [
    "## ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dc5e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864abe92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "faaff11b",
   "metadata": {},
   "source": [
    "## cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144ba64f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca06c3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801f4b7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3fd22e06",
   "metadata": {},
   "source": [
    "## model chart dew point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98089a0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "01db342a",
   "metadata": {},
   "source": [
    "## best model dewpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252dad5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29792139",
   "metadata": {},
   "source": [
    "# solar radiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a624aba",
   "metadata": {},
   "source": [
    "## rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26537f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d693ed08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46dcfb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eac83abe",
   "metadata": {},
   "source": [
    "## rf-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12001145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2fa78be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf04363",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42cabc64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "912fbeda",
   "metadata": {},
   "source": [
    "## xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1c1e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aace1114",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6cc49c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f0cadc6a",
   "metadata": {},
   "source": [
    "## light gbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba125cc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ab2b43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e14dcdf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405e17f7",
   "metadata": {},
   "source": [
    "## catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3a751",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a8da9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20901589",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "976dee5e",
   "metadata": {},
   "source": [
    "## gru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7039aa13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935c74c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2c9463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ca7e5a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d5527d09",
   "metadata": {},
   "source": [
    "## lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7be59e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093f1859",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1e64af0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94505ce1",
   "metadata": {},
   "source": [
    "## ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59259bac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc993fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c7669736",
   "metadata": {},
   "source": [
    "## cnn-lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c752c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3e2883",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df1a6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e79783d",
   "metadata": {},
   "source": [
    "## model chart solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e090a50",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "62e67225",
   "metadata": {},
   "source": [
    "## best model solar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739d74d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
