{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c041d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpd\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m df_chittagong = pd.read_csv(\u001b[33m'\u001b[39m\u001b[33m../1980-2024-dataset/Chittagong_historical_weather_1980_2024.csv\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/Chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aac7654b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msns\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Create a correlation matrix\u001b[39;00m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df_chittagong.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Chittagong Correlation test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c028b4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_chittagong' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdf_chittagong\u001b[49m.head()\n",
      "\u001b[31mNameError\u001b[39m: name 'df_chittagong' is not defined"
     ]
    }
   ],
   "source": [
    "df_chittagong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae507e04",
   "metadata": {},
   "source": [
    "Temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "caf24d48",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'matplotlib'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df_chittagong.columns:\n\u001b[32m      4\u001b[39m     df_chittagong[\u001b[33m'\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m'\u001b[39m] = pd.to_datetime(df_chittagong[[\u001b[33m'\u001b[39m\u001b[33myear\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mday\u001b[39m\u001b[33m'\u001b[39m]])\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'matplotlib'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'date' not in df_chittagong.columns:\n",
    "    df_chittagong['date'] = pd.to_datetime(df_chittagong[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "\n",
    "min_date = df_chittagong['date'].min()\n",
    "max_date = df_chittagong['date'].max()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_chittagong['date'], df_chittagong['temperature(degree C)'], linewidth=0.2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (°C)')\n",
    "plt.title('Temperature (chittagong)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42609019",
   "metadata": {},
   "source": [
    "# RF with lagging rolling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2dc36f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','dew_point' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\n",
    "    'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "]\n",
    "\n",
    "\n",
    "#FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols \n",
    "#0.8640  0.8793 ~ 1.53\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eef7e5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results for temperature:\n",
      "MSE: 1.5497\n",
      "RMSE: 1.2449\n",
      "R² Score: 0.8634\n",
      "Average RMSE from CV: 1.1454\n",
      "Average R² from CV: 0.8793\n",
      "Avarage MSE: 1.3207818864982008\n",
      "Individual Fold RMSEs: [np.float64(1.2046498144630846), np.float64(1.0618408597237345), np.float64(1.1664069494588172), np.float64(1.0180043712199345), np.float64(1.2760815702992907)]\n",
      "\n",
      " R2 ~ -1.5851\n",
      "\n",
      "Random Forest Feature Importances (from last fold):\n",
      "                 Feature  Importance\n",
      "0             dew_roll_7    0.197988\n",
      "1              dew_point    0.173849\n",
      "2              dew_lag_1    0.169026\n",
      "3              dew_lag_2    0.112095\n",
      "4              dew_lag_3    0.098978\n",
      "5             atm_roll_7    0.059300\n",
      "6      day_of_year_sin_3    0.047419\n",
      "7      day_of_year_cos_1    0.038932\n",
      "11     day_of_year_sin_1    0.029099\n",
      "10             atm_lag_1    0.026848\n",
      "8   atmospheric_pressure    0.024150\n",
      "9              atm_lag_2    0.022315\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "\n",
    "Random Forest Feature Importances (from last fold):\n",
    "                 Feature  Importance\n",
    "15            dew_roll_7    0.194442\n",
    "1              dew_point    0.167315\n",
    "9              dew_lag_1    0.160662\n",
    "11             dew_lag_2    0.118672\n",
    "13             dew_lag_3    0.075158\n",
    "14            atm_roll_7    0.054073\n",
    "6      day_of_year_sin_3    0.041480\n",
    "3      day_of_year_cos_1    0.029418\n",
    "0   atmospheric_pressure    0.028322\n",
    "10             atm_lag_2    0.024513\n",
    "8              atm_lag_1    0.024098\n",
    "2      day_of_year_sin_1    0.019313\n",
    "12             atm_lag_3    0.019240\n",
    "4      day_of_year_sin_2    0.018426\n",
    "5      day_of_year_cos_2    0.016615\n",
    "7      day_of_year_cos_3    0.008253\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for temperature:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'R² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169cc72",
   "metadata": {},
   "source": [
    "#RF -LSTM HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "994e75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\n",
    "    'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "]\n",
    "\n",
    "\n",
    "#FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols \n",
    "#0.8640  0.8793 ~ 1.53\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m StandardScaler\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlayers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout, Input\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mean_squared_error, r2_score\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _tf_keras \u001b[38;5;28;01mas\u001b[39;00m _tf_keras\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\_tf_keras\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_tf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\_tf_keras\\keras\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations \u001b[38;5;28;01mas\u001b[39;00m activations\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications \u001b[38;5;28;01mas\u001b[39;00m applications\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m callbacks \u001b[38;5;28;01mas\u001b[39;00m callbacks\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\activations\\__init__.py:7\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[33;03m\"\"\"DO NOT EDIT.\u001b[39;00m\n\u001b[32m      2\u001b[39m \n\u001b[32m      3\u001b[39m \u001b[33;03mThis file was autogenerated. Do not edit it by hand,\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03msince your modifications would be overwritten.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m deserialize \u001b[38;5;28;01mas\u001b[39;00m deserialize\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get \u001b[38;5;28;01mas\u001b[39;00m get\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m serialize \u001b[38;5;28;01mas\u001b[39;00m serialize\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m activations\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m applications\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\activations\\__init__.py:3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtypes\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m celu\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m elu\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m exponential\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\activations\\activations.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ops\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfig\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtorch\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m      4\u001b[39m     \u001b[38;5;66;03m# When using the torch backend,\u001b[39;00m\n\u001b[32m      5\u001b[39m     \u001b[38;5;66;03m# torch needs to be imported first, otherwise it will segfault\u001b[39;00m\n\u001b[32m      6\u001b[39m     \u001b[38;5;66;03m# upon import.\u001b[39;00m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\config.py:448\u001b[39m\n\u001b[32m    445\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    446\u001b[39m         _NNX_ENABLED = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m448\u001b[39m \u001b[43mset_nnx_enabled\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_NNX_ENABLED\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\config.py:249\u001b[39m, in \u001b[36mset_nnx_enabled\u001b[39m\u001b[34m(value)\u001b[39m\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mset_nnx_enabled\u001b[39m(value):\n\u001b[32m    248\u001b[39m     \u001b[38;5;28;01mglobal\u001b[39;00m _NNX_ENABLED\n\u001b[32m--> \u001b[39m\u001b[32m249\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m global_state\n\u001b[32m    251\u001b[39m     _NNX_ENABLED = \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[32m    252\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _NNX_ENABLED:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\common\\__init__.py:2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend_utils\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdtypes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m result_type\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutocastScope\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Variable \u001b[38;5;28;01mas\u001b[39;00m KerasVariable\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\common\\dtypes.py:5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m config\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mvariables\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m standardize_dtype\n\u001b[32m      7\u001b[39m BOOL_TYPES = (\u001b[33m\"\u001b[39m\u001b[33mbool\u001b[39m\u001b[33m\"\u001b[39m,)\n\u001b[32m      8\u001b[39m INT_TYPES = (\n\u001b[32m      9\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint8\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     10\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33muint16\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mint64\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     17\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\backend\\common\\variables.py:12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbackend\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcommon\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mstateless_scope\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m in_stateless_scope\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnaming\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m auto_name\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mVariable\u001b[39;00m:\n\u001b[32m     16\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Represents a backend-agnostic variable in Keras.\u001b[39;00m\n\u001b[32m     17\u001b[39m \n\u001b[32m     18\u001b[39m \u001b[33;03m    A `Variable` acts as a container for state. It holds a tensor value and can\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     89\u001b[39m \u001b[33;03m    ```\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\utils\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01maudio_dataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m audio_dataset_from_directory\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdataset_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m split_dataset\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfile_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_file\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\utils\\audio_dataset_utils.py:4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dataset_utils\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow \u001b[38;5;28;01mas\u001b[39;00m tf\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodule_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tensorflow_io \u001b[38;5;28;01mas\u001b[39;00m tfio\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\utils\\dataset_utils.py:10\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m backend\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tree\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mapi_export\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m keras_export\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m file_utils\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\tree\\__init__.py:1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_paths\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m assert_same_structure\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m flatten\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\tree\\tree_api.py:13\u001b[39m\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m torchtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m optree.available:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m optree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m dmtree.available:\n\u001b[32m     15\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01msrc\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtree\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m dmtree_impl \u001b[38;5;28;01mas\u001b[39;00m tree_impl\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python314\\Lib\\site-packages\\keras\\src\\tree\\optree_impl.py:13\u001b[39m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Register backend-specific node classes\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m backend() == \u001b[33m\"\u001b[39m\u001b[33mtensorflow\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ListWrapper\n\u001b[32m     14\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtensorflow\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpython\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mtrackable\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_structures\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _DictWrapper\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 5  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mae')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823eed9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d224be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab56b48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65da2fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fff34ab",
   "metadata": {},
   "source": [
    "# DEW POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results for dewpoint:\n",
      "MSE: 0.9249\n",
      "RMSE: 0.9617\n",
      "R² Score: 0.9573\n",
      "Average RMSE from CV: 0.9200\n",
      "Average R² from CV: 0.9582\n",
      "Avarage MSE: 0.8517048465957778\n",
      "Individual Fold RMSEs: [np.float64(1.0131271835534301), np.float64(0.8921609481786396), np.float64(0.9571135642972124), np.float64(0.7958769561519949), np.float64(0.941626189723659)]\n",
      "\n",
      " R2 ~ -0.0895\n",
      "\n",
      "Random Forest Feature Importances (from last fold):\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)    0.216409\n",
      "20                  mintem_roll_7    0.147318\n",
      "11                   mintem_lag_1    0.130031\n",
      "14                   mintem_lag_2    0.117017\n",
      "17                   mintem_lag_3    0.075768\n",
      "18                     atm_roll_7    0.059077\n",
      "1                        humidity    0.046999\n",
      "12                      atm_lag_2    0.037238\n",
      "0            atmospheric_pressure    0.030732\n",
      "9                       atm_lag_1    0.030342\n",
      "15                      atm_lag_3    0.029113\n",
      "10                      hum_lag_1    0.017414\n",
      "19                     dew_roll_7    0.016231\n",
      "5               day_of_year_sin_2    0.011840\n",
      "3               day_of_year_sin_1    0.007623\n",
      "4               day_of_year_cos_1    0.006831\n",
      "7               day_of_year_sin_3    0.005571\n",
      "13                      hum_lag_2    0.004986\n",
      "16                      hum_lag_3    0.003794\n",
      "6               day_of_year_cos_2    0.003199\n",
      "8               day_of_year_cos_3    0.002466\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "Random Forest Feature Importances (from last fold):\n",
    "                          Feature  Importance\n",
    "2   minimum_temperature(degree C)    0.216909\n",
    "20                  mintem_roll_7    0.146150\n",
    "11                   mintem_lag_1    0.131497\n",
    "14                   mintem_lag_2    0.122413\n",
    "17                   mintem_lag_3    0.073617\n",
    "18                     atm_roll_7    0.056844\n",
    "12                      atm_lag_2    0.039081\n",
    "0            atmospheric_pressure    0.035956\n",
    "4               day_of_year_cos_1    0.025367\n",
    "15                      atm_lag_3    0.025037\n",
    "9                       atm_lag_1    0.024415\n",
    "1                        humidity    0.024317\n",
    "7               day_of_year_sin_3    0.019913\n",
    "3               day_of_year_sin_1    0.010451\n",
    "19                     dew_roll_7    0.009862\n",
    "10                      hum_lag_1    0.009594\n",
    "5               day_of_year_sin_2    0.008322\n",
    "6               day_of_year_cos_2    0.007584\n",
    "13                      hum_lag_2    0.005335\n",
    "16                      hum_lag_3    0.004013\n",
    "8               day_of_year_cos_3    0.003323\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for dewpoint:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'R² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc11ef",
   "metadata": {},
   "source": [
    "# RF-LSTM hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-1.12 [best]\n",
    "#     # 'day_of_year_sin_2', #\n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abcf8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step\n",
      "Windowed UV R2: 0.9586\n",
      "Windowed UV MSE: 0.9026\n",
      "Windowed UV RMSE: 0.9500\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 5  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mae')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3b35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n",
      "--- HYBRID MODEL PERFORMANCE ---\n",
      "Final temperature R2: 0.9577\n",
      "Final temperature MSE: 0.9230\n",
      "Final temperature RMSE: 0.9230\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# --- STEP 1: Selective Features for LSTM ---\n",
    "# We only give the LSTM the most important \"weather\" features to reduce noise\n",
    "lstm_feature_cols = FEATURES\n",
    "X_train_slim = X_train[lstm_feature_cols]\n",
    "X_test_slim = X_test[lstm_feature_cols]\n",
    "\n",
    "scaler_slim = StandardScaler()\n",
    "X_train_scaled = scaler_slim.fit_transform(X_train_slim)\n",
    "X_test_scaled = scaler_slim.transform(X_test_slim)\n",
    "\n",
    "# --- STEP 2: Create Sequences ---\n",
    "window_size = 7 # Try a full week\n",
    "X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "\n",
    "# --- STEP 3: Optimized LSTM ---\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "    LSTM(100, activation='tanh', return_sequences=True), # Return sequences for deeper learning\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2) \n",
    "])\n",
    "\n",
    "# Use a slightly slower learning rate to find the pattern\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber') # Huber loss is great for weather outliers\n",
    "lstm_model.fit(X_train_seq, y_train_res_seq, epochs=60, batch_size=64, verbose=0)\n",
    "\n",
    "# 1. Align the Test Data (Skip the first 7 days used for the window)\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# 2. Get the \"Base\" predictions from your Random Forest\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# 3. Get the \"Corrections\" from the LSTM\n",
    "# X_test_seq was created during your sequence step\n",
    "lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# 4. Combine them: Base + Correction\n",
    "final_uv = rf_t_pred + lstm_corrections[:, 0]\n",
    "\n",
    "rf_lstm_r2 = r2_score(y_test_aligned, final_uv)\n",
    "rf_lstm_mse = mean_squared_error(y_test_aligned, final_uv)\n",
    "rf_lstm_rmse = np.sqrt(rf_lstm_mse)\n",
    "\n",
    "# 5. Output the New Results\n",
    "print(\"--- HYBRID MODEL PERFORMANCE ---\")\n",
    "print(f\"Final temperature R2: {rf_lstm_r2:.4f}\")\n",
    "print(f\"Final temperature MSE: {rf_lstm_mse:.4f}\")\n",
    "print(f\"Final temperature RMSE: {rf_lstm_mse:.4f}\")\n",
    "# final r2 90.90/89.87 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab773",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Processing Fold 1 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
      "Fold 1 R2: 0.9413\n",
      "Fold 1 MSE: 1.1284\n",
      "Fold 1 RMSE: 1.0623\n",
      "--- Processing Fold 2 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 2 R2: 0.9606\n",
      "Fold 2 MSE: 0.7915\n",
      "Fold 2 RMSE: 0.8897\n",
      "--- Processing Fold 3 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
      "Fold 3 R2: 0.9614\n",
      "Fold 3 MSE: 0.9067\n",
      "Fold 3 RMSE: 0.9522\n",
      "--- Processing Fold 4 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 4 R2: 0.9719\n",
      "Fold 4 MSE: 0.6224\n",
      "Fold 4 RMSE: 0.7889\n",
      "--- Processing Fold 5 ---\n",
      "\u001b[1m21/21\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
      "Fold 5 R2: 0.9514\n",
      "Fold 5 MSE: 0.8820\n",
      "Fold 5 RMSE: 0.9391\n",
      "\n",
      "--- FINAL CROSS-VALIDATION RESULTS ---\n",
      "Mean R2: 0.9573 (+/- 0.0103)\n",
      "Mean mse: 0.8662 (+/- 0.1647)\n",
      "Mean rmse: 0.9264 (+/- 0.0889)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Assuming X and y are your full datasets before the train/test split\n",
    "# X_full, y_full, rf_model, create_sequences need to be defined in your workspace\n",
    "\n",
    "DEW = 'dew_point'\n",
    "X_full = df[FEATURES]\n",
    "y_full = df[DEW]\n",
    "\n",
    "fold = 1\n",
    "lstmRf_hybrid_r2_scores = []\n",
    "lstmRf_hybrid_mse_scores = []\n",
    "lstmRf_hybrid_rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "\n",
    "    print(f\"--- Processing Fold {fold} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train_cv, X_test_cv = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    # --- STEP 1: Random Forest Base Model (Required for Residuals) ---\n",
    "\n",
    "    # Training the RF on the current fold's training set\n",
    "    rf_model.fit(X_train_cv, y_train_cv)\n",
    "    train_residuals = y_train_cv - rf_model.predict(X_train_cv)\n",
    "    \n",
    "    # --- STEP 2: Preprocessing for LSTM ---\n",
    "    lstm_feature_cols = FEATURES\n",
    "\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_cv[lstm_feature_cols])\n",
    "    X_test_scaled = scaler.transform(X_test_cv[lstm_feature_cols])\n",
    "    \n",
    "    # --- STEP 3: Create Sequences ---\n",
    "    window_size = 5\n",
    "    X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals.values, window_size)\n",
    "    # We pass zeros for y_test as we only need the X sequences for prediction\n",
    "    X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "    \n",
    "    # --- STEP 4: Train LSTM ---\n",
    "    # Re-initialize the model each fold to avoid weight leakage\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "        LSTM(100, activation='tanh', return_sequences=True),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber')\n",
    "    lstm_model.fit(X_train_seq, y_train_res_seq, epochs=30, batch_size=64, verbose=0)\n",
    "    \n",
    "    # --- STEP 5: Hybrid Prediction & Evaluation ---\n",
    "    # Align target data (drop first 'window_size' rows)\n",
    "    y_test_aligned = y_test_cv.iloc[window_size:]\n",
    "    rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n",
    "    \n",
    "    lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "    lstmRf_hybrid_prediction = rf_base_pred + lstm_corrections[:, 0] # Adjust index if target is multi-output\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_mse_kf = mean_squared_error(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_rmse_kf = np.sqrt(lstmRf_hybrid_mse_kf)\n",
    "\n",
    "    lstmRf_hybrid_r2_scores.append(lstmRf_hybrid_r2_kf)\n",
    "    lstmRf_hybrid_mse_scores.append(lstmRf_hybrid_mse_kf)\n",
    "    lstmRf_hybrid_rmse_scores.append(lstmRf_hybrid_rmse_kf)\n",
    "\n",
    "    print(f\"Fold {fold} R2: {lstmRf_hybrid_r2_kf:.4f}\")\n",
    "    print(f\"Fold {fold} MSE: {lstmRf_hybrid_mse_kf:.4f}\")\n",
    "    print(f\"Fold {fold} RMSE: {lstmRf_hybrid_rmse_kf:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "rf_lstm_r2_cv = np.mean(lstmRf_hybrid_r2_scores)\n",
    "rf_lstm_mse_cv = np.mean(lstmRf_hybrid_mse_scores)\n",
    "rf_lstm_rmse_cv = np.mean(lstmRf_hybrid_rmse_scores)\n",
    "\n",
    "print(\"\\n--- FINAL CROSS-VALIDATION RESULTS ---\")\n",
    "print(f\"Mean R2: {rf_lstm_r2_cv:.4f} (+/- {np.std(lstmRf_hybrid_r2_scores):.4f})\")\n",
    "print(f\"Mean mse: {rf_lstm_mse_cv:.4f} (+/- {np.std(lstmRf_hybrid_mse_scores):.4f})\")\n",
    "print(f\"Mean rmse: {rf_lstm_rmse_cv:.4f} (+/- {np.std(lstmRf_hybrid_rmse_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd03e8",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae46987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "XGBoost Results for dewpoint:\n",
      "Mean Squared Error: 0.5801\n",
      "RMSE: 0.7617\n",
      "R² Score: 0.9732\n",
      "\n",
      "\n",
      "Average RMSE from CV: 0.7178\n",
      "Average R² from CV: 0.9745\n",
      "Avarage MSE: 0.5874709703894918\n",
      "Individual Fold RMSEs: [np.float64(0.7681015564798639), np.float64(0.6808038175198191), np.float64(0.7640772895399975), np.float64(0.6093066031326871), np.float64(0.7664665487739774)]\n",
      "\n",
      " R2 ~ -0.1300\n",
      "\n",
      "XGBoost Feature Importances:\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)    0.846339\n",
      "20                  mintem_roll_7    0.067849\n",
      "1                        humidity    0.047366\n",
      "11                   mintem_lag_1    0.010126\n",
      "4               day_of_year_cos_1    0.004890\n",
      "7               day_of_year_sin_3    0.004740\n",
      "3               day_of_year_sin_1    0.001849\n",
      "0            atmospheric_pressure    0.001822\n",
      "19                     dew_roll_7    0.001741\n",
      "6               day_of_year_cos_2    0.001445\n",
      "18                     atm_roll_7    0.001438\n",
      "14                   mintem_lag_2    0.001344\n",
      "10                      hum_lag_1    0.001264\n",
      "5               day_of_year_sin_2    0.001141\n",
      "16                      hum_lag_3    0.001068\n",
      "15                      atm_lag_3    0.001026\n",
      "17                   mintem_lag_3    0.000954\n",
      "13                      hum_lag_2    0.000944\n",
      "12                      atm_lag_2    0.000917\n",
      "8               day_of_year_cos_3    0.000888\n",
      "9                       atm_lag_1    0.000850\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "'''\n",
    "\n",
    "XGBoost Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)    0.686006\n",
    "14                    temp_roll_7    0.137628\n",
    "6               day_of_year_sin_3    0.045227\n",
    "3               day_of_year_cos_1    0.018808\n",
    "4               day_of_year_sin_2    0.016560\n",
    "2               day_of_year_sin_1    0.016441\n",
    "15                     atm_roll_7    0.011273\n",
    "7               day_of_year_cos_3    0.010194\n",
    "8                      temp_lag_1    0.009055\n",
    "1            atmospheric_pressure    0.008524\n",
    "5               day_of_year_cos_2    0.007986\n",
    "9                       atm_lag_1    0.007748\n",
    "11                      atm_lag_2    0.006355\n",
    "12                     temp_lag_3    0.006175\n",
    "10                     temp_lag_2    0.006093\n",
    "13                      atm_lag_3    0.005929\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "xgb_model = {}\n",
    "y_preds_xgb = {}\n",
    "rmses_xgb = {}\n",
    "r2s_xgb = {}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Results for dewpoint:\")\n",
    "print(f'Mean Squared Error: {mse_xgb:.4f}')\n",
    "print(f'RMSE: {rmse_xgb:.4f}')\n",
    "print(f'R² Score: {r2_xgb:.4f}')\n",
    "\n",
    "\n",
    "# K-Fold cross-validation for XGBoost\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_xgb = []\n",
    "r2_list_xgb = [] # Added to track R2 across all folds\n",
    "mse_list_xgb = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        xgb_model_kf = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_xgb = xgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "\n",
    "        mse_kf_xgb = mean_squared_error(y_test_kf, y_pred_kf_xgb)\n",
    "        rmse_kf_xgb = np.sqrt(mse_kf_xgb)\n",
    "        r2_kf_xgb = r2_score(y_test_kf, y_pred_kf_xgb)\n",
    "\n",
    "        mse_list_xgb.append(mse_kf_xgb)\n",
    "        rmse_list_xgb.append(rmse_kf_xgb)\n",
    "        r2_list_xgb.append(r2_kf_xgb)\n",
    "\n",
    "\n",
    "        average_rmse_xgb = np.mean(rmse_list_xgb)\n",
    "        average_r2_xgb = np.mean(r2_list_xgb)\n",
    "        average_mse_xgb = np.mean(mse_kf_xgb)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_xgb:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_xgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_xgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_xgb}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "importance = xgb_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_xgb - average_r2_xgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(feature_importance_df_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235d76b",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8262d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46647cfa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Results for dewpont:\n",
      "Mean Squared Error: 0.5764\n",
      "RMSE: 0.7592\n",
      "R² Score: 0.9734\n",
      "\n",
      "\n",
      "Average RMSE from CV: 0.7648\n",
      "Average R² from CV: 0.9712\n",
      "Avarage MSE: 0.574130576082273\n",
      "Individual Fold RMSEs: [np.float64(0.8274440970627969), np.float64(0.7495333869488892), np.float64(0.8481463964472645), np.float64(0.6411106208658657), np.float64(0.7577140463804752)]\n",
      "\n",
      " R2 ~ 0.2208\n",
      "\n",
      "LightGBM Feature Importances:\n",
      "                          Feature  Importance\n",
      "1                        humidity         980\n",
      "2   minimum_temperature(degree C)         814\n",
      "10                      hum_lag_1         396\n",
      "20                  mintem_roll_7         270\n",
      "13                      hum_lag_2         228\n",
      "19                     dew_roll_7         228\n",
      "4               day_of_year_cos_1         223\n",
      "7               day_of_year_sin_3         196\n",
      "18                     atm_roll_7         184\n",
      "0            atmospheric_pressure         177\n",
      "11                   mintem_lag_1         171\n",
      "16                      hum_lag_3         150\n",
      "3               day_of_year_sin_1         144\n",
      "5               day_of_year_sin_2         144\n",
      "14                   mintem_lag_2         116\n",
      "15                      atm_lag_3         116\n",
      "8               day_of_year_cos_3         101\n",
      "17                   mintem_lag_3          97\n",
      "6               day_of_year_cos_2          85\n",
      "12                      atm_lag_2          84\n",
      "9                       atm_lag_1          83\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "''''\n",
    "LightGBM Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)         852\n",
    "14                    temp_roll_7         431\n",
    "8                      temp_lag_1         384\n",
    "15                     atm_roll_7         384\n",
    "6               day_of_year_sin_3         324\n",
    "4               day_of_year_sin_2         309\n",
    "9                       atm_lag_1         302\n",
    "1            atmospheric_pressure         291\n",
    "7               day_of_year_cos_3         277\n",
    "11                      atm_lag_2         261\n",
    "3               day_of_year_cos_1         260\n",
    "2               day_of_year_sin_1         234\n",
    "5               day_of_year_cos_2         234\n",
    "12                     temp_lag_3         192\n",
    "13                      atm_lag_3         172\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "lgb_model = {}\n",
    "y_preds_lgb = {}\n",
    "rmses_lgb = {}\n",
    "r2s_lgb = {}\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "        # n_estimators=800,\n",
    "        # learning_rate=0.01,\n",
    "        # max_depth=8,\n",
    "        # subsample=0.8,\n",
    "        # colsample_bytree=0.8,\n",
    "        # random_state=42,\n",
    "        # verbosity=-1\n",
    "\n",
    "        n_estimators=300,        # Reduced to prevent memorization as UV r 4k dataset\n",
    "        learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "        max_depth=6,             # Shallow trees are better for 4k rows\n",
    "        num_leaves=20,           # Controls complexity\n",
    "        min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "        subsample=0.7,           # More aggressive sampling for better generalization\n",
    "        colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "                              # Clean console\n",
    "    )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_lgb = np.sqrt(mse_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Results for dewpont:\")\n",
    "print(f'Mean Squared Error: {mse_lgb:.4f}')\n",
    "print(f'RMSE: {rmse_lgb:.4f}')\n",
    "print(f'R² Score: {r2_lgb:.4f}')\n",
    "\n",
    "\n",
    "    # --- 6) 5-fold CV R^2 ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "rmse_list_lgb = []\n",
    "r2_list_lgb = []\n",
    "mse_list_lgb = []\n",
    "\n",
    "\n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        lgb_model_kf = lgb.LGBMRegressor(\n",
    "            \n",
    "            n_estimators=300,        # Reduced to prevent memorization\n",
    "            learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "            max_depth=6,             # Shallow trees are better for 4k rows\n",
    "            num_leaves=20,           # Controls complexity\n",
    "            min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "            subsample=0.7,           # More aggressive sampling for better generalization\n",
    "            colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "            random_state=42,\n",
    "            verbosity=-1             # Clean console\n",
    "        )\n",
    "\n",
    "        lgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_lgb = lgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_lgb = mean_squared_error(y_test_kf, y_pred_kf_lgb)\n",
    "        rmse_kf_lgb = np.sqrt(mse_kf_lgb)\n",
    "        r2_kf_lgb = r2_score(y_test_kf, y_pred_kf_lgb)\n",
    "        \n",
    "        mse_list_lgb.append(mse_kf_lgb)\n",
    "        rmse_list_lgb.append(rmse_kf_lgb)\n",
    "        r2_list_lgb.append(r2_kf_lgb)\n",
    "\n",
    "        average_rmse_lgb = np.mean(rmse_list_lgb)\n",
    "        average_r2_lgb = np.mean(r2_list_lgb)\n",
    "        average_mse_lgb = np.mean(mse_kf_lgb)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_lgb:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_lgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_lgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lgb}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = lgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_lgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_lgb = feature_importance_df_lgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_lgb - average_r2_lgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(feature_importance_df_lgb)\n",
    "\n",
    "##### year wise analysis #####\n",
    "# 1980 to 2024 -> ~ 0.69 (44 years)\n",
    "# 2014 to 2024 -> ~ 0.82 (10 years)\n",
    "# 2017 to 2024 -> ~ 0.042 (7 years) **** yey 😂😂😂\n",
    "# 2018 to 2024 -> ~ 0.78 (6 years)\n",
    "# 2019 to 2024 -> ~ 1.15 (5 yrs)\n",
    "# 2021 to 2024 -> ~ 2.33 (3 yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14ec0",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7da7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca49131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdc53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LightGBM Results for dewpoint :\n",
      "Mean Squared Error: 0.5266\n",
      "RMSE: 0.7257\n",
      "R² Score: 0.9757\n",
      "\n",
      "\n",
      "Average RMSE from CV: 0.6957\n",
      "Average R² from CV: 0.9760\n",
      "Avarage MSE: 0.4872825728814568\n",
      "Individual Fold RMSEs: [np.float64(0.7650895782045696), np.float64(0.6827576794309387), np.float64(0.699375335766351), np.float64(0.5974869707553688), np.float64(0.7340137685407506)]\n",
      "\n",
      " R2 ~ -0.0311\n",
      "\n",
      "CatBoost Feature Importances:\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)   34.646273\n",
      "1                        humidity   26.699010\n",
      "20                  mintem_roll_7    8.421365\n",
      "11                   mintem_lag_1    6.740471\n",
      "7               day_of_year_sin_3    3.657639\n",
      "14                   mintem_lag_2    2.783425\n",
      "17                   mintem_lag_3    2.214900\n",
      "5               day_of_year_sin_2    1.815127\n",
      "18                     atm_roll_7    1.698234\n",
      "10                      hum_lag_1    1.351951\n",
      "19                     dew_roll_7    1.326244\n",
      "9                       atm_lag_1    1.274869\n",
      "12                      atm_lag_2    1.100552\n",
      "0            atmospheric_pressure    0.999728\n",
      "13                      hum_lag_2    0.920530\n",
      "3               day_of_year_sin_1    0.888419\n",
      "6               day_of_year_cos_2    0.788961\n",
      "16                      hum_lag_3    0.773105\n",
      "8               day_of_year_cos_3    0.683445\n",
      "4               day_of_year_cos_1    0.667335\n",
      "15                      atm_lag_3    0.548417\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "''''\n",
    "CatBoost Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)   43.960021\n",
    "8                      temp_lag_1   10.761866\n",
    "6               day_of_year_sin_3    7.732805\n",
    "14                    temp_roll_7    7.551957\n",
    "3               day_of_year_cos_1    4.006128\n",
    "2               day_of_year_sin_1    3.907999\n",
    "10                     temp_lag_2    3.767410\n",
    "4               day_of_year_sin_2    3.340417\n",
    "9                       atm_lag_1    2.673542\n",
    "12                     temp_lag_3    2.317698\n",
    "5               day_of_year_cos_2    2.278235\n",
    "1            atmospheric_pressure    1.917982\n",
    "15                     atm_roll_7    1.841164\n",
    "7               day_of_year_cos_3    1.651114\n",
    "13                      atm_lag_3    1.179426\n",
    "11                      atm_lag_2    1.112237\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "cat_model = {}\n",
    "y_preds_cat = {}\n",
    "rmses_cat = {}\n",
    "r2s_cat = {}\n",
    "\n",
    "# loss_function='RMSE' is standard for regression\n",
    "cat_model = CatBoostRegressor(\n",
    "        iterations=800,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "        bootstrap_type='Bayesian',\n",
    "        bagging_temperature=1,\n",
    "        random_strength=1,\n",
    "        loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "mse_cat = mean_squared_error(y_test, y_pred)\n",
    "rmse_cat = np.sqrt(mse_cat)\n",
    "r2_cat = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"\\nLightGBM Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_cat:.4f}')\n",
    "print(f'RMSE: {rmse_cat:.4f}')\n",
    "print(f'R² Score: {r2_cat:.4f}')\n",
    "\n",
    "\n",
    "    # 3. 5-Fold Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_cat = []\n",
    "r2_list_cat = []\n",
    "mse_list_cat = []\n",
    "    \n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        cat_model_kf = CatBoostRegressor(\n",
    "            iterations=800,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "            bootstrap_type='Bayesian',\n",
    "            bagging_temperature=1,\n",
    "            random_strength=1,\n",
    "            loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        cat_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_cat = cat_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_cat = mean_squared_error(y_test_kf, y_pred_kf_cat)\n",
    "        rmse_kf_cat = np.sqrt(mse_kf_cat)\n",
    "        r2_kf_cat = r2_score(y_test_kf, y_pred_kf_cat)\n",
    "\n",
    "        mse_list_cat.append(mse_kf_cat)\n",
    "        rmse_list_cat.append(rmse_kf_cat)\n",
    "        r2_list_cat.append(r2_kf_cat)\n",
    "\n",
    "        average_rmse_cat = np.mean(rmse_list_cat)\n",
    "        average_r2_cat = np.mean(r2_list_cat)\n",
    "        average_mse_cat = np.mean(mse_list_cat)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_cat:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_cat:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_cat}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_cat}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = cat_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_cat = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_cat = feature_importance_df_cat.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_cat - average_r2_cat)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nCatBoost Feature Importances:\")\n",
    "print(feature_importance_df_cat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a711e",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7603234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cccdf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 9ms/step - loss: 0.4036 - val_loss: 0.0698\n",
      "Epoch 2/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0732 - val_loss: 0.0494\n",
      "Epoch 3/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0592 - val_loss: 0.0369\n",
      "Epoch 4/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0488 - val_loss: 0.0296\n",
      "Epoch 5/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0441 - val_loss: 0.0279\n",
      "Epoch 6/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0413 - val_loss: 0.0258\n",
      "Epoch 7/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0378 - val_loss: 0.0221\n",
      "Epoch 8/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0361 - val_loss: 0.0247\n",
      "Epoch 9/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0356 - val_loss: 0.0243\n",
      "Epoch 10/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0343 - val_loss: 0.0232\n",
      "Epoch 11/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0309 - val_loss: 0.0212\n",
      "Epoch 12/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0298 - val_loss: 0.0224\n",
      "Epoch 13/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0292 - val_loss: 0.0203\n",
      "Epoch 14/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0285 - val_loss: 0.0188\n",
      "Epoch 15/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0285 - val_loss: 0.0196\n",
      "Epoch 16/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0281 - val_loss: 0.0234\n",
      "Epoch 17/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0270 - val_loss: 0.0194\n",
      "Epoch 18/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0267 - val_loss: 0.0207\n",
      "Epoch 19/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0264 - val_loss: 0.0168\n",
      "Epoch 20/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0267 - val_loss: 0.0186\n",
      "Epoch 21/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0261 - val_loss: 0.0182\n",
      "Epoch 22/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0245 - val_loss: 0.0217\n",
      "Epoch 23/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0246 - val_loss: 0.0192\n",
      "Epoch 24/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0235 - val_loss: 0.0201\n",
      "Epoch 25/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0239 - val_loss: 0.0187\n",
      "Epoch 26/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0244 - val_loss: 0.0168\n",
      "Epoch 27/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0237 - val_loss: 0.0204\n",
      "Epoch 28/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0236 - val_loss: 0.0225\n",
      "Epoch 29/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0230 - val_loss: 0.0190\n",
      "Epoch 30/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0235 - val_loss: 0.0166\n",
      "Epoch 31/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0225 - val_loss: 0.0245\n",
      "Epoch 32/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0225 - val_loss: 0.0198\n",
      "Epoch 33/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0220 - val_loss: 0.0185\n",
      "Epoch 34/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0230 - val_loss: 0.0206\n",
      "Epoch 35/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0224 - val_loss: 0.0189\n",
      "Epoch 36/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0213 - val_loss: 0.0208\n",
      "Epoch 37/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0219 - val_loss: 0.0161\n",
      "Epoch 38/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0220 - val_loss: 0.0199\n",
      "Epoch 39/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0214 - val_loss: 0.0175\n",
      "Epoch 40/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - val_loss: 0.0177\n",
      "Epoch 41/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - val_loss: 0.0200\n",
      "Epoch 42/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0216 - val_loss: 0.0230\n",
      "Epoch 43/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0208 - val_loss: 0.0179\n",
      "Epoch 44/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0206 - val_loss: 0.0203\n",
      "Epoch 45/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0218 - val_loss: 0.0194\n",
      "Epoch 46/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0202 - val_loss: 0.0173\n",
      "Epoch 47/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0207 - val_loss: 0.0180\n",
      "Epoch 48/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0207 - val_loss: 0.0219\n",
      "Epoch 49/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0197 - val_loss: 0.0152\n",
      "Epoch 50/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0196 - val_loss: 0.0195\n",
      "Epoch 51/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0196 - val_loss: 0.0199\n",
      "Epoch 52/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0196 - val_loss: 0.0169\n",
      "Epoch 53/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0207 - val_loss: 0.0280\n",
      "Epoch 54/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0193 - val_loss: 0.0151\n",
      "Epoch 55/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0197 - val_loss: 0.0215\n",
      "Epoch 56/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0197 - val_loss: 0.0197\n",
      "Epoch 57/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0188 - val_loss: 0.0197\n",
      "Epoch 58/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0192 - val_loss: 0.0201\n",
      "Epoch 59/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0195 - val_loss: 0.0192\n",
      "Epoch 60/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0189 - val_loss: 0.0224\n",
      "Epoch 61/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0192 - val_loss: 0.0199\n",
      "Epoch 62/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0192 - val_loss: 0.0182\n",
      "Epoch 63/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 64/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0210 - val_loss: 0.0182\n",
      "Epoch 65/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0188 - val_loss: 0.0189\n",
      "Epoch 66/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0178 - val_loss: 0.0211\n",
      "Epoch 67/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - val_loss: 0.0181\n",
      "Epoch 68/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0191 - val_loss: 0.0166\n",
      "Epoch 69/100\n",
      "\u001b[1m41/41\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0185 - val_loss: 0.0176\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "GRU Results for dewpoint :\n",
      "Mean Squared Error: 0.5713\n",
      "RMSE: 0.7559\n",
      "R² Score: 0.9736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "\n",
      "Average RMSE from CV: 0.6715\n",
      "Average R² from CV: 0.9778\n",
      "Average MSE: 0.4539\n",
      "Individual Fold RMSEs: [np.float64(0.6907915030277364), np.float64(0.6763904917548407), np.float64(0.7243966952498848), np.float64(0.5672626253598062), np.float64(0.6989034769401116)]\n",
      "\n",
      " R2 ~ -0.4154\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create an instance with specific parameters\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15,          # Wait 15 epochs for improvement before stopping\n",
    "    restore_best_weights=True  # Very important: keeps the best version of your model\n",
    ")\n",
    "\n",
    "# 1. Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for GRU: (samples, time_steps, features)\n",
    "# Here we use time_steps=1. If you want sequences, you'd need a sliding window function.\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split data (matching your non-shuffle 80/20 split)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train initial model\n",
    "gru_model = build_gru((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_gru = mean_squared_error(y_test_unscaled, y_pred)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_unscaled, y_pred)\n",
    "\n",
    "print(f\"\\nGRU Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'RMSE: {rmse_gru:.4f}')\n",
    "print(f'R² Score: {r2_gru:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild/Reset model for each fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_gru = np.mean(r2_list_gru)\n",
    "average_mse_gru = np.mean(mse_list_gru)\n",
    "average_rmse_gru = np.mean(rmse_list_gru)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_gru:.4f}\")\n",
    "print(f\"Average R² from CV: {average_r2_gru:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_gru:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_gru}\")\n",
    "\n",
    "diff = (r2_gru - np.mean(r2_list_gru))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40631911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRU Permutation Feature Importances:\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)    9.580473\n",
      "1                        humidity    5.578181\n",
      "11                   mintem_lag_1    0.353980\n",
      "10                      hum_lag_1    0.256608\n",
      "0            atmospheric_pressure    0.211829\n",
      "9                       atm_lag_1    0.190588\n",
      "3               day_of_year_sin_1    0.145599\n",
      "4               day_of_year_cos_1    0.143081\n",
      "5               day_of_year_sin_2    0.112858\n",
      "14                   mintem_lag_2    0.083270\n",
      "12                      atm_lag_2    0.082693\n",
      "15                      atm_lag_3    0.082049\n",
      "6               day_of_year_cos_2    0.069078\n",
      "13                      hum_lag_2    0.063270\n",
      "19                     dew_roll_7    0.055099\n",
      "7               day_of_year_sin_3    0.048927\n",
      "17                   mintem_lag_3    0.041781\n",
      "18                     atm_roll_7    0.034959\n",
      "16                      hum_lag_3    0.025115\n",
      "8               day_of_year_cos_3    0.004964\n",
      "20                  mintem_roll_7    0.000000\n"
     ]
    }
   ],
   "source": [
    "# Permutation Importance Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculates importance by measuring how much the MSE increases \n",
    "    when a single feature is randomly shuffled.\n",
    "    \"\"\"\n",
    "    # Baseline prediction\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    baseline_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                     scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X_val.shape[2]):  # Iterate through each feature\n",
    "        save = X_val[:, :, i].copy()\n",
    "        \n",
    "        # Shuffle the current feature across all samples\n",
    "        np.random.shuffle(X_val[:, :, i])\n",
    "        \n",
    "        # Predict with shuffled feature\n",
    "        shuffled_preds = model.predict(X_val, verbose=0)\n",
    "        shuffled_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                         scaler_y.inverse_transform(shuffled_preds))\n",
    "        \n",
    "        # Importance is the increase in error\n",
    "        importances.append(max(0, shuffled_mse - baseline_mse))\n",
    "        \n",
    "        # Restore the original feature values\n",
    "        X_val[:, :, i] = save\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    return importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Execute ---\n",
    "# Note: Use your X_test and y_test from the previous step\n",
    "feature_importance_gru = calculate_permutation_importance(\n",
    "    gru_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGRU Permutation Feature Importances:\")\n",
    "print(feature_importance_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c87fc",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899dd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "\n",
      "LSTM Results for dewpoint :\n",
      "Mean Squared Error: 0.5075\n",
      "RMSE: 0.7124\n",
      "R² Score: 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\rnn\\rnn.py:199: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "\n",
      "\n",
      "Average RMSE from LSTM CV: 0.6856\n",
      "Average R² from LSTM CV: 0.9765\n",
      "Average MSE: 0.4747\n",
      "Individual Fold RMSEs: [np.float64(0.771537253327876), np.float64(0.6593466596716631), np.float64(0.6640529841416938), np.float64(0.5840070892409674), np.float64(0.74915206186084)]\n",
      "\n",
      " R2 ~ 0.0048\n",
      "\n",
      "LSTM Permutation Feature Importances:\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)   11.187425\n",
      "1                        humidity    5.407693\n",
      "0            atmospheric_pressure    0.432645\n",
      "11                   mintem_lag_1    0.337832\n",
      "7               day_of_year_sin_3    0.152239\n",
      "6               day_of_year_cos_2    0.129973\n",
      "19                     dew_roll_7    0.107944\n",
      "12                      atm_lag_2    0.106737\n",
      "5               day_of_year_sin_2    0.084999\n",
      "4               day_of_year_cos_1    0.081569\n",
      "10                      hum_lag_1    0.070541\n",
      "3               day_of_year_sin_1    0.062296\n",
      "20                  mintem_roll_7    0.054372\n",
      "9                       atm_lag_1    0.052017\n",
      "13                      hum_lag_2    0.041632\n",
      "18                     atm_roll_7    0.029076\n",
      "14                   mintem_lag_2    0.026630\n",
      "8               day_of_year_cos_3    0.011659\n",
      "16                      hum_lag_3    0.008397\n",
      "15                      atm_lag_3    0.000000\n",
      "17                   mintem_lag_3    0.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for LSTM: [samples, time_steps, features]\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split matching your CatBoost logic (shuffle=False)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "lstm_model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_actual, y_pred_lstm)\n",
    "\n",
    "print(f\"\\nLSTM Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_lstm:.4f}')    \n",
    "print(f'RMSE: {rmse_lstm:.4f}')\n",
    "print(f'R² Score: {r2_lstm:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_lstm = []\n",
    "r2_list_lstm = []\n",
    "mse_list_lstm = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    lstm_kf = build_lstm((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    lstm_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = lstm_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_lstm.append(np.sqrt(mse_kf))\n",
    "    mse_list_lstm.append(mse_kf)\n",
    "    r2_list_lstm.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "\n",
    "average_r2_lstm = np.mean(r2_list_lstm)\n",
    "average_mse_lstm = np.mean(mse_list_lstm)\n",
    "average_rmse_lstm = np.mean(rmse_list_lstm)\n",
    " \n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from LSTM CV: {average_rmse_lstm:.4f}\")\n",
    "print(f\"Average R² from LSTM CV: { average_r2_lstm:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_lstm:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lstm}\")\n",
    "\n",
    "def calculate_lstm_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Computes permutation importance for a trained LSTM model.\n",
    "    \"\"\"\n",
    "    # 1. Get baseline score (Inverse scale to get real-world MSE)\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    \n",
    "    # Iterate through each feature index\n",
    "    for i in range(X_val.shape[2]):\n",
    "        # Create a copy to avoid permanent shuffling\n",
    "        X_permuted = X_val.copy()\n",
    "        \n",
    "        # 2. Shuffle the specific feature across all samples\n",
    "        # Shuffling happens across the 'samples' dimension for the i-th feature\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        \n",
    "        # 3. Predict with the permuted feature\n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        # 4. Importance = Increase in Error (shuffled error - baseline error)\n",
    "        importance = max(0, permuted_mse - baseline_mse)\n",
    "        importance_results.append(importance)\n",
    "\n",
    "    # Organize into a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names, \n",
    "        'Importance': importance_results\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# --- Execution ---\n",
    "# Using the X_test and y_test from your LSTM training\n",
    "lstm_importance_df = calculate_lstm_permutation_importance(\n",
    "    lstm_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "diff = (r2_lstm - np.mean(r2_list_lstm))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLSTM Permutation Feature Importances:\")\n",
    "print(lstm_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35fd5c",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483cd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "ANN Results for dew :\n",
      "Mean Squared Error: 1.1402\n",
      "RMSE: 1.0678\n",
      "R² Score: 0.9474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m22/22\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step \n",
      "\n",
      "\n",
      "Average RMSE from ANN CV: 0.8447\n",
      "Average R² from ANN CV: 0.9647\n",
      "Average MSE: 0.7278\n",
      "Individual Fold RMSEs: [np.float64(0.7975167259175522), np.float64(0.7278047203194365), np.float64(1.026185963644703), np.float64(0.7292707910114785), np.float64(0.9425465511857174)]\n",
      "\n",
      " R2 ~ -1.7299\n",
      "\n",
      "ANN Permutation Feature Importances:\n",
      "                          Feature  Importance\n",
      "2   minimum_temperature(degree C)    8.307318\n",
      "1                        humidity    5.449028\n",
      "5               day_of_year_sin_2    0.405231\n",
      "11                   mintem_lag_1    0.390728\n",
      "0            atmospheric_pressure    0.277333\n",
      "6               day_of_year_cos_2    0.181384\n",
      "12                      atm_lag_2    0.136323\n",
      "19                     dew_roll_7    0.115909\n",
      "7               day_of_year_sin_3    0.113648\n",
      "13                      hum_lag_2    0.025714\n",
      "10                      hum_lag_1    0.013388\n",
      "9                       atm_lag_1    0.012289\n",
      "3               day_of_year_sin_1    0.000000\n",
      "4               day_of_year_cos_1    0.000000\n",
      "8               day_of_year_cos_3    0.000000\n",
      "15                      atm_lag_3    0.000000\n",
      "14                   mintem_lag_2    0.000000\n",
      "16                      hum_lag_3    0.000000\n",
      "17                   mintem_lag_3    0.000000\n",
      "18                     atm_roll_7    0.000000\n",
      "20                  mintem_roll_7    0.000000\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X_ann = StandardScaler()\n",
    "scaler_y_ann = StandardScaler()\n",
    "\n",
    "X_scaled_ann = scaler_X_ann.fit_transform(X)\n",
    "y_scaled_ann = scaler_y_ann.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split matching your CatBoost logic (80/20, shuffle=False)\n",
    "split_idx = int(len(X_scaled_ann) * 0.8)\n",
    "X_train_ann, X_test_ann = X_scaled_ann[:split_idx], X_scaled_ann[split_idx:]\n",
    "y_train_ann, y_test_ann = y_scaled_ann[:split_idx], y_scaled_ann[split_idx:]\n",
    "\n",
    "# build model \n",
    "\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Linear output for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "ann_model = build_ann(X_train_ann.shape[1])\n",
    "ann_model.fit(X_train_ann, y_train_ann, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled_ann = ann_model.predict(X_test_ann)\n",
    "y_pred_ann = scaler_y_ann.inverse_transform(y_pred_scaled_ann)\n",
    "y_test_actual = scaler_y_ann.inverse_transform(y_test_ann)\n",
    "\n",
    "mse_ann = mean_squared_error(y_test_actual, y_pred_ann)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "r2_ann = r2_score(y_test_actual, y_pred_ann)\n",
    "\n",
    "print(f\"\\nANN Results for dew :\")\n",
    "print(f'Mean Squared Error: {mse_ann:.4f}')\n",
    "print(f'RMSE: {rmse_ann:.4f}')\n",
    "print(f'R² Score: {r2_ann:.4f}')\n",
    "\n",
    "# CV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_ann = []\n",
    "r2_list_ann = []\n",
    "mse_list_ann = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled_ann):\n",
    "    X_train_kf, X_test_kf = X_scaled_ann[train_index], X_scaled_ann[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled_ann[train_index], y_scaled_ann[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    ann_kf = build_ann(X_train_kf.shape[1])\n",
    "    ann_kf.fit(X_train_kf, y_train_kf, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = ann_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y_ann.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y_ann.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_ann.append(np.sqrt(mse_kf))\n",
    "    mse_list_ann.append(mse_kf)\n",
    "    r2_list_ann.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_ann = np.mean(r2_list_ann)\n",
    "average_mse_ann = np.mean(mse_list_ann)\n",
    "average_rmse_ann = np.mean(rmse_list_ann)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from ANN CV: {average_rmse_ann:.4f}\")\n",
    "print(f\"Average R² from ANN CV: {average_r2_ann:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_ann:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_ann}\")\n",
    "\n",
    "# importance\n",
    "def calculate_ann_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    for i in range(X_val.shape[1]): # Iterate through 2D features\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        importance_results.append(max(0, permuted_mse - baseline_mse))\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance_results}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "ann_importance_df = calculate_ann_permutation_importance(ann_model, X_test_ann, y_test_ann, scaler_y_ann, FEATURES)\n",
    "\n",
    "diff = (r2_ann - np.mean(r2_list_ann))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nANN Permutation Feature Importances:\")\n",
    "print(ann_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa81501",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune CNN Filters\n",
    "    hp_filters = hp.Int('filters', min_value=32, max_value=64, step=32)\n",
    "    model.add(Conv1D(filters=hp_filters, kernel_size=3, activation='relu', input_shape=(100,1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Tune Dropout\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Tune LSTM Units\n",
    "    hp_lstm_units = hp.Int('lstm_units', min_value=32, max_value=64, step=16)\n",
    "    model.add(LSTM(units=hp_lstm_units, activation='tanh'))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Final Dense Layers\n",
    "    model.add(Dense(hp.Int('dense_units', 16, 64, 16), activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Tune Learning Rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example: Using the last 30 days to predict tomorrow\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# X_3D shape will be (Total_Days, 30, 1)\n",
    "X_3D, y_target = create_sequences(df['dew_point'].values)\n",
    "X_3D = X_3D.reshape((X_3D.shape[0], X_3D.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n",
      "Base Results: R2: 0.8709, MSE: 2.8143, RMSE: 1.6776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python313\\Lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:113: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step\n",
      "5-Fold CV Average: R2: 0.8832, MSE: 2.4665, RMSE: 1.5266\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# 1. Prepare 3D Data (Samples, Time Steps, Features)\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_hybrid(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. CNN Stage: Extracts spatial/local patterns from the window\n",
    "        # Reducing filters to 32 is often better for ~4k rows to prevent noise capture\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100,1)),\n",
    "        BatchNormalization(), # Stabilizes learning and speeds up convergence\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2), # Standard regularization\n",
    "\n",
    "        # 2. LSTM Stage: Learns temporal dependencies\n",
    "        # tanh is the standard and most stable activation for LSTM\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        # 3. Fully Connected Stage\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1) # Output for UV prediction\n",
    "    ])\n",
    "    \n",
    "    # Using a slightly lower learning rate (0.0005) helps with smaller datasets\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Data\n",
    "window = 30\n",
    "data_values = df['dew_point'].values.reshape(-1, 1)\n",
    "X, y = create_sequences(data_values, window)\n",
    "\n",
    "# --- BASE PERFORMANCE ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "model = build_hybrid((window, 1))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, y_pred)\n",
    "base_rmse = np.sqrt(base_mse)\n",
    "base_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Base Results: R2: {base_r2:.4f}, MSE: {base_mse:.4f}, RMSE: {base_rmse:.4f}\")\n",
    "\n",
    "# --- 5-FOLD CROSS VALIDATION ---\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_r2, cv_mse, cv_rmse = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    m = build_hybrid((window, 1))\n",
    "    m.fit(X[train_idx], y[train_idx], epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    p = m.predict(X[test_idx])\n",
    "    cv_r2.append(r2_score(y[test_idx], p))\n",
    "    cv_mse.append(mean_squared_error(y[test_idx], p))\n",
    "    cv_rmse.append(np.sqrt(cv_mse[-1]))\n",
    "\n",
    "\n",
    "cnn_lstm_r2_cv = np.mean(cv_r2)\n",
    "cnn_lstm_mse_cv = np.mean(cv_mse)\n",
    "cnn_lstm_rmse_cv = np.mean(cv_rmse)\n",
    "\n",
    "print(f\"5-Fold CV Average: R2: {cnn_lstm_r2_cv:.4f}, MSE: {cnn_lstm_mse_cv:.4f}, RMSE: {cnn_lstm_rmse_cv:.4f}\")\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60abe20",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58428e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Model       R^2      CVR2   R2 DIFF       MSE    MSE CV  \\\n",
      "7              ANN  0.947407  0.964706 -0.017299  1.140215  0.727804   \n",
      "8  CNN-LSTM hybrid  0.870916  0.883188 -0.012272  2.814331  2.466508   \n",
      "5              GRU  0.973646  0.977800 -0.004154  0.571349  0.453940   \n",
      "2          XGBoost  0.973241  0.974541 -0.001300  0.580126  0.587471   \n",
      "0    Random Forest  0.957339  0.958234 -0.000895  0.924888  0.851705   \n",
      "4         CatBoost  0.975712  0.976023 -0.000311  0.526571  0.487283   \n",
      "6             LSTM  0.976591  0.976543  0.000048  0.507500  0.474653   \n",
      "1   RF-LSTM hybrid  0.957695  0.957331  0.000364  0.922968  0.866189   \n",
      "3         LightGBM  0.973413  0.971205  0.002208  0.576415  0.574131   \n",
      "\n",
      "       RMSE   RMSE CV  \n",
      "7  1.067808  0.844665  \n",
      "8  1.677597  1.526641  \n",
      "5  0.755876  0.671549  \n",
      "2  0.761660  0.717751  \n",
      "0  0.961711  0.919981  \n",
      "4  0.725652  0.695745  \n",
      "6  0.712391  0.685619  \n",
      "1  0.960712  0.926434  \n",
      "3  0.759220  0.764790  \n"
     ]
    }
   ],
   "source": [
    "# model performance chart \n",
    "MODEL_NAMES = [\"Random Forest\", \"RF-LSTM hybrid\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"GRU\", \"LSTM\", \"ANN\", \"CNN-LSTM hybrid\"]\n",
    "\n",
    "R_SQUARED_VALUES = [r2_rf, rf_lstm_r2, r2_xgb, r2_lgb, r2_cat, r2_gru, r2_lstm, r2_ann, base_r2 ]\n",
    "R2CV = [average_r2_rf, rf_lstm_r2_cv, average_r2_xgb, average_r2_lgb, average_r2_cat, average_r2_gru, average_r2_lstm, average_r2_ann, cnn_lstm_r2_cv ]\n",
    "\n",
    "R2_DIFF = [\n",
    "    (r2_rf - average_r2_rf), \n",
    "    (rf_lstm_r2 - rf_lstm_r2_cv), \n",
    "    (r2_xgb - average_r2_xgb), \n",
    "    (r2_lgb - average_r2_lgb), \n",
    "    (r2_cat - average_r2_cat), \n",
    "    (r2_gru - average_r2_gru), \n",
    "    (r2_lstm - average_r2_lstm), \n",
    "    (r2_ann - average_r2_ann), \n",
    "    (base_r2 - cnn_lstm_r2_cv)\n",
    "]\n",
    "\n",
    "MSE_VALUES = [mse_rf, rf_lstm_mse, mse_xgb, mse_lgb, mse_cat, mse_gru, mse_lstm, mse_ann, base_mse ]\n",
    "MSE_CV = [average_mse_rf, rf_lstm_mse_cv, average_mse_xgb, average_mse_lgb, average_mse_cat, average_mse_gru, average_mse_lstm, average_mse_ann, cnn_lstm_mse_cv ]\n",
    "\n",
    "RMSE_VALUES = [rmse_rf, rf_lstm_rmse, rmse_xgb, rmse_lgb, rmse_cat, rmse_gru, rmse_lstm, rmse_ann , base_rmse ]\n",
    "RMSE_CV = [average_rmse_rf, rf_lstm_rmse_cv, average_rmse_xgb, average_rmse_lgb, average_rmse_cat, average_rmse_gru, average_rmse_lstm, average_rmse_ann, cnn_lstm_rmse_cv]\n",
    "\n",
    "data = {\n",
    "    \"Model\": MODEL_NAMES,\n",
    "    \"R^2\": R_SQUARED_VALUES,\n",
    "    \"CVR2\": R2CV,\n",
    "    \"R2 DIFF\": R2_DIFF,\n",
    "    \"MSE\": MSE_VALUES,\n",
    "    \"MSE CV\": MSE_CV,\n",
    "    \"RMSE\": RMSE_VALUES,\n",
    "    \"RMSE CV\": RMSE_CV\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(data).sort_values(by=[\"R2 DIFF\",\"R^2\"], ascending= [True, True])\n",
    "\n",
    "print (df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433b50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Best Logical Model is: LSTM\n",
      "--- Reason: Balanced score across R^2 (0.9766) and Stability (DIFF: 0.0000)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define weights for your 'Best Logical Model' criteria\n",
    "# We want high R^2, low RMSE, and low R2 DIFF (stability)\n",
    "weights = {\n",
    "    'R^2': 0.4,       # Predictive power\n",
    "    'MSE': 0.3,\n",
    "    'RMSE': 0.3,     # Magnitude of error\n",
    "    'R2 DIFF': 0.3    # Robustness/Generalization\n",
    "}\n",
    "\n",
    "# 2. Create a Ranking Score (Lower is better)\n",
    "# .rank(ascending=False) means highest value gets rank 1\n",
    "# .rank(ascending=True) means lowest value gets rank 1\n",
    "df_performance['Score'] = (\n",
    "    df_performance['R^2'].rank(ascending=False) * weights['R^2'] +\n",
    "    df_performance['MSE'].rank(ascending=True) * weights['MSE'] +\n",
    "    df_performance['RMSE'].rank(ascending=True) * weights['RMSE']  +\n",
    "    df_performance['R2 DIFF'].rank(ascending=True) * weights['R2 DIFF']\n",
    ")\n",
    "\n",
    "# 3. Extract the winner\n",
    "best_logical_model = df_performance.loc[df_performance['Score'].idxmin()]\n",
    "\n",
    "print(f\"The Best Logical Model is: {best_logical_model['Model']}\")\n",
    "print(f\"--- Reason: Balanced score across R^2 ({best_logical_model['R^2']:.4f}) \"\n",
    "      f\"and Stability (DIFF: {best_logical_model['R2 DIFF']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea13b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
