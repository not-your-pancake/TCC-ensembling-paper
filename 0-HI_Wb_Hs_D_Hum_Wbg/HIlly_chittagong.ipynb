{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "727c041d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/Chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac7654b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a correlation matrix\n",
    "corr_matrix = df_chittagong.corr()\n",
    "\n",
    "# Plot the correlation matrix\n",
    "plt.figure(figsize=(14, 12))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "plt.title('Chittagong Correlation test')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c028b4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_chittagong.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae507e04",
   "metadata": {},
   "source": [
    "Temperature\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf24d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if 'date' not in df_chittagong.columns:\n",
    "    df_chittagong['date'] = pd.to_datetime(df_chittagong[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "\n",
    "min_date = df_chittagong['date'].min()\n",
    "max_date = df_chittagong['date'].max()\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(df_chittagong['date'], df_chittagong['temperature(degree C)'], linewidth=0.2)\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Temperature (Â°C)')\n",
    "plt.title('Temperature (chittagong)')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42609019",
   "metadata": {},
   "source": [
    "# RF with lagging rolling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc36f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','dew_point' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\n",
    "    'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "]\n",
    "\n",
    "\n",
    "#FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols \n",
    "#0.8640  0.8793 ~ 1.53\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef7e5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "\n",
    "Random Forest Feature Importances (from last fold):\n",
    "                 Feature  Importance\n",
    "15            dew_roll_7    0.194442\n",
    "1              dew_point    0.167315\n",
    "9              dew_lag_1    0.160662\n",
    "11             dew_lag_2    0.118672\n",
    "13             dew_lag_3    0.075158\n",
    "14            atm_roll_7    0.054073\n",
    "6      day_of_year_sin_3    0.041480\n",
    "3      day_of_year_cos_1    0.029418\n",
    "0   atmospheric_pressure    0.028322\n",
    "10             atm_lag_2    0.024513\n",
    "8              atm_lag_1    0.024098\n",
    "2      day_of_year_sin_1    0.019313\n",
    "12             atm_lag_3    0.019240\n",
    "4      day_of_year_sin_2    0.018426\n",
    "5      day_of_year_cos_2    0.016615\n",
    "7      day_of_year_cos_3    0.008253\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for temperature:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'RÂ² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3169cc72",
   "metadata": {},
   "source": [
    "#RF -LSTM HYBRID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994e75cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "FEATURES = [\n",
    "    'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "]\n",
    "\n",
    "\n",
    "#FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols \n",
    "#0.8640  0.8793 ~ 1.53\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 5  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mae')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f823eed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# --- STEP 1: Selective Features for LSTM ---\n",
    "# We only give the LSTM the most important \"weather\" features to reduce noise\n",
    "lstm_feature_cols = [\n",
    "    'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "]\n",
    "X_train_slim = X_train[lstm_feature_cols]\n",
    "X_test_slim = X_test[lstm_feature_cols]\n",
    "\n",
    "scaler_slim = StandardScaler()\n",
    "X_train_scaled = scaler_slim.fit_transform(X_train_slim)\n",
    "X_test_scaled = scaler_slim.transform(X_test_slim)\n",
    "\n",
    "# --- STEP 2: Create Sequences ---\n",
    "window_size = 7 # Try a full week\n",
    "X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "\n",
    "# --- STEP 3: Optimized LSTM ---\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "    LSTM(100, activation='tanh', return_sequences=True), # Return sequences for deeper learning\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2) \n",
    "])\n",
    "\n",
    "# Use a slightly slower learning rate to find the pattern\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber') # Huber loss is great for weather outliers\n",
    "lstm_model.fit(X_train_seq, y_train_res_seq, epochs=60, batch_size=64, verbose=0)\n",
    "\n",
    "# 1. Align the Test Data (Skip the first 7 days used for the window)\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# 2. Get the \"Base\" predictions from your Random Forest\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# 3. Get the \"Corrections\" from the LSTM\n",
    "# X_test_seq was created during your sequence step\n",
    "lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# 4. Combine them: Base + Correction\n",
    "final_uv = rf_t_pred + lstm_corrections[:, 0]\n",
    "\n",
    "rf_lstm_r2 = r2_score(y_test_aligned, final_uv)\n",
    "rf_lstm_mse = mean_squared_error(y_test_aligned, final_uv)\n",
    "rf_lstm_rmse = np.sqrt(rf_lstm_mse)\n",
    "\n",
    "# 5. Output the New Results\n",
    "print(\"--- HYBRID MODEL PERFORMANCE ---\")\n",
    "print(f\"Final temperature R2: {rf_lstm_r2:.4f}\")\n",
    "print(f\"Final temperature MSE: {rf_lstm_mse:.4f}\")\n",
    "print(f\"Final temperature RMSE: {rf_lstm_mse:.4f}\")\n",
    "# final r2 90.90/89.87 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d224be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Assuming X and y are your full datasets before the train/test split\n",
    "# X_full, y_full, rf_model, create_sequences need to be defined in your workspace\n",
    "\n",
    "TEMP = 'temperature(degree C)'\n",
    "X_full = df[FEATURES]\n",
    "y_full = df[TEMP]\n",
    "\n",
    "fold = 1\n",
    "lstmRf_hybrid_r2_scores = []\n",
    "lstmRf_hybrid_mse_scores = []\n",
    "lstmRf_hybrid_rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "\n",
    "    print(f\"--- Processing Fold {fold} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train_cv, X_test_cv = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    # --- STEP 1: Random Forest Base Model (Required for Residuals) ---\n",
    "\n",
    "    # Training the RF on the current fold's training set\n",
    "    rf_model.fit(X_train_cv, y_train_cv)\n",
    "    train_residuals = y_train_cv - rf_model.predict(X_train_cv)\n",
    "    \n",
    "    # --- STEP 2: Preprocessing for LSTM ---\n",
    "    lstm_feature_cols = [\n",
    "       'dew_roll_7',\n",
    "    'dew_point',\n",
    "    'dew_lag_1',\n",
    "    'dew_lag_2',\n",
    "    'dew_lag_3',\n",
    "    'atm_roll_7',\n",
    "    'day_of_year_sin_3',\n",
    "    'day_of_year_cos_1',\n",
    "    'atmospheric_pressure', # ~ 1.7\n",
    "    'atm_lag_2',  # 1.9\n",
    "    'atm_lag_1',\n",
    "    'day_of_year_sin_1',  # 0.8634 - 0.8739 - 1.5851\n",
    "    #'atm_lag_3', # 0.863 - 0.879 ` -1.5`\n",
    "    #'day_of_year_sin_2', #0.86/0.87 ~ -1.7\n",
    "   # 'day_of_year_cos_2',\n",
    "   # 'day_of_year_cos_3'\n",
    "    ]\n",
    "\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_cv[lstm_feature_cols])\n",
    "    X_test_scaled = scaler.transform(X_test_cv[lstm_feature_cols])\n",
    "    \n",
    "    # --- STEP 3: Create Sequences ---\n",
    "    window_size = 5\n",
    "    X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals.values, window_size)\n",
    "    # We pass zeros for y_test as we only need the X sequences for prediction\n",
    "    X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "    \n",
    "    # --- STEP 4: Train LSTM ---\n",
    "    # Re-initialize the model each fold to avoid weight leakage\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "        LSTM(100, activation='tanh', return_sequences=True),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber')\n",
    "    lstm_model.fit(X_train_seq, y_train_res_seq, epochs=30, batch_size=64, verbose=0)\n",
    "    \n",
    "    # --- STEP 5: Hybrid Prediction & Evaluation ---\n",
    "    # Align target data (drop first 'window_size' rows)\n",
    "    y_test_aligned = y_test_cv.iloc[window_size:]\n",
    "    rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n",
    "    \n",
    "    lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "    lstmRf_hybrid_prediction = rf_base_pred + lstm_corrections[:, 0] # Adjust index if target is multi-output\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_mse_kf = mean_squared_error(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_rmse_kf = np.sqrt(lstmRf_hybrid_mse_kf)\n",
    "\n",
    "    lstmRf_hybrid_r2_scores.append(lstmRf_hybrid_r2_kf)\n",
    "    lstmRf_hybrid_mse_scores.append(lstmRf_hybrid_mse_kf)\n",
    "    lstmRf_hybrid_rmse_scores.append(lstmRf_hybrid_rmse_kf)\n",
    "\n",
    "    print(f\"Fold {fold} R2: {lstmRf_hybrid_r2_kf:.4f}\")\n",
    "    print(f\"Fold {fold} MSE: {lstmRf_hybrid_mse_kf:.4f}\")\n",
    "    print(f\"Fold {fold} RMSE: {lstmRf_hybrid_rmse_kf:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "rf_lstm_r2_cv = np.mean(lstmRf_hybrid_r2_scores)\n",
    "rf_lstm_mse_cv = np.mean(lstmRf_hybrid_mse_scores)\n",
    "rf_lstm_rmse_cv = np.mean(lstmRf_hybrid_rmse_scores)\n",
    "\n",
    "print(\"\\n--- FINAL CROSS-VALIDATION RESULTS ---\")\n",
    "print(f\"Mean R2: {rf_lstm_r2_cv:.4f} (+/- {np.std(lstmRf_hybrid_r2_scores):.4f})\")\n",
    "print(f\"Mean mse: {rf_lstm_mse_cv:.4f} (+/- {np.std(lstmRf_hybrid_mse_scores):.4f})\")\n",
    "print(f\"Mean rmse: {rf_lstm_rmse_cv:.4f} (+/- {np.std(lstmRf_hybrid_rmse_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fec0d55",
   "metadata": {},
   "source": [
    "#XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab56b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','dew point' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = [ 'atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "'dew_roll_7',\n",
    "'day_of_year_sin_3',\n",
    "'dew_point',\n",
    "'day_of_year_cos_1',\n",
    "'day_of_year_sin_1',\n",
    "'day_of_year_sin_2',\n",
    "'dew_lag_1',\n",
    "'atm_roll_7',\n",
    "'day_of_year_cos_3',\n",
    "'day_of_year_cos_2',\n",
    "'dew_lag_2',\n",
    "'atm_lag_2',\n",
    "'atm_lag_1',\n",
    "#'dew_lag_3', # 0.8598/0.8665 ~0.66\n",
    "#'atmospheric_pressure',\n",
    "#'atm_lag_3' #0.854/0.867 ~1.2\n",
    "# #0.85/0.86 ~1.1\n",
    "\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 0.84/0.86 ~1.7\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65da2fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, TimeSeriesSplit\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "XGBoost Feature Importances:\n",
    "                 Feature  Importance\n",
    "15            dew_roll_7    0.682703\n",
    "6      day_of_year_sin_3    0.138269\n",
    "1              dew_point    0.048434\n",
    "3      day_of_year_cos_1    0.037955\n",
    "2      day_of_year_sin_1    0.014656\n",
    "4      day_of_year_sin_2    0.012544\n",
    "9              dew_lag_1    0.011720\n",
    "14            atm_roll_7    0.009275\n",
    "7      day_of_year_cos_3    0.007737\n",
    "5      day_of_year_cos_2    0.007175\n",
    "11             dew_lag_2    0.005761\n",
    "10             atm_lag_2    0.005529\n",
    "8              atm_lag_1    0.005146\n",
    "13             dew_lag_3    0.004750\n",
    "0   atmospheric_pressure    0.004188\n",
    "12             atm_lag_3    0.004157\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "xgb_model = {}\n",
    "y_preds_xgb = {}\n",
    "rmses_xgb = {}\n",
    "r2s_xgb = {}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Results for temperature:\")\n",
    "print(f'Mean Squared Error: {mse_xgb:.4f}')\n",
    "print(f'RMSE: {rmse_xgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_xgb:.4f}')\n",
    "\n",
    "\n",
    "# K-Fold cross-validation for XGBoost\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_xgb = []\n",
    "r2_list_xgb = [] # Added to track R2 across all folds\n",
    "mse_list_xgb = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        xgb_model_kf = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_xgb = xgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "\n",
    "        mse_kf_xgb = mean_squared_error(y_test_kf, y_pred_kf_xgb)\n",
    "        rmse_kf_xgb = np.sqrt(mse_kf_xgb)\n",
    "        r2_kf_xgb = r2_score(y_test_kf, y_pred_kf_xgb)\n",
    "\n",
    "        mse_list_xgb.append(mse_kf_xgb)\n",
    "        rmse_list_xgb.append(rmse_kf_xgb)\n",
    "        r2_list_xgb.append(r2_kf_xgb)\n",
    "\n",
    "\n",
    "        average_rmse_xgb = np.mean(rmse_list_xgb)\n",
    "        average_r2_xgb = np.mean(r2_list_xgb)\n",
    "        average_mse_xgb = np.mean(mse_kf_xgb)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_xgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_xgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_xgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_xgb}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "importance = xgb_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_xgb - average_r2_xgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(feature_importance_df_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdf3a43",
   "metadata": {},
   "source": [
    "#LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78631c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# df = df[\n",
    "#     (df['date'] >= '2018-01-01') & (df['date'] <= '2024-12-31')\n",
    "# ]\n",
    "\n",
    "# 'atmospheric_pressure','minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([ f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['temp_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['temp_roll_7', 'atm_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e52e155c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "'dew_point',\n",
    "'temp_roll_7',\n",
    "'day_of_year_sin_3',\n",
    "'atm_roll_7',\n",
    "'day_of_year_sin_2',\n",
    "'day_of_year_cos_1',\n",
    "'dew_lag_3',\n",
    "'atmospheric_pressure',\n",
    "'dew_lag_1',\n",
    "'day_of_year_cos_3',\n",
    "'day_of_year_sin_1',\n",
    "'atm_lag_1',\n",
    "'day_of_year_cos_2',\n",
    "'atm_lag_2', # 0.8865 / 0.8935 ~0.6968\n",
    "#'atm_lag_3',\n",
    "#'dew_lag_2'\n",
    "# 0.88/0.89 ~0.69\n",
    "\n",
    " ]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 0.88/0.89 ~ -.69\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdb9491",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "''''\n",
    "LightGBM Feature Importances:\n",
    "                 Feature  Importance\n",
    "1              dew_point         636\n",
    "14           temp_roll_7         584\n",
    "6      day_of_year_sin_3         351\n",
    "15            atm_roll_7         350\n",
    "4      day_of_year_sin_2         320\n",
    "3      day_of_year_cos_1         295\n",
    "13             dew_lag_3         274\n",
    "0   atmospheric_pressure         262\n",
    "9              dew_lag_1         261\n",
    "7      day_of_year_cos_3         226\n",
    "2      day_of_year_sin_1         224\n",
    "8              atm_lag_1         220\n",
    "5      day_of_year_cos_2         211\n",
    "10             atm_lag_2         206\n",
    "12             atm_lag_3         190\n",
    "11             dew_lag_2         180\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "lgb_model = {}\n",
    "y_preds_lgb = {}\n",
    "rmses_lgb = {}\n",
    "r2s_lgb = {}\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "        # n_estimators=800,\n",
    "        # learning_rate=0.01,\n",
    "        # max_depth=8,\n",
    "        # subsample=0.8,\n",
    "        # colsample_bytree=0.8,\n",
    "        # random_state=42,\n",
    "        # verbosity=-1\n",
    "\n",
    "        n_estimators=300,        # Reduced to prevent memorization as UV r 4k dataset\n",
    "        learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "        max_depth=6,             # Shallow trees are better for 4k rows\n",
    "        num_leaves=20,           # Controls complexity\n",
    "        min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "        subsample=0.7,           # More aggressive sampling for better generalization\n",
    "        colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "                              # Clean console\n",
    "    )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_lgb = np.sqrt(mse_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Results for temp:\")\n",
    "print(f'Mean Squared Error: {mse_lgb:.4f}')\n",
    "print(f'RMSE: {rmse_lgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_lgb:.4f}')\n",
    "\n",
    "\n",
    "    # --- 6) 5-fold CV R^2 ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "rmse_list_lgb = []\n",
    "r2_list_lgb = []\n",
    "mse_list_lgb = []\n",
    "\n",
    "\n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        lgb_model_kf = lgb.LGBMRegressor(\n",
    "            \n",
    "            n_estimators=300,        # Reduced to prevent memorization\n",
    "            learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "            max_depth=6,             # Shallow trees are better for 4k rows\n",
    "            num_leaves=20,           # Controls complexity\n",
    "            min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "            subsample=0.7,           # More aggressive sampling for better generalization\n",
    "            colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "            random_state=42,\n",
    "            verbosity=-1             # Clean console\n",
    "        )\n",
    "\n",
    "        lgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_lgb = lgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_lgb = mean_squared_error(y_test_kf, y_pred_kf_lgb)\n",
    "        rmse_kf_lgb = np.sqrt(mse_kf_lgb)\n",
    "        r2_kf_lgb = r2_score(y_test_kf, y_pred_kf_lgb)\n",
    "        \n",
    "        mse_list_lgb.append(mse_kf_lgb)\n",
    "        rmse_list_lgb.append(rmse_kf_lgb)\n",
    "        r2_list_lgb.append(r2_kf_lgb)\n",
    "\n",
    "        average_rmse_lgb = np.mean(rmse_list_lgb)\n",
    "        average_r2_lgb = np.mean(r2_list_lgb)\n",
    "        average_mse_lgb = np.mean(mse_kf_lgb)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_lgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_lgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_lgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lgb}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = lgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_lgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_lgb = feature_importance_df_lgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_lgb - average_r2_lgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(feature_importance_df_lgb)\n",
    "\n",
    "##### year wise analysis #####\n",
    "# 1980 to 2024 -> ~ 0.69 (44 years)\n",
    "# 2014 to 2024 -> ~ 0.82 (10 years)\n",
    "# 2017 to 2024 -> ~ 0.042 (7 years) **** yey ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
    "# 2018 to 2024 -> ~ 0.78 (6 years)\n",
    "# 2019 to 2024 -> ~ 1.15 (5 yrs)\n",
    "# 2021 to 2024 -> ~ 2.33 (3 yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dedc8c2",
   "metadata": {},
   "source": [
    "#CATBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fe8198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cc304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'temp_lag_1',\n",
    "\n",
    "\n",
    "#     ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013345c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "''''\n",
    "CatBoost Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)   43.960021\n",
    "8                      temp_lag_1   10.761866\n",
    "6               day_of_year_sin_3    7.732805\n",
    "14                    temp_roll_7    7.551957\n",
    "3               day_of_year_cos_1    4.006128\n",
    "2               day_of_year_sin_1    3.907999\n",
    "10                     temp_lag_2    3.767410\n",
    "4               day_of_year_sin_2    3.340417\n",
    "9                       atm_lag_1    2.673542\n",
    "12                     temp_lag_3    2.317698\n",
    "5               day_of_year_cos_2    2.278235\n",
    "1            atmospheric_pressure    1.917982\n",
    "15                     atm_roll_7    1.841164\n",
    "7               day_of_year_cos_3    1.651114\n",
    "13                      atm_lag_3    1.179426\n",
    "11                      atm_lag_2    1.112237\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "cat_model = {}\n",
    "y_preds_cat = {}\n",
    "rmses_cat = {}\n",
    "r2s_cat = {}\n",
    "\n",
    "# loss_function='RMSE' is standard for regression\n",
    "cat_model = CatBoostRegressor(\n",
    "        iterations=800,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "        bootstrap_type='Bayesian',\n",
    "        bagging_temperature=1,\n",
    "        random_strength=1,\n",
    "        loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "mse_cat = mean_squared_error(y_test, y_pred)\n",
    "rmse_cat = np.sqrt(mse_cat)\n",
    "r2_cat = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"\\nLightGBM Results for UV :\")\n",
    "print(f'Mean Squared Error: {mse_cat:.4f}')\n",
    "print(f'RMSE: {rmse_cat:.4f}')\n",
    "print(f'RÂ² Score: {r2_cat:.4f}')\n",
    "\n",
    "\n",
    "    # 3. 5-Fold Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_cat = []\n",
    "r2_list_cat = []\n",
    "mse_list_cat = []\n",
    "    \n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        cat_model_kf = CatBoostRegressor(\n",
    "            iterations=800,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "            bootstrap_type='Bayesian',\n",
    "            bagging_temperature=1,\n",
    "            random_strength=1,\n",
    "            loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        cat_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_cat = cat_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_cat = mean_squared_error(y_test_kf, y_pred_kf_cat)\n",
    "        rmse_kf_cat = np.sqrt(mse_kf_cat)\n",
    "        r2_kf_cat = r2_score(y_test_kf, y_pred_kf_cat)\n",
    "\n",
    "        mse_list_cat.append(mse_kf_cat)\n",
    "        rmse_list_cat.append(rmse_kf_cat)\n",
    "        r2_list_cat.append(r2_kf_cat)\n",
    "\n",
    "        average_rmse_cat = np.mean(rmse_list_cat)\n",
    "        average_r2_cat = np.mean(r2_list_cat)\n",
    "        average_mse_cat = np.mean(mse_list_cat)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_cat:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_cat:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_cat}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_cat}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = cat_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_cat = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_cat = feature_importance_df_cat.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_cat - average_r2_cat)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nCatBoost Feature Importances:\")\n",
    "print(feature_importance_df_cat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "575ad824",
   "metadata": {},
   "source": [
    "#GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7ac3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# df = df[\n",
    "#     (df['date'] >= '2010-01-01') & (df['date'] <= '2024-12-31')\n",
    "# ]\n",
    "\n",
    "\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([ f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654d08d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "    \n",
    "    ]\n",
    "\n",
    "''''\n",
    "\n",
    "'''\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 90.47/91.73 *-1.26\n",
    "\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab0b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create an instance with specific parameters\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15,          # Wait 15 epochs for improvement before stopping\n",
    "    restore_best_weights=True  # Very important: keeps the best version of your model\n",
    ")\n",
    "\n",
    "# 1. Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for GRU: (samples, time_steps, features)\n",
    "# Here we use time_steps=1. If you want sequences, you'd need a sliding window function.\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split data (matching your non-shuffle 80/20 split)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train initial model\n",
    "gru_model = build_gru((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_gru = mean_squared_error(y_test_unscaled, y_pred)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_unscaled, y_pred)\n",
    "\n",
    "print(f\"\\nGRU Results for temperature :\")\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'RMSE: {rmse_gru:.4f}')\n",
    "print(f'RÂ² Score: {r2_gru:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild/Reset model for each fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_gru = np.mean(r2_list_gru)\n",
    "average_mse_gru = np.mean(mse_list_gru)\n",
    "average_rmse_gru = np.mean(rmse_list_gru)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_gru:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_gru:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_gru:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_gru}\")\n",
    "\n",
    "diff = (r2_gru - np.mean(r2_list_gru))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28257b4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculates importance by measuring how much the MSE increases \n",
    "    when a single feature is randomly shuffled.\n",
    "    \"\"\"\n",
    "    # Baseline prediction\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    baseline_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                     scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X_val.shape[2]):  # Iterate through each feature\n",
    "        save = X_val[:, :, i].copy()\n",
    "        \n",
    "        # Shuffle the current feature across all samples\n",
    "        np.random.shuffle(X_val[:, :, i])\n",
    "        \n",
    "        # Predict with shuffled feature\n",
    "        shuffled_preds = model.predict(X_val, verbose=0)\n",
    "        shuffled_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                         scaler_y.inverse_transform(shuffled_preds))\n",
    "        \n",
    "        # Importance is the increase in error\n",
    "        importances.append(max(0, shuffled_mse - baseline_mse))\n",
    "        \n",
    "        # Restore the original feature values\n",
    "        X_val[:, :, i] = save\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    return importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Execute ---\n",
    "# Note: Use your X_test and y_test from the previous step\n",
    "feature_importance_gru = calculate_permutation_importance(\n",
    "    gru_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGRU Permutation Feature Importances:\")\n",
    "print(feature_importance_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12919b2a",
   "metadata": {},
   "source": [
    "#lSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b04cc9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([ f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862ee15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 91.24/92.25 *1.01\n",
    "##\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08cbc50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for LSTM: [samples, time_steps, features]\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split matching your CatBoost logic (shuffle=False)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "lstm_model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_actual, y_pred_lstm)\n",
    "\n",
    "print(f\"\\nLSTM Results for temp :\")\n",
    "print(f'Mean Squared Error: {mse_lstm:.4f}')    \n",
    "print(f'RMSE: {rmse_lstm:.4f}')\n",
    "print(f'RÂ² Score: {r2_lstm:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_lstm = []\n",
    "r2_list_lstm = []\n",
    "mse_list_lstm = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    lstm_kf = build_lstm((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    lstm_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = lstm_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_lstm.append(np.sqrt(mse_kf))\n",
    "    mse_list_lstm.append(mse_kf)\n",
    "    r2_list_lstm.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "\n",
    "average_r2_lstm = np.mean(r2_list_lstm)\n",
    "average_mse_lstm = np.mean(mse_list_lstm)\n",
    "average_rmse_lstm = np.mean(rmse_list_lstm)\n",
    " \n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from LSTM CV: {average_rmse_lstm:.4f}\")\n",
    "print(f\"Average RÂ² from LSTM CV: { average_r2_lstm:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_lstm:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lstm}\")\n",
    "\n",
    "def calculate_lstm_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Computes permutation importance for a trained LSTM model.\n",
    "    \"\"\"\n",
    "    # 1. Get baseline score (Inverse scale to get real-world MSE)\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    \n",
    "    # Iterate through each feature index\n",
    "    for i in range(X_val.shape[2]):\n",
    "        # Create a copy to avoid permanent shuffling\n",
    "        X_permuted = X_val.copy()\n",
    "        \n",
    "        # 2. Shuffle the specific feature across all samples\n",
    "        # Shuffling happens across the 'samples' dimension for the i-th feature\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        \n",
    "        # 3. Predict with the permuted feature\n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        # 4. Importance = Increase in Error (shuffled error - baseline error)\n",
    "        importance = max(0, permuted_mse - baseline_mse)\n",
    "        importance_results.append(importance)\n",
    "\n",
    "    # Organize into a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names, \n",
    "        'Importance': importance_results\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# --- Execution ---\n",
    "# Using the X_test and y_test from your LSTM training\n",
    "lstm_importance_df = calculate_lstm_permutation_importance(\n",
    "    lstm_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "diff = (r2_lstm - np.mean(r2_list_lstm))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLSTM Permutation Feature Importances:\")\n",
    "print(lstm_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8a8b80",
   "metadata": {},
   "source": [
    "#ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50a64a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','dew_point' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = [ 'atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n",
    "\n",
    "FEATURES = [\n",
    "   \n",
    "\n",
    "\n",
    "    ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 89.36/91.27 * 1.91\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82fc6609",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X_ann = StandardScaler()\n",
    "scaler_y_ann = StandardScaler()\n",
    "\n",
    "X_scaled_ann = scaler_X_ann.fit_transform(X)\n",
    "y_scaled_ann = scaler_y_ann.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split matching your CatBoost logic (80/20, shuffle=False)\n",
    "split_idx = int(len(X_scaled_ann) * 0.8)\n",
    "X_train_ann, X_test_ann = X_scaled_ann[:split_idx], X_scaled_ann[split_idx:]\n",
    "y_train_ann, y_test_ann = y_scaled_ann[:split_idx], y_scaled_ann[split_idx:]\n",
    "\n",
    "# build model \n",
    "\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Linear output for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "ann_model = build_ann(X_train_ann.shape[1])\n",
    "ann_model.fit(X_train_ann, y_train_ann, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled_ann = ann_model.predict(X_test_ann)\n",
    "y_pred_ann = scaler_y_ann.inverse_transform(y_pred_scaled_ann)\n",
    "y_test_actual = scaler_y_ann.inverse_transform(y_test_ann)\n",
    "\n",
    "mse_ann = mean_squared_error(y_test_actual, y_pred_ann)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "r2_ann = r2_score(y_test_actual, y_pred_ann)\n",
    "\n",
    "print(f\"\\nANN Results for temperature :\")\n",
    "print(f'Mean Squared Error: {mse_ann:.4f}')\n",
    "print(f'RMSE: {rmse_ann:.4f}')\n",
    "print(f'RÂ² Score: {r2_ann:.4f}')\n",
    "\n",
    "# CV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_ann = []\n",
    "r2_list_ann = []\n",
    "mse_list_ann = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled_ann):\n",
    "    X_train_kf, X_test_kf = X_scaled_ann[train_index], X_scaled_ann[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled_ann[train_index], y_scaled_ann[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    ann_kf = build_ann(X_train_kf.shape[1])\n",
    "    ann_kf.fit(X_train_kf, y_train_kf, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = ann_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y_ann.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y_ann.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_ann.append(np.sqrt(mse_kf))\n",
    "    mse_list_ann.append(mse_kf)\n",
    "    r2_list_ann.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_ann = np.mean(r2_list_ann)\n",
    "average_mse_ann = np.mean(mse_list_ann)\n",
    "average_rmse_ann = np.mean(rmse_list_ann)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from ANN CV: {average_rmse_ann:.4f}\")\n",
    "print(f\"Average RÂ² from ANN CV: {average_r2_ann:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_ann:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_ann}\")\n",
    "\n",
    "# importance\n",
    "def calculate_ann_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    for i in range(X_val.shape[1]): # Iterate through 2D features\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        importance_results.append(max(0, permuted_mse - baseline_mse))\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance_results}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "ann_importance_df = calculate_ann_permutation_importance(ann_model, X_test_ann, y_test_ann, scaler_y_ann, FEATURES)\n",
    "\n",
    "diff = (r2_ann - np.mean(r2_list_ann))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nANN Permutation Feature Importances:\")\n",
    "print(ann_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82d0add5",
   "metadata": {},
   "source": [
    "#CNN-LSTM model hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b902683",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    # df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "\n",
    "    lag_cols.extend([ f'atm_lag_{lag}', f'dew_lag_{lag}'])\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "# df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = [ 'atm_roll_7', 'dew_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FEATURES = ['atmospheric_pressure','dew_point'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['temperature(degree C)']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3492bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune CNN Filters\n",
    "    hp_filters = hp.Int('filters', min_value=32, max_value=64, step=32)\n",
    "    model.add(Conv1D(filters=hp_filters, kernel_size=3, activation='relu', input_shape=(100,1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Tune Dropout\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Tune LSTM Units\n",
    "    hp_lstm_units = hp.Int('lstm_units', min_value=32, max_value=64, step=16)\n",
    "    model.add(LSTM(units=hp_lstm_units, activation='tanh'))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Final Dense Layers\n",
    "    model.add(Dense(hp.Int('dense_units', 16, 64, 16), activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Tune Learning Rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example: Using the last 30 days to predict tomorrow\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# X_3D shape will be (Total_Days, 30, 1)\n",
    "X_3D, y_target = create_sequences(df['temperature(degree C)'].values)\n",
    "X_3D = X_3D.reshape((X_3D.shape[0], X_3D.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c6723fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# 1. Prepare 3D Data (Samples, Time Steps, Features)\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_hybrid(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. CNN Stage: Extracts spatial/local patterns from the window\n",
    "        # Reducing filters to 32 is often better for ~4k rows to prevent noise capture\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100,1)),\n",
    "        BatchNormalization(), # Stabilizes learning and speeds up convergence\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2), # Standard regularization\n",
    "\n",
    "        # 2. LSTM Stage: Learns temporal dependencies\n",
    "        # tanh is the standard and most stable activation for LSTM\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        # 3. Fully Connected Stage\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1) # Output for UV prediction\n",
    "    ])\n",
    "    \n",
    "    # Using a slightly lower learning rate (0.0005) helps with smaller datasets\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Data\n",
    "window = 30\n",
    "data_values = df['temperature(degree C)'].values.reshape(-1, 1)\n",
    "X, y = create_sequences(data_values, window)\n",
    "\n",
    "# --- BASE PERFORMANCE ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "model = build_hybrid((window, 1))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, y_pred)\n",
    "base_rmse = np.sqrt(base_mse)\n",
    "base_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Base Results: R2: {base_r2:.4f}, MSE: {base_mse:.4f}, RMSE: {base_rmse:.4f}\")\n",
    "\n",
    "# --- 5-FOLD CROSS VALIDATION ---\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_r2, cv_mse, cv_rmse = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    m = build_hybrid((window, 1))\n",
    "    m.fit(X[train_idx], y[train_idx], epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    p = m.predict(X[test_idx])\n",
    "    cv_r2.append(r2_score(y[test_idx], p))\n",
    "    cv_mse.append(mean_squared_error(y[test_idx], p))\n",
    "    cv_rmse.append(np.sqrt(cv_mse[-1]))\n",
    "\n",
    "\n",
    "cnn_lstm_r2_cv = np.mean(cv_r2)\n",
    "cnn_lstm_mse_cv = np.mean(cv_mse)\n",
    "cnn_lstm_rmse_cv = np.mean(cv_rmse)\n",
    "\n",
    "print(f\"5-Fold CV Average: R2: {cnn_lstm_r2_cv:.4f}, MSE: {cnn_lstm_mse_cv:.4f}, RMSE: {cnn_lstm_rmse_cv:.4f}\")\n",
    "print ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bcc0ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance chart \n",
    "MODEL_NAMES = [\"Random Forest\", \"RF-LSTM hybrid\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"GRU\", \"LSTM\", \"ANN\", \"CNN-LSTM hybrid\"]\n",
    "\n",
    "R_SQUARED_VALUES = [r2_rf, rf_lstm_r2, r2_xgb, r2_lgb, r2_cat, r2_gru, r2_lstm, r2_ann, base_r2 ]\n",
    "R2CV = [average_r2_rf, rf_lstm_r2_cv, average_r2_xgb, average_r2_lgb, average_r2_cat, average_r2_gru, average_r2_lstm, average_r2_ann, cnn_lstm_r2_cv ]\n",
    "\n",
    "R2_DIFF = [\n",
    "    (r2_rf - average_r2_rf), \n",
    "    (rf_lstm_r2 - rf_lstm_r2_cv), \n",
    "    (r2_xgb - average_r2_xgb), \n",
    "    (r2_lgb - average_r2_lgb), \n",
    "    (r2_cat - average_r2_cat), \n",
    "    (r2_gru - average_r2_gru), \n",
    "    (r2_lstm - average_r2_lstm), \n",
    "    (r2_ann - average_r2_ann), \n",
    "    (base_r2 - cnn_lstm_r2_cv)\n",
    "]\n",
    "\n",
    "MSE_VALUES = [mse_rf, rf_lstm_mse, mse_xgb, mse_lgb, mse_cat, mse_gru, mse_lstm, mse_ann, base_mse ]\n",
    "MSE_CV = [average_mse_rf, rf_lstm_mse_cv, average_mse_xgb, average_mse_lgb, average_mse_cat, average_mse_gru, average_mse_lstm, average_mse_ann, cnn_lstm_mse_cv ]\n",
    "\n",
    "RMSE_VALUES = [rmse_rf, rf_lstm_rmse, rmse_xgb, rmse_lgb, rmse_cat, rmse_gru, rmse_lstm, rmse_ann , base_rmse ]\n",
    "RMSE_CV = [average_rmse_rf, rf_lstm_rmse_cv, average_rmse_xgb, average_rmse_lgb, average_rmse_cat, average_rmse_gru, average_rmse_lstm, average_rmse_ann, cnn_lstm_rmse_cv]\n",
    "\n",
    "data = {\n",
    "    \"Model\": MODEL_NAMES,\n",
    "    \"R^2\": R_SQUARED_VALUES,\n",
    "    \"CVR2\": R2CV,\n",
    "    \"R2 DIFF\": R2_DIFF,\n",
    "    \"MSE\": MSE_VALUES,\n",
    "    \"MSE CV\": MSE_CV,\n",
    "    \"RMSE\": RMSE_VALUES,\n",
    "    \"RMSE CV\": RMSE_CV\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(data).sort_values(by=[\"R2 DIFF\",\"R^2\"], ascending= [True, True])\n",
    "\n",
    "print (df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define weights for your 'Best Logical Model' criteria\n",
    "# We want high R^2, low RMSE, and low R2 DIFF (stability)\n",
    "weights = {\n",
    "    'R^2': 0.4,       # Predictive power\n",
    "    'MSE': 0.3,\n",
    "    'RMSE': 0.3,     # Magnitude of error\n",
    "    'R2 DIFF': 0.3    # Robustness/Generalization\n",
    "}\n",
    "\n",
    "# 2. Create a Ranking Score (Lower is better)\n",
    "# .rank(ascending=False) means highest value gets rank 1\n",
    "# .rank(ascending=True) means lowest value gets rank 1\n",
    "df_performance['Score'] = (\n",
    "    df_performance['R^2'].rank(ascending=False) * weights['R^2'] +\n",
    "    df_performance['MSE'].rank(ascending=True) * weights['MSE'] +\n",
    "    df_performance['RMSE'].rank(ascending=True) * weights['RMSE']  +\n",
    "    df_performance['R2 DIFF'].rank(ascending=True) * weights['R2 DIFF']\n",
    ")\n",
    "\n",
    "# 3. Extract the winner\n",
    "best_logical_model = df_performance.loc[df_performance['Score'].idxmin()]\n",
    "\n",
    "print(f\"The Best Logical Model is: {best_logical_model['Model']}\")\n",
    "print(f\"--- Reason: Balanced score across R^2 ({best_logical_model['R^2']:.4f}) \"\n",
    "      f\"and Stability (DIFF: {best_logical_model['R2 DIFF']:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d85de02",
   "metadata": {},
   "source": [
    "<h1>Humidity Prediction </h1>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>temperature(degree C)</th>\n",
       "      <th>feels_like(degree C)</th>\n",
       "      <th>max_temperature(degree C)</th>\n",
       "      <th>minimum_temperature(degree C)</th>\n",
       "      <th>humidity</th>\n",
       "      <th>precipitation</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>atmospheric_pressure</th>\n",
       "      <th>UV</th>\n",
       "      <th>solar_radiation</th>\n",
       "      <th>dew_point</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1980</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   day  month  year  temperature(degree C)  feels_like(degree C)  \\\n",
       "0    1      1  1980                    NaN                   NaN   \n",
       "1    2      1  1980                    NaN                   NaN   \n",
       "2    3      1  1980                    NaN                   NaN   \n",
       "3    4      1  1980                    NaN                   NaN   \n",
       "4    5      1  1980                    NaN                   NaN   \n",
       "\n",
       "   max_temperature(degree C)  minimum_temperature(degree C)  humidity  \\\n",
       "0                        0.0                            0.0       NaN   \n",
       "1                        0.0                            0.0       NaN   \n",
       "2                        0.0                            0.0       NaN   \n",
       "3                        0.0                            0.0       NaN   \n",
       "4                        0.0                            0.0       NaN   \n",
       "\n",
       "   precipitation  windspeed  atmospheric_pressure  UV  solar_radiation  \\\n",
       "0            NaN        NaN                   NaN NaN              NaN   \n",
       "1            NaN        NaN                   NaN NaN              NaN   \n",
       "2            NaN        NaN                   NaN NaN              NaN   \n",
       "3            NaN        NaN                   NaN NaN              NaN   \n",
       "4            NaN        NaN                   NaN NaN              NaN   \n",
       "\n",
       "   dew_point  \n",
       "0        NaN  \n",
       "1        NaN  \n",
       "2        NaN  \n",
       "3        NaN  \n",
       "4        NaN  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_chittagong.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'dew_point','solar_radiation','UV','atmospheric_pressure','precipitation\t' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'dew_lag_{lag}'] = df['dew_point'].shift(lag)\n",
    "    df[f'solar_lag_{lag}'] = df['solar_radiation'].shift(lag)\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'dew_lag_{lag}', f'solar_lag_{lag}', f'atm_lag_{lag}', f'ppt_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['dew_roll_7'] = df['dew_point'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['solar_roll_7'] = df['solar_radiation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['ppt_roll_7'] = df['precipitation'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['dew_roll_7', 'solar_roll_7' ,'atm_roll_7', 'ppt_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [\n",
    "   \n",
    "    ]\n",
    "\n",
    "FEATURES = [ 'dew_point','solar_radiation','atmospheric_pressure','precipitation'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    " \n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['humidity']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Random Forest Results for humidity:\n",
      "MSE: 20.9791\n",
      "RMSE: 4.5803\n",
      "RÂ² Score: 0.7754\n",
      "Average RMSE from CV: 4.2416\n",
      "Average RÂ² from CV: 0.7447\n",
      "Avarage MSE: 18.039810483557336\n",
      "Individual Fold RMSEs: [np.float64(4.2458889403543525), np.float64(4.088269140090962), np.float64(4.343100077299967), np.float64(3.9442915648352397), np.float64(4.58667425617623)]\n",
      "\n",
      " R2 ~ 3.0649\n",
      "\n",
      "Random Forest Feature Importances (from last fold):\n",
      "                 Feature  Importance\n",
      "0              dew_point    0.186775\n",
      "3          precipitation    0.108665\n",
      "10             dew_lag_1    0.088150\n",
      "1        solar_radiation    0.080636\n",
      "13             ppt_lag_1    0.053163\n",
      "25            ppt_roll_7    0.052779\n",
      "22            dew_roll_7    0.051517\n",
      "14             dew_lag_2    0.037632\n",
      "11           solar_lag_1    0.033232\n",
      "18             dew_lag_3    0.029392\n",
      "23          solar_roll_7    0.028234\n",
      "4      day_of_year_sin_1    0.023292\n",
      "17             ppt_lag_2    0.022505\n",
      "24            atm_roll_7    0.020220\n",
      "6      day_of_year_sin_2    0.019927\n",
      "5      day_of_year_cos_1    0.018931\n",
      "15           solar_lag_2    0.016846\n",
      "16             atm_lag_2    0.016830\n",
      "20             atm_lag_3    0.016316\n",
      "7      day_of_year_cos_2    0.016253\n",
      "19           solar_lag_3    0.014812\n",
      "2   atmospheric_pressure    0.014456\n",
      "12             atm_lag_1    0.014212\n",
      "8      day_of_year_sin_3    0.012930\n",
      "9      day_of_year_cos_3    0.012264\n",
      "21             ppt_lag_3    0.010033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "\n",
    "Random Forest Feature Importances (from last fold):\n",
    "                 Feature  Importance\n",
    "0              dew_point    0.186775\n",
    "3          precipitation    0.108665\n",
    "10             dew_lag_1    0.088150\n",
    "1        solar_radiation    0.080636\n",
    "13             ppt_lag_1    0.053163\n",
    "25            ppt_roll_7    0.052779\n",
    "22            dew_roll_7    0.051517\n",
    "14             dew_lag_2    0.037632\n",
    "11           solar_lag_1    0.033232\n",
    "18             dew_lag_3    0.029392\n",
    "23          solar_roll_7    0.028234\n",
    "4      day_of_year_sin_1    0.023292\n",
    "17             ppt_lag_2    0.022505\n",
    "24            atm_roll_7    0.020220\n",
    "6      day_of_year_sin_2    0.019927\n",
    "5      day_of_year_cos_1    0.018931\n",
    "15           solar_lag_2    0.016846\n",
    "16             atm_lag_2    0.016830\n",
    "20             atm_lag_3    0.016316\n",
    "7      day_of_year_cos_2    0.016253\n",
    "19           solar_lag_3    0.014812\n",
    "2   atmospheric_pressure    0.014456\n",
    "12             atm_lag_1    0.014212\n",
    "8      day_of_year_sin_3    0.012930\n",
    "9      day_of_year_cos_3    0.012264\n",
    "21             ppt_lag_3    0.010033\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for humidity:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'RÂ² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2fff34ab",
   "metadata": {},
   "source": [
    "# DEW POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8fb4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5b806c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52252dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "'''\n",
    "Random Forest Feature Importances (from last fold):\n",
    "                          Feature  Importance\n",
    "2   minimum_temperature(degree C)    0.216909\n",
    "20                  mintem_roll_7    0.146150\n",
    "11                   mintem_lag_1    0.131497\n",
    "14                   mintem_lag_2    0.122413\n",
    "17                   mintem_lag_3    0.073617\n",
    "18                     atm_roll_7    0.056844\n",
    "12                      atm_lag_2    0.039081\n",
    "0            atmospheric_pressure    0.035956\n",
    "4               day_of_year_cos_1    0.025367\n",
    "15                      atm_lag_3    0.025037\n",
    "9                       atm_lag_1    0.024415\n",
    "1                        humidity    0.024317\n",
    "7               day_of_year_sin_3    0.019913\n",
    "3               day_of_year_sin_1    0.010451\n",
    "19                     dew_roll_7    0.009862\n",
    "10                      hum_lag_1    0.009594\n",
    "5               day_of_year_sin_2    0.008322\n",
    "6               day_of_year_cos_2    0.007584\n",
    "13                      hum_lag_2    0.005335\n",
    "16                      hum_lag_3    0.004013\n",
    "8               day_of_year_cos_3    0.003323\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "\n",
    "rf_model = {}\n",
    "y_preds_rf = {}\n",
    "rmses_rf = {} # eigula active korte hobe\n",
    "r2s_rf = {}\n",
    "\n",
    "\n",
    "rf_model = RandomForestRegressor(\n",
    "        n_estimators= 800 ,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = rf_model.predict(X_test)\n",
    "\n",
    "\n",
    "    # model evaluation\n",
    "mse_rf = mean_squared_error(y_test, y_pred)\n",
    "rmse_rf = np.sqrt(mse_rf)\n",
    "r2_rf = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nRandom Forest Results for dewpoint:\")\n",
    "print(f'MSE: {mse_rf:.4f}')\n",
    "print(f'RMSE: {rmse_rf:.4f}')\n",
    "print(f'RÂ² Score: {r2_rf:.4f}')\n",
    "\n",
    "    # k fold cross-validation \n",
    "    # 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_rf = []\n",
    "r2_list_rf = [] # Added to track R2 across all folds\n",
    "mse_list_rf = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "    # iloc is used to split by integer position\n",
    "    X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "    \n",
    "    # FIX: y is already a Series, just use iloc[index] \n",
    "    y_train_kf = y.iloc[train_index]\n",
    "    y_test_kf = y.iloc[test_index]\n",
    "    \n",
    "    rf_model_kf = RandomForestRegressor(\n",
    "        n_estimators=800,\n",
    "        random_state=42,\n",
    "        max_depth=22,\n",
    "        min_samples_split=2,\n",
    "        min_samples_leaf=1,\n",
    "        n_jobs=-1,\n",
    "        max_features='sqrt'\n",
    "    )\n",
    "\n",
    "    # Training\n",
    "    rf_model_kf.fit(X_train_kf, y_train_kf)\n",
    "    y_pred_kf_rf = rf_model_kf.predict(X_test_kf)\n",
    "\n",
    "    # Metrics\n",
    "    mse_kf_rf = mean_squared_error(y_test_kf, y_pred_kf_rf)\n",
    "    rmse_kf_rf = np.sqrt(mse_kf_rf)\n",
    "    r2_kf_rf = r2_score(y_test_kf, y_pred_kf_rf)\n",
    "    \n",
    "    rmse_list_rf.append(rmse_kf_rf)\n",
    "    r2_list_rf.append(r2_kf_rf)\n",
    "    mse_list_rf.append(mse_kf_rf)\n",
    "\n",
    "# Final Aggregates\n",
    "average_rmse_rf = np.mean(rmse_list_rf)\n",
    "average_r2_rf = np.mean(r2_list_rf)\n",
    "average_mse_rf = np.mean(mse_list_rf)\n",
    "\n",
    "print(f\"Average RMSE from CV: {average_rmse_rf:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_rf:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_rf}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_rf}\")\n",
    "\n",
    "# Feature importance - Using the model from the LAST fold\n",
    "importance = rf_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_rf = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_rf = feature_importance_df_rf.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_rf - average_r2_rf)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nRandom Forest Feature Importances (from last fold):\")\n",
    "print(feature_importance_df_rf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcdc11ef",
   "metadata": {},
   "source": [
    "# RF-LSTM hybrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cecd24f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-1.12 [best]\n",
    "#     # 'day_of_year_sin_2', #\n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# 0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8abcf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "def create_sequences(x_data, y_data, window_size=5):\n",
    "    X_seq, y_seq = [], []\n",
    "    for i in range(window_size, len(x_data)):\n",
    "        X_seq.append(x_data[i-window_size:i]) # Grab the previous 'n' days\n",
    "        y_seq.append(y_data[i])               # The error of the CURRENT day\n",
    "    return np.array(X_seq), np.array(y_seq)\n",
    "\n",
    "# 1. Chronological Split (No Shuffling!)\n",
    "train_size = int(len(df) * 0.8)\n",
    "\n",
    "# These keep their column names (Good for RF)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "# 2. Re-fit your RF models on X_train explicitly to ensure they \"own\" the names\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# Get predictions on the training set using the DataFrames\n",
    "train_preds_temp = rf_model.predict(X_train)\n",
    "\n",
    "\n",
    "# Calculate Residuals (Errors)\n",
    "res_temp = y_train.values - train_preds_temp\n",
    "\n",
    "\n",
    "# Combine into a single error target for the LSTM\n",
    "train_residuals = np.column_stack([res_temp])\n",
    "\n",
    "# 1. Scale the features for the LSTM\n",
    "scaler_x = StandardScaler()\n",
    "X_train_scaled = scaler_x.fit_transform(X_train)\n",
    "X_test_scaled = scaler_x.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "# --- NEW SLIDING WINDOW BLOCK ---\n",
    "window_size = 5  # You can try 3, 5, or 7\n",
    "\n",
    "# Create sequences for training\n",
    "X_train_lstm, train_residuals_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "\n",
    "# Create sequences for testing\n",
    "X_test_lstm, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 3)), window_size)\n",
    "\n",
    "# Update the LSTM Input Shape\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_lstm.shape[2])), # Updated: shape is now (5, features)\n",
    "    LSTM(32, activation='tanh'), \n",
    "    Dense(16, activation='relu'),\n",
    "    Dense(3) \n",
    "])\n",
    "# Note: Use train_residuals_seq here instead of train_residuals\n",
    "lstm_model.compile(optimizer='adam', loss='mae')\n",
    "lstm_model.fit(X_train_lstm, train_residuals_seq, epochs=40, batch_size=32, verbose=0)\n",
    "\n",
    "# --- UPDATED PREDICTION BLOCK ---\n",
    "# We skip the first 'window_size' rows of X_test to match the LSTM output\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# Get RF predictions on the ALIGNED test set\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# Get LSTM corrections (These will already be aligned because of create_sequences)\n",
    "corrections = lstm_model.predict(X_test_lstm)\n",
    "\n",
    "# Combine\n",
    "final_temp = rf_t_pred + corrections[:, 0]\n",
    "\n",
    "\n",
    "print(f\"Windowed UV R2: {r2_score(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV MSE: {mean_squared_error(y_test_aligned, final_temp):.4f}\")\n",
    "print(f\"Windowed UV RMSE: {np.sqrt(mean_squared_error(y_test_aligned, final_temp)):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b3b35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from keras.optimizers import Adam, SGD\n",
    "\n",
    "# --- STEP 1: Selective Features for LSTM ---\n",
    "# We only give the LSTM the most important \"weather\" features to reduce noise\n",
    "lstm_feature_cols = FEATURES\n",
    "X_train_slim = X_train[lstm_feature_cols]\n",
    "X_test_slim = X_test[lstm_feature_cols]\n",
    "\n",
    "scaler_slim = StandardScaler()\n",
    "X_train_scaled = scaler_slim.fit_transform(X_train_slim)\n",
    "X_test_scaled = scaler_slim.transform(X_test_slim)\n",
    "\n",
    "# --- STEP 2: Create Sequences ---\n",
    "window_size = 7 # Try a full week\n",
    "X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals, window_size)\n",
    "X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "\n",
    "# --- STEP 3: Optimized LSTM ---\n",
    "lstm_model = Sequential([\n",
    "    Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "    LSTM(100, activation='tanh', return_sequences=True), # Return sequences for deeper learning\n",
    "    LSTM(50, activation='tanh'),\n",
    "    Dropout(0.3),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2) \n",
    "])\n",
    "\n",
    "# Use a slightly slower learning rate to find the pattern\n",
    "\n",
    "lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber') # Huber loss is great for weather outliers\n",
    "lstm_model.fit(X_train_seq, y_train_res_seq, epochs=60, batch_size=64, verbose=0)\n",
    "\n",
    "# 1. Align the Test Data (Skip the first 7 days used for the window)\n",
    "X_test_aligned = X_test.iloc[window_size:]\n",
    "y_test_aligned = y_test.iloc[window_size:]\n",
    "\n",
    "# 2. Get the \"Base\" predictions from your Random Forest\n",
    "rf_t_pred = rf_model.predict(X_test_aligned)\n",
    "\n",
    "# 3. Get the \"Corrections\" from the LSTM\n",
    "# X_test_seq was created during your sequence step\n",
    "lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "\n",
    "# 4. Combine them: Base + Correction\n",
    "final_uv = rf_t_pred + lstm_corrections[:, 0]\n",
    "\n",
    "rf_lstm_r2 = r2_score(y_test_aligned, final_uv)\n",
    "rf_lstm_mse = mean_squared_error(y_test_aligned, final_uv)\n",
    "rf_lstm_rmse = np.sqrt(rf_lstm_mse)\n",
    "\n",
    "# 5. Output the New Results\n",
    "print(\"--- HYBRID MODEL PERFORMANCE ---\")\n",
    "print(f\"Final temperature R2: {rf_lstm_r2:.4f}\")\n",
    "print(f\"Final temperature MSE: {rf_lstm_mse:.4f}\")\n",
    "print(f\"Final temperature RMSE: {rf_lstm_mse:.4f}\")\n",
    "# final r2 90.90/89.87 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f6ab773",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# 1. Initialize TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Assuming X and y are your full datasets before the train/test split\n",
    "# X_full, y_full, rf_model, create_sequences need to be defined in your workspace\n",
    "\n",
    "DEW = 'dew_point'\n",
    "X_full = df[FEATURES]\n",
    "y_full = df[DEW]\n",
    "\n",
    "fold = 1\n",
    "lstmRf_hybrid_r2_scores = []\n",
    "lstmRf_hybrid_mse_scores = []\n",
    "lstmRf_hybrid_rmse_scores = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_full):\n",
    "\n",
    "    print(f\"--- Processing Fold {fold} ---\")\n",
    "    \n",
    "    # Split Data\n",
    "    X_train_cv, X_test_cv = X_full.iloc[train_index], X_full.iloc[test_index]\n",
    "    y_train_cv, y_test_cv = y_full.iloc[train_index], y_full.iloc[test_index]\n",
    "    \n",
    "    # --- STEP 1: Random Forest Base Model (Required for Residuals) ---\n",
    "\n",
    "    # Training the RF on the current fold's training set\n",
    "    rf_model.fit(X_train_cv, y_train_cv)\n",
    "    train_residuals = y_train_cv - rf_model.predict(X_train_cv)\n",
    "    \n",
    "    # --- STEP 2: Preprocessing for LSTM ---\n",
    "    lstm_feature_cols = FEATURES\n",
    "\n",
    "    scaler = StandardScaler().set_output(transform=\"pandas\")\n",
    "\n",
    "    X_train_scaled = scaler.fit_transform(X_train_cv[lstm_feature_cols])\n",
    "    X_test_scaled = scaler.transform(X_test_cv[lstm_feature_cols])\n",
    "    \n",
    "    # --- STEP 3: Create Sequences ---\n",
    "    window_size = 5\n",
    "    X_train_seq, y_train_res_seq = create_sequences(X_train_scaled, train_residuals.values, window_size)\n",
    "    # We pass zeros for y_test as we only need the X sequences for prediction\n",
    "    X_test_seq, _ = create_sequences(X_test_scaled, np.zeros((len(X_test_scaled), 2)), window_size)\n",
    "    \n",
    "    # --- STEP 4: Train LSTM ---\n",
    "    # Re-initialize the model each fold to avoid weight leakage\n",
    "    lstm_model = Sequential([\n",
    "        Input(shape=(window_size, X_train_scaled.shape[1])),\n",
    "        LSTM(100, activation='tanh', return_sequences=True),\n",
    "        LSTM(50, activation='tanh'),\n",
    "        Dropout(0.3),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) \n",
    "    ])\n",
    "    \n",
    "    lstm_model.compile(optimizer=Adam(learning_rate=0.001), loss='huber')\n",
    "    lstm_model.fit(X_train_seq, y_train_res_seq, epochs=30, batch_size=64, verbose=0)\n",
    "    \n",
    "    # --- STEP 5: Hybrid Prediction & Evaluation ---\n",
    "    # Align target data (drop first 'window_size' rows)\n",
    "    y_test_aligned = y_test_cv.iloc[window_size:]\n",
    "    rf_base_pred = rf_model.predict(X_test_cv.iloc[window_size:])\n",
    "    \n",
    "    lstm_corrections = lstm_model.predict(X_test_seq)\n",
    "    lstmRf_hybrid_prediction = rf_base_pred + lstm_corrections[:, 0] # Adjust index if target is multi-output\n",
    "    \n",
    "    # Calculate Metrics\n",
    "    lstmRf_hybrid_r2_kf = r2_score(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_mse_kf = mean_squared_error(y_test_aligned, lstmRf_hybrid_prediction)\n",
    "    lstmRf_hybrid_rmse_kf = np.sqrt(lstmRf_hybrid_mse_kf)\n",
    "\n",
    "    lstmRf_hybrid_r2_scores.append(lstmRf_hybrid_r2_kf)\n",
    "    lstmRf_hybrid_mse_scores.append(lstmRf_hybrid_mse_kf)\n",
    "    lstmRf_hybrid_rmse_scores.append(lstmRf_hybrid_rmse_kf)\n",
    "\n",
    "    print(f\"Fold {fold} R2: {lstmRf_hybrid_r2_kf:.4f}\")\n",
    "    print(f\"Fold {fold} MSE: {lstmRf_hybrid_mse_kf:.4f}\")\n",
    "    print(f\"Fold {fold} RMSE: {lstmRf_hybrid_rmse_kf:.4f}\")\n",
    "    fold += 1\n",
    "\n",
    "rf_lstm_r2_cv = np.mean(lstmRf_hybrid_r2_scores)\n",
    "rf_lstm_mse_cv = np.mean(lstmRf_hybrid_mse_scores)\n",
    "rf_lstm_rmse_cv = np.mean(lstmRf_hybrid_rmse_scores)\n",
    "\n",
    "print(\"\\n--- FINAL CROSS-VALIDATION RESULTS ---\")\n",
    "print(f\"Mean R2: {rf_lstm_r2_cv:.4f} (+/- {np.std(lstmRf_hybrid_r2_scores):.4f})\")\n",
    "print(f\"Mean mse: {rf_lstm_mse_cv:.4f} (+/- {np.std(lstmRf_hybrid_mse_scores):.4f})\")\n",
    "print(f\"Mean rmse: {rf_lstm_rmse_cv:.4f} (+/- {np.std(lstmRf_hybrid_rmse_scores):.4f})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affd03e8",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c4451f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60a41fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae46987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "'''\n",
    "\n",
    "XGBoost Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)    0.686006\n",
    "14                    temp_roll_7    0.137628\n",
    "6               day_of_year_sin_3    0.045227\n",
    "3               day_of_year_cos_1    0.018808\n",
    "4               day_of_year_sin_2    0.016560\n",
    "2               day_of_year_sin_1    0.016441\n",
    "15                     atm_roll_7    0.011273\n",
    "7               day_of_year_cos_3    0.010194\n",
    "8                      temp_lag_1    0.009055\n",
    "1            atmospheric_pressure    0.008524\n",
    "5               day_of_year_cos_2    0.007986\n",
    "9                       atm_lag_1    0.007748\n",
    "11                      atm_lag_2    0.006355\n",
    "12                     temp_lag_3    0.006175\n",
    "10                     temp_lag_2    0.006093\n",
    "13                      atm_lag_3    0.005929\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "xgb_model = {}\n",
    "y_preds_xgb = {}\n",
    "rmses_xgb = {}\n",
    "r2s_xgb = {}\n",
    "\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "        objective='reg:squarederror',\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.1,\n",
    "        max_depth=5,\n",
    "        random_state=42\n",
    "    )\n",
    "xgb_model.fit(X_train, y_train)\n",
    "y_pred = xgb_model.predict(X_test)\n",
    "\n",
    "mse_xgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_xgb = np.sqrt(mse_xgb)\n",
    "r2_xgb = r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "print(f\"\\nXGBoost Results for dewpoint:\")\n",
    "print(f'Mean Squared Error: {mse_xgb:.4f}')\n",
    "print(f'RMSE: {rmse_xgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_xgb:.4f}')\n",
    "\n",
    "\n",
    "# K-Fold cross-validation for XGBoost\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_xgb = []\n",
    "r2_list_xgb = [] # Added to track R2 across all folds\n",
    "mse_list_xgb = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        xgb_model_kf = xgb.XGBRegressor(\n",
    "            objective='reg:squarederror',\n",
    "            n_estimators=800,\n",
    "            learning_rate=0.1,\n",
    "            max_depth=5,\n",
    "            random_state=42\n",
    "        )\n",
    "\n",
    "        xgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_xgb = xgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "\n",
    "        mse_kf_xgb = mean_squared_error(y_test_kf, y_pred_kf_xgb)\n",
    "        rmse_kf_xgb = np.sqrt(mse_kf_xgb)\n",
    "        r2_kf_xgb = r2_score(y_test_kf, y_pred_kf_xgb)\n",
    "\n",
    "        mse_list_xgb.append(mse_kf_xgb)\n",
    "        rmse_list_xgb.append(rmse_kf_xgb)\n",
    "        r2_list_xgb.append(r2_kf_xgb)\n",
    "\n",
    "\n",
    "        average_rmse_xgb = np.mean(rmse_list_xgb)\n",
    "        average_r2_xgb = np.mean(r2_list_xgb)\n",
    "        average_mse_xgb = np.mean(mse_kf_xgb)\n",
    "        \n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_xgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_xgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_xgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_xgb}\")\n",
    "\n",
    "# Feature importance for XGBoost\n",
    "importance = xgb_model_kf.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_xgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_xgb = feature_importance_df_xgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_xgb - average_r2_xgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "print(\"\\nXGBoost Feature Importances:\")\n",
    "print(feature_importance_df_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3235d76b",
   "metadata": {},
   "source": [
    "# LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7489ff2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8262d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46647cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "\n",
    "''''\n",
    "LightGBM Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)         852\n",
    "14                    temp_roll_7         431\n",
    "8                      temp_lag_1         384\n",
    "15                     atm_roll_7         384\n",
    "6               day_of_year_sin_3         324\n",
    "4               day_of_year_sin_2         309\n",
    "9                       atm_lag_1         302\n",
    "1            atmospheric_pressure         291\n",
    "7               day_of_year_cos_3         277\n",
    "11                      atm_lag_2         261\n",
    "3               day_of_year_cos_1         260\n",
    "2               day_of_year_sin_1         234\n",
    "5               day_of_year_cos_2         234\n",
    "12                     temp_lag_3         192\n",
    "13                      atm_lag_3         172\n",
    "\n",
    "\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "lgb_model = {}\n",
    "y_preds_lgb = {}\n",
    "rmses_lgb = {}\n",
    "r2s_lgb = {}\n",
    "\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "        # n_estimators=800,\n",
    "        # learning_rate=0.01,\n",
    "        # max_depth=8,\n",
    "        # subsample=0.8,\n",
    "        # colsample_bytree=0.8,\n",
    "        # random_state=42,\n",
    "        # verbosity=-1\n",
    "\n",
    "        n_estimators=300,        # Reduced to prevent memorization as UV r 4k dataset\n",
    "        learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "        max_depth=6,             # Shallow trees are better for 4k rows\n",
    "        num_leaves=20,           # Controls complexity\n",
    "        min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "        subsample=0.7,           # More aggressive sampling for better generalization\n",
    "        colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "        random_state=42,\n",
    "        verbosity=-1\n",
    "                              # Clean console\n",
    "    )\n",
    "\n",
    "lgb_model.fit(X_train, y_train)\n",
    "y_pred = lgb_model.predict(X_test)\n",
    "\n",
    "mse_lgb = mean_squared_error(y_test, y_pred)\n",
    "rmse_lgb = np.sqrt(mse_lgb)\n",
    "r2_lgb = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"\\nLightGBM Results for dewpont:\")\n",
    "print(f'Mean Squared Error: {mse_lgb:.4f}')\n",
    "print(f'RMSE: {rmse_lgb:.4f}')\n",
    "print(f'RÂ² Score: {r2_lgb:.4f}')\n",
    "\n",
    "\n",
    "    # --- 6) 5-fold CV R^2 ---\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "rmse_list_lgb = []\n",
    "r2_list_lgb = []\n",
    "mse_list_lgb = []\n",
    "\n",
    "\n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        lgb_model_kf = lgb.LGBMRegressor(\n",
    "            \n",
    "            n_estimators=300,        # Reduced to prevent memorization\n",
    "            learning_rate=0.03,      # Slightly faster learning for fewer trees\n",
    "            max_depth=6,             # Shallow trees are better for 4k rows\n",
    "            num_leaves=20,           # Controls complexity\n",
    "            min_child_samples=40,    # Ensures each \"leaf\" represents enough data\n",
    "            subsample=0.7,           # More aggressive sampling for better generalization\n",
    "            colsample_bytree=0.7,    # Uses 70% of features per tree\n",
    "            random_state=42,\n",
    "            verbosity=-1             # Clean console\n",
    "        )\n",
    "\n",
    "        lgb_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_lgb = lgb_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_lgb = mean_squared_error(y_test_kf, y_pred_kf_lgb)\n",
    "        rmse_kf_lgb = np.sqrt(mse_kf_lgb)\n",
    "        r2_kf_lgb = r2_score(y_test_kf, y_pred_kf_lgb)\n",
    "        \n",
    "        mse_list_lgb.append(mse_kf_lgb)\n",
    "        rmse_list_lgb.append(rmse_kf_lgb)\n",
    "        r2_list_lgb.append(r2_kf_lgb)\n",
    "\n",
    "        average_rmse_lgb = np.mean(rmse_list_lgb)\n",
    "        average_r2_lgb = np.mean(r2_list_lgb)\n",
    "        average_mse_lgb = np.mean(mse_kf_lgb)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_lgb:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_lgb:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_lgb}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lgb}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = lgb_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_lgb = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_lgb = feature_importance_df_lgb.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_lgb - average_r2_lgb)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLightGBM Feature Importances:\")\n",
    "print(feature_importance_df_lgb)\n",
    "\n",
    "##### year wise analysis #####\n",
    "# 1980 to 2024 -> ~ 0.69 (44 years)\n",
    "# 2014 to 2024 -> ~ 0.82 (10 years)\n",
    "# 2017 to 2024 -> ~ 0.042 (7 years) **** yey ðŸ˜‚ðŸ˜‚ðŸ˜‚\n",
    "# 2018 to 2024 -> ~ 0.78 (6 years)\n",
    "# 2019 to 2024 -> ~ 1.15 (5 yrs)\n",
    "# 2021 to 2024 -> ~ 2.33 (3 yrs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b14ec0",
   "metadata": {},
   "source": [
    "# CatBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7da7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca49131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fdc53a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from catboost import CatBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "\n",
    "\n",
    "''''\n",
    "CatBoost Feature Importances:\n",
    "                          Feature  Importance\n",
    "0   minimum_temperature(degree C)   43.960021\n",
    "8                      temp_lag_1   10.761866\n",
    "6               day_of_year_sin_3    7.732805\n",
    "14                    temp_roll_7    7.551957\n",
    "3               day_of_year_cos_1    4.006128\n",
    "2               day_of_year_sin_1    3.907999\n",
    "10                     temp_lag_2    3.767410\n",
    "4               day_of_year_sin_2    3.340417\n",
    "9                       atm_lag_1    2.673542\n",
    "12                     temp_lag_3    2.317698\n",
    "5               day_of_year_cos_2    2.278235\n",
    "1            atmospheric_pressure    1.917982\n",
    "15                     atm_roll_7    1.841164\n",
    "7               day_of_year_cos_3    1.651114\n",
    "13                      atm_lag_3    1.179426\n",
    "11                      atm_lag_2    1.112237\n",
    "'''\n",
    "\n",
    "targets = y\n",
    "cat_model = {}\n",
    "y_preds_cat = {}\n",
    "rmses_cat = {}\n",
    "r2s_cat = {}\n",
    "\n",
    "# loss_function='RMSE' is standard for regression\n",
    "cat_model = CatBoostRegressor(\n",
    "        iterations=800,\n",
    "        learning_rate=0.03,\n",
    "        depth=6,\n",
    "        l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "        bootstrap_type='Bayesian',\n",
    "        bagging_temperature=1,\n",
    "        random_strength=1,\n",
    "        loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "        random_seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "cat_model.fit(X_train, y_train)\n",
    "y_pred = cat_model.predict(X_test)\n",
    "\n",
    "mse_cat = mean_squared_error(y_test, y_pred)\n",
    "rmse_cat = np.sqrt(mse_cat)\n",
    "r2_cat = r2_score(y_test, y_pred)\n",
    "    \n",
    "print(f\"\\nLightGBM Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_cat:.4f}')\n",
    "print(f'RMSE: {rmse_cat:.4f}')\n",
    "print(f'RÂ² Score: {r2_cat:.4f}')\n",
    "\n",
    "\n",
    "    # 3. 5-Fold Cross-Validation\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "rmse_list_cat = []\n",
    "r2_list_cat = []\n",
    "mse_list_cat = []\n",
    "    \n",
    "for train_index,test_index in tscv.split(X):\n",
    "        X_train_kf, X_test_kf = X.iloc[train_index], X.iloc[test_index]\n",
    "        y_train_kf = y.iloc[train_index]\n",
    "        y_test_kf = y.iloc[test_index]\n",
    "\n",
    "        cat_model_kf = CatBoostRegressor(\n",
    "            iterations=800,\n",
    "            learning_rate=0.03,\n",
    "            depth=6,\n",
    "            l2_leaf_reg=5,           # Regularization is your friend with 4k rows\n",
    "            bootstrap_type='Bayesian',\n",
    "            bagging_temperature=1,\n",
    "            random_strength=1,\n",
    "            loss_function='RMSE',    # Or 'Huber' if you want it to match your LSTM\n",
    "            random_seed=42,\n",
    "            verbose=0\n",
    "        )\n",
    "\n",
    "        cat_model_kf.fit(X_train_kf, y_train_kf)\n",
    "        y_pred_kf_cat = cat_model_kf.predict(X_test_kf)\n",
    "\n",
    "        mse_kf_cat = mean_squared_error(y_test_kf, y_pred_kf_cat)\n",
    "        rmse_kf_cat = np.sqrt(mse_kf_cat)\n",
    "        r2_kf_cat = r2_score(y_test_kf, y_pred_kf_cat)\n",
    "\n",
    "        mse_list_cat.append(mse_kf_cat)\n",
    "        rmse_list_cat.append(rmse_kf_cat)\n",
    "        r2_list_cat.append(r2_kf_cat)\n",
    "\n",
    "        average_rmse_cat = np.mean(rmse_list_cat)\n",
    "        average_r2_cat = np.mean(r2_list_cat)\n",
    "        average_mse_cat = np.mean(mse_list_cat)\n",
    "\n",
    "\n",
    "print (\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_cat:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_cat:.4f}\")\n",
    "print(f\"Avarage MSE: {average_mse_cat}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_cat}\")\n",
    "\n",
    "# Feature importance for LightGBM\n",
    "importance = cat_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "feature_importance_df_cat = pd.DataFrame({'Feature': feature_names, 'Importance': importance})\n",
    "feature_importance_df_cat = feature_importance_df_cat.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "diff = (r2_cat - average_r2_cat)*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nCatBoost Feature Importances:\")\n",
    "print(feature_importance_df_cat)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4a711e",
   "metadata": {},
   "source": [
    "# GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7603234",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "724d8de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cccdf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import GRU, Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Create an instance with specific parameters\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=15,          # Wait 15 epochs for improvement before stopping\n",
    "    restore_best_weights=True  # Very important: keeps the best version of your model\n",
    ")\n",
    "\n",
    "# 1. Scale the data\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for GRU: (samples, time_steps, features)\n",
    "# Here we use time_steps=1. If you want sequences, you'd need a sliding window function.\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split data (matching your non-shuffle 80/20 split)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "\n",
    "def build_gru(input_shape):\n",
    "    model = Sequential([\n",
    "        GRU(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train initial model\n",
    "gru_model = build_gru((X_train.shape[1], X_train.shape[2]))\n",
    "gru_model.fit(\n",
    "    X_train, \n",
    "    y_train, \n",
    "    epochs=100, \n",
    "    batch_size=64,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stopping], \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Predict and Inverse Scale\n",
    "y_pred_scaled = gru_model.predict(X_test)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_unscaled = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_gru = mean_squared_error(y_test_unscaled, y_pred)\n",
    "rmse_gru = np.sqrt(mse_gru)\n",
    "r2_gru = r2_score(y_test_unscaled, y_pred)\n",
    "\n",
    "print(f\"\\nGRU Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_gru:.4f}')\n",
    "print(f'RMSE: {rmse_gru:.4f}')\n",
    "print(f'RÂ² Score: {r2_gru:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_gru = []\n",
    "r2_list_gru = []\n",
    "mse_list_gru = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Rebuild/Reset model for each fold\n",
    "    gru_kf = build_gru((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    gru_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = gru_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_gru.append(np.sqrt(mse_kf))\n",
    "    mse_list_gru.append(mse_kf)\n",
    "    r2_list_gru.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_gru = np.mean(r2_list_gru)\n",
    "average_mse_gru = np.mean(mse_list_gru)\n",
    "average_rmse_gru = np.mean(rmse_list_gru)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from CV: {average_rmse_gru:.4f}\")\n",
    "print(f\"Average RÂ² from CV: {average_r2_gru:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_gru:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_gru}\")\n",
    "\n",
    "diff = (r2_gru - np.mean(r2_list_gru))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40631911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Permutation Importance Implementation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Calculates importance by measuring how much the MSE increases \n",
    "    when a single feature is randomly shuffled.\n",
    "    \"\"\"\n",
    "    # Baseline prediction\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    baseline_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                     scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importances = []\n",
    "    \n",
    "    for i in range(X_val.shape[2]):  # Iterate through each feature\n",
    "        save = X_val[:, :, i].copy()\n",
    "        \n",
    "        # Shuffle the current feature across all samples\n",
    "        np.random.shuffle(X_val[:, :, i])\n",
    "        \n",
    "        # Predict with shuffled feature\n",
    "        shuffled_preds = model.predict(X_val, verbose=0)\n",
    "        shuffled_mse = mean_squared_error(scaler_y.inverse_transform(y_val), \n",
    "                                         scaler_y.inverse_transform(shuffled_preds))\n",
    "        \n",
    "        # Importance is the increase in error\n",
    "        importances.append(max(0, shuffled_mse - baseline_mse))\n",
    "        \n",
    "        # Restore the original feature values\n",
    "        X_val[:, :, i] = save\n",
    "\n",
    "    # Create DataFrame\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "    return importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# --- Execute ---\n",
    "# Note: Use your X_test and y_test from the previous step\n",
    "feature_importance_gru = calculate_permutation_importance(\n",
    "    gru_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "\n",
    "print(\"\\nGRU Permutation Feature Importances:\")\n",
    "print(feature_importance_gru)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895c87fc",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60633d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae88ace9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7899dd10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "\n",
    "X_scaled = scaler_X.fit_transform(X)\n",
    "y_scaled = scaler_y.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# 2. Reshape for LSTM: [samples, time_steps, features]\n",
    "X_reshaped = X_scaled.reshape((X_scaled.shape[0], 1, X_scaled.shape[1]))\n",
    "\n",
    "# Split matching your CatBoost logic (shuffle=False)\n",
    "split_idx = int(len(X_reshaped) * 0.8)\n",
    "X_train, X_test = X_reshaped[:split_idx], X_reshaped[split_idx:]\n",
    "y_train, y_test = y_scaled[:split_idx], y_scaled[split_idx:]\n",
    "\n",
    "def build_lstm(input_shape):\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=input_shape, return_sequences=False, recurrent_dropout=0.1),\n",
    "        Dropout(0.2),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.0005), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "lstm_model = build_lstm((X_train.shape[1], X_train.shape[2]))\n",
    "lstm_model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled = lstm_model.predict(X_test)\n",
    "y_pred_lstm = scaler_y.inverse_transform(y_pred_scaled)\n",
    "y_test_actual = scaler_y.inverse_transform(y_test)\n",
    "\n",
    "mse_lstm = mean_squared_error(y_test_actual, y_pred_lstm)\n",
    "rmse_lstm = np.sqrt(mse_lstm)\n",
    "r2_lstm = r2_score(y_test_actual, y_pred_lstm)\n",
    "\n",
    "print(f\"\\nLSTM Results for dewpoint :\")\n",
    "print(f'Mean Squared Error: {mse_lstm:.4f}')    \n",
    "print(f'RMSE: {rmse_lstm:.4f}')\n",
    "print(f'RÂ² Score: {r2_lstm:.4f}')\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_lstm = []\n",
    "r2_list_lstm = []\n",
    "mse_list_lstm = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_reshaped):\n",
    "    X_train_kf, X_test_kf = X_reshaped[train_index], X_reshaped[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled[train_index], y_scaled[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    lstm_kf = build_lstm((X_train_kf.shape[1], X_train_kf.shape[2]))\n",
    "    lstm_kf.fit(X_train_kf, y_train_kf, epochs=30, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = lstm_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_lstm.append(np.sqrt(mse_kf))\n",
    "    mse_list_lstm.append(mse_kf)\n",
    "    r2_list_lstm.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "\n",
    "average_r2_lstm = np.mean(r2_list_lstm)\n",
    "average_mse_lstm = np.mean(mse_list_lstm)\n",
    "average_rmse_lstm = np.mean(rmse_list_lstm)\n",
    " \n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from LSTM CV: {average_rmse_lstm:.4f}\")\n",
    "print(f\"Average RÂ² from LSTM CV: { average_r2_lstm:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_lstm:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_lstm}\")\n",
    "\n",
    "def calculate_lstm_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    \"\"\"\n",
    "    Computes permutation importance for a trained LSTM model.\n",
    "    \"\"\"\n",
    "    # 1. Get baseline score (Inverse scale to get real-world MSE)\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    \n",
    "    # Iterate through each feature index\n",
    "    for i in range(X_val.shape[2]):\n",
    "        # Create a copy to avoid permanent shuffling\n",
    "        X_permuted = X_val.copy()\n",
    "        \n",
    "        # 2. Shuffle the specific feature across all samples\n",
    "        # Shuffling happens across the 'samples' dimension for the i-th feature\n",
    "        np.random.shuffle(X_permuted[:, :, i])\n",
    "        \n",
    "        # 3. Predict with the permuted feature\n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        # 4. Importance = Increase in Error (shuffled error - baseline error)\n",
    "        importance = max(0, permuted_mse - baseline_mse)\n",
    "        importance_results.append(importance)\n",
    "\n",
    "    # Organize into a DataFrame\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names, \n",
    "        'Importance': importance_results\n",
    "    }).sort_values(by='Importance', ascending=False)\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# --- Execution ---\n",
    "# Using the X_test and y_test from your LSTM training\n",
    "lstm_importance_df = calculate_lstm_permutation_importance(\n",
    "    lstm_model, \n",
    "    X_test, \n",
    "    y_test, \n",
    "    scaler_y, \n",
    "    FEATURES\n",
    ")\n",
    "\n",
    "diff = (r2_lstm - np.mean(r2_list_lstm))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nLSTM Permutation Feature Importances:\")\n",
    "print(lstm_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd35fd5c",
   "metadata": {},
   "source": [
    "# ANN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1f716c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c62ce38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483cd48",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# 1. Scale Features and Target\n",
    "scaler_X_ann = StandardScaler()\n",
    "scaler_y_ann = StandardScaler()\n",
    "\n",
    "X_scaled_ann = scaler_X_ann.fit_transform(X)\n",
    "y_scaled_ann = scaler_y_ann.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Split matching your CatBoost logic (80/20, shuffle=False)\n",
    "split_idx = int(len(X_scaled_ann) * 0.8)\n",
    "X_train_ann, X_test_ann = X_scaled_ann[:split_idx], X_scaled_ann[split_idx:]\n",
    "y_train_ann, y_test_ann = y_scaled_ann[:split_idx], y_scaled_ann[split_idx:]\n",
    "\n",
    "# build model \n",
    "\n",
    "def build_ann(input_dim):\n",
    "    model = Sequential([\n",
    "        Dense(128, activation='relu', input_dim=input_dim),\n",
    "        Dropout(0.2),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(1) # Linear output for regression\n",
    "    ])\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Initial Training\n",
    "ann_model = build_ann(X_train_ann.shape[1])\n",
    "ann_model.fit(X_train_ann, y_train_ann, epochs=100, batch_size=32, verbose=0)\n",
    "\n",
    "# Predictions & Inverse Scaling\n",
    "y_pred_scaled_ann = ann_model.predict(X_test_ann)\n",
    "y_pred_ann = scaler_y_ann.inverse_transform(y_pred_scaled_ann)\n",
    "y_test_actual = scaler_y_ann.inverse_transform(y_test_ann)\n",
    "\n",
    "mse_ann = mean_squared_error(y_test_actual, y_pred_ann)\n",
    "rmse_ann = np.sqrt(mse_ann)\n",
    "r2_ann = r2_score(y_test_actual, y_pred_ann)\n",
    "\n",
    "print(f\"\\nANN Results for dew :\")\n",
    "print(f'Mean Squared Error: {mse_ann:.4f}')\n",
    "print(f'RMSE: {rmse_ann:.4f}')\n",
    "print(f'RÂ² Score: {r2_ann:.4f}')\n",
    "\n",
    "# CV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_list_ann = []\n",
    "r2_list_ann = []\n",
    "mse_list_ann = []\n",
    "\n",
    "for train_index, test_index in tscv.split(X_scaled_ann):\n",
    "    X_train_kf, X_test_kf = X_scaled_ann[train_index], X_scaled_ann[test_index]\n",
    "    y_train_kf, y_test_kf = y_scaled_ann[train_index], y_scaled_ann[test_index]\n",
    "\n",
    "    # Re-instantiate model for each fold\n",
    "    ann_kf = build_ann(X_train_kf.shape[1])\n",
    "    ann_kf.fit(X_train_kf, y_train_kf, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "    # Predict and Inverse\n",
    "    y_pred_kf_scaled = ann_kf.predict(X_test_kf)\n",
    "    y_pred_kf = scaler_y_ann.inverse_transform(y_pred_kf_scaled)\n",
    "    y_test_kf_unscaled = scaler_y_ann.inverse_transform(y_test_kf)\n",
    "\n",
    "    mse_kf = mean_squared_error(y_test_kf_unscaled, y_pred_kf)\n",
    "    rmse_list_ann.append(np.sqrt(mse_kf))\n",
    "    mse_list_ann.append(mse_kf)\n",
    "    r2_list_ann.append(r2_score(y_test_kf_unscaled, y_pred_kf))\n",
    "\n",
    "average_r2_ann = np.mean(r2_list_ann)\n",
    "average_mse_ann = np.mean(mse_list_ann)\n",
    "average_rmse_ann = np.mean(rmse_list_ann)\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Average RMSE from ANN CV: {average_rmse_ann:.4f}\")\n",
    "print(f\"Average RÂ² from ANN CV: {average_r2_ann:.4f}\")\n",
    "print(f\"Average MSE: {average_mse_ann:.4f}\")\n",
    "print(f\"Individual Fold RMSEs: {rmse_list_ann}\")\n",
    "\n",
    "# importance\n",
    "def calculate_ann_permutation_importance(model, X_val, y_val, scaler_y, feature_names):\n",
    "    baseline_preds = model.predict(X_val, verbose=0)\n",
    "    y_val_unscaled = scaler_y.inverse_transform(y_val)\n",
    "    baseline_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(baseline_preds))\n",
    "    \n",
    "    importance_results = []\n",
    "    for i in range(X_val.shape[1]): # Iterate through 2D features\n",
    "        X_permuted = X_val.copy()\n",
    "        np.random.shuffle(X_permuted[:, i])\n",
    "        \n",
    "        permuted_preds = model.predict(X_permuted, verbose=0)\n",
    "        permuted_mse = mean_squared_error(y_val_unscaled, scaler_y.inverse_transform(permuted_preds))\n",
    "        \n",
    "        importance_results.append(max(0, permuted_mse - baseline_mse))\n",
    "\n",
    "    return pd.DataFrame({'Feature': feature_names, 'Importance': importance_results}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "ann_importance_df = calculate_ann_permutation_importance(ann_model, X_test_ann, y_test_ann, scaler_y_ann, FEATURES)\n",
    "\n",
    "diff = (r2_ann - np.mean(r2_list_ann))*100\n",
    "print ( f'\\n R2 ~ {diff:.4f}')\n",
    "\n",
    "print(\"\\nANN Permutation Feature Importances:\")\n",
    "print(ann_importance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa81501",
   "metadata": {},
   "source": [
    "# CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a01dba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "df_chittagong = pd.read_csv('../1980-2024-dataset/chittagong_historical_weather_1980_2024.csv')\n",
    "df_chittagong = df_chittagong.drop('district', axis =1 )\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "df = df_chittagong.copy()\n",
    "if 'date' not in df.columns:\n",
    "    df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "\n",
    "\n",
    "# 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)' [collected based on the corrilation matrix]\n",
    "\n",
    "# adding lagging for 3 days\n",
    "lags = [1, 2, 3]\n",
    "lag_cols = []\n",
    "\n",
    "for lag in lags:\n",
    "    df[f'atm_lag_{lag}'] = df['atmospheric_pressure'].shift(lag)\n",
    "    df[f'hum_lag_{lag}'] = df['humidity'].shift(lag)\n",
    "    df[f'mintem_lag_{lag}'] = df['minimum_temperature(degree C)'].shift(lag)\n",
    "    # df[f'UV_lag_{lag}'] = df['UV'].shift(lag)\n",
    "    # df[f'ppt_lag_{lag}'] = df['precipitation'].shift(lag)\n",
    "    \n",
    "    lag_cols.extend([f'atm_lag_{lag}', f'hum_lag_{lag}', f'mintem_lag_{lag}'])\n",
    "\n",
    "\n",
    "# 7-day Rolling Average\n",
    "df['atm_roll_7'] = df['atmospheric_pressure'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['dew_roll_7'] = df['humidity'].transform(lambda x: x.rolling(window=7).mean())\n",
    "df['mintem_roll_7'] = df['minimum_temperature(degree C)'].transform(lambda x: x.rolling(window =7).mean())\n",
    "\n",
    "\n",
    "rolling_cols = ['atm_roll_7', 'dew_roll_7', 'mintem_roll_7']\n",
    "\n",
    "\n",
    "df = df.dropna().reset_index(drop=True)\n",
    "\n",
    "\n",
    "df['date'] = pd.to_datetime(df[['year', 'month', 'day']])\n",
    "df = df.sort_values('date').reset_index(drop=True)\n",
    "\n",
    "df['day_of_year'] = df['date'].dt.dayofyear\n",
    "\n",
    "def add_fourier_features(df,col,period,n_terms=10):\n",
    "    for n in range(1, n_terms + 1):\n",
    "        df[f'{col}_sin_{n}'] = np.sin(2 * np.pi * n * df.index / period)\n",
    "        df[f'{col}_cos_{n}'] = np.cos(2 * np.pi * n * df.index / period)\n",
    "    return df\n",
    "\n",
    "df = add_fourier_features(df, 'day_of_year', period=365, n_terms=3)\n",
    "fourier_cols = [c for c in df.columns if c.startswith('day_of_year_sin') or c.startswith('day_of_year_cos')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30f4ecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FEATURES = [\n",
    "#     'minimum_temperature(degree C)',\n",
    "#     'mintem_roll_7',\n",
    "#     'mintem_lag_1',\n",
    "#     'mintem_lag_2',\n",
    "#     'mintem_lag_3',\n",
    "#     'atm_roll_7',\n",
    "#     'atm_lag_2',\n",
    "#     'atmospheric_pressure',\n",
    "#     'day_of_year_cos_1',\n",
    "#     'atm_lag_3',\n",
    "#     'atm_lag_1',\n",
    "#     'humidity', # 93.44/94.73 *-1.28\n",
    "#     'day_of_year_sin_3', # 93.38/94.71 *-1.32\n",
    "#     'day_of_year_sin_1', # 93.20/94.49 *-1.29\n",
    "#     'dew_roll_7',\n",
    "#     'hum_lag_1', # 93.75/94.87 *-0.096 [best]\n",
    "#     # 'day_of_year_sin_2', \n",
    "# ]\n",
    "\n",
    "FEATURES = [ 'atmospheric_pressure','humidity', 'minimum_temperature(degree C)'] + fourier_cols + lag_cols + rolling_cols # (first e eita uncomment kore feature importance dekhben. then 0.02 minmum importance gula note down kore prediction korben)\n",
    "# *0.08\n",
    "\n",
    "X = df[FEATURES]\n",
    "y = df['dew_point']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319a6146",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def model_builder(hp):\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Tune CNN Filters\n",
    "    hp_filters = hp.Int('filters', min_value=32, max_value=64, step=32)\n",
    "    model.add(Conv1D(filters=hp_filters, kernel_size=3, activation='relu', input_shape=(100,1)))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    \n",
    "    # Tune Dropout\n",
    "    hp_dropout = hp.Float('dropout', min_value=0.2, max_value=0.4, step=0.1)\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Tune LSTM Units\n",
    "    hp_lstm_units = hp.Int('lstm_units', min_value=32, max_value=64, step=16)\n",
    "    model.add(LSTM(units=hp_lstm_units, activation='tanh'))\n",
    "    model.add(Dropout(hp_dropout))\n",
    "\n",
    "    # Final Dense Layers\n",
    "    model.add(Dense(hp.Int('dense_units', 16, 64, 16), activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    # Tune Learning Rate\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=hp_learning_rate), loss='mse')\n",
    "    return model\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Example: Using the last 30 days to predict tomorrow\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# X_3D shape will be (Total_Days, 30, 1)\n",
    "X_3D, y_target = create_sequences(df['dew_point'].values)\n",
    "X_3D = X_3D.reshape((X_3D.shape[0], X_3D.shape[1], 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb17d1da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv1D, MaxPooling1D, LSTM, Dense, Dropout\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# 1. Prepare 3D Data (Samples, Time Steps, Features)\n",
    "def create_sequences(data, window_size=30):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size):\n",
    "        X.append(data[i:(i + window_size)])\n",
    "        y.append(data[i + window_size]) \n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def build_hybrid(input_shape):\n",
    "    model = Sequential([\n",
    "        # 1. CNN Stage: Extracts spatial/local patterns from the window\n",
    "        # Reducing filters to 32 is often better for ~4k rows to prevent noise capture\n",
    "        Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(100,1)),\n",
    "        BatchNormalization(), # Stabilizes learning and speeds up convergence\n",
    "        MaxPooling1D(pool_size=2),\n",
    "        Dropout(0.2), # Standard regularization\n",
    "\n",
    "        # 2. LSTM Stage: Learns temporal dependencies\n",
    "        # tanh is the standard and most stable activation for LSTM\n",
    "        LSTM(64, activation='tanh', return_sequences=False), \n",
    "        Dropout(0.2),\n",
    "\n",
    "        # 3. Fully Connected Stage\n",
    "        Dense(32, activation='relu'),\n",
    "        Dropout(0.1),\n",
    "        Dense(1) # Output for UV prediction\n",
    "    ])\n",
    "    \n",
    "    # Using a slightly lower learning rate (0.0005) helps with smaller datasets\n",
    "    optimizer = Adam(learning_rate=0.0005)\n",
    "    model.compile(optimizer=optimizer, loss='mse', metrics=['mae'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Setup Data\n",
    "window = 30\n",
    "data_values = df['dew_point'].values.reshape(-1, 1)\n",
    "X, y = create_sequences(data_values, window)\n",
    "\n",
    "# --- BASE PERFORMANCE ---\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)\n",
    "model = build_hybrid((window, 1))\n",
    "model.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "base_mse = mean_squared_error(y_test, y_pred)\n",
    "base_rmse = np.sqrt(base_mse)\n",
    "base_r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Base Results: R2: {base_r2:.4f}, MSE: {base_mse:.4f}, RMSE: {base_rmse:.4f}\")\n",
    "\n",
    "# --- 5-FOLD CROSS VALIDATION ---\n",
    "kf = KFold(n_splits=5, shuffle=False)\n",
    "cv_r2, cv_mse, cv_rmse = [], [], []\n",
    "\n",
    "for train_idx, test_idx in kf.split(X):\n",
    "    m = build_hybrid((window, 1))\n",
    "    m.fit(X[train_idx], y[train_idx], epochs=10, batch_size=32, verbose=0)\n",
    "    \n",
    "    p = m.predict(X[test_idx])\n",
    "    cv_r2.append(r2_score(y[test_idx], p))\n",
    "    cv_mse.append(mean_squared_error(y[test_idx], p))\n",
    "    cv_rmse.append(np.sqrt(cv_mse[-1]))\n",
    "\n",
    "\n",
    "cnn_lstm_r2_cv = np.mean(cv_r2)\n",
    "cnn_lstm_mse_cv = np.mean(cv_mse)\n",
    "cnn_lstm_rmse_cv = np.mean(cv_rmse)\n",
    "\n",
    "print(f\"5-Fold CV Average: R2: {cnn_lstm_r2_cv:.4f}, MSE: {cnn_lstm_mse_cv:.4f}, RMSE: {cnn_lstm_rmse_cv:.4f}\")\n",
    "print ()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f60abe20",
   "metadata": {},
   "source": [
    "# Model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a58428e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model performance chart \n",
    "MODEL_NAMES = [\"Random Forest\", \"RF-LSTM hybrid\", \"XGBoost\", \"LightGBM\", \"CatBoost\", \"GRU\", \"LSTM\", \"ANN\", \"CNN-LSTM hybrid\"]\n",
    "\n",
    "R_SQUARED_VALUES = [r2_rf, rf_lstm_r2, r2_xgb, r2_lgb, r2_cat, r2_gru, r2_lstm, r2_ann, base_r2 ]\n",
    "R2CV = [average_r2_rf, rf_lstm_r2_cv, average_r2_xgb, average_r2_lgb, average_r2_cat, average_r2_gru, average_r2_lstm, average_r2_ann, cnn_lstm_r2_cv ]\n",
    "\n",
    "R2_DIFF = [\n",
    "    (r2_rf - average_r2_rf), \n",
    "    (rf_lstm_r2 - rf_lstm_r2_cv), \n",
    "    (r2_xgb - average_r2_xgb), \n",
    "    (r2_lgb - average_r2_lgb), \n",
    "    (r2_cat - average_r2_cat), \n",
    "    (r2_gru - average_r2_gru), \n",
    "    (r2_lstm - average_r2_lstm), \n",
    "    (r2_ann - average_r2_ann), \n",
    "    (base_r2 - cnn_lstm_r2_cv)\n",
    "]\n",
    "\n",
    "MSE_VALUES = [mse_rf, rf_lstm_mse, mse_xgb, mse_lgb, mse_cat, mse_gru, mse_lstm, mse_ann, base_mse ]\n",
    "MSE_CV = [average_mse_rf, rf_lstm_mse_cv, average_mse_xgb, average_mse_lgb, average_mse_cat, average_mse_gru, average_mse_lstm, average_mse_ann, cnn_lstm_mse_cv ]\n",
    "\n",
    "RMSE_VALUES = [rmse_rf, rf_lstm_rmse, rmse_xgb, rmse_lgb, rmse_cat, rmse_gru, rmse_lstm, rmse_ann , base_rmse ]\n",
    "RMSE_CV = [average_rmse_rf, rf_lstm_rmse_cv, average_rmse_xgb, average_rmse_lgb, average_rmse_cat, average_rmse_gru, average_rmse_lstm, average_rmse_ann, cnn_lstm_rmse_cv]\n",
    "\n",
    "data = {\n",
    "    \"Model\": MODEL_NAMES,\n",
    "    \"R^2\": R_SQUARED_VALUES,\n",
    "    \"CVR2\": R2CV,\n",
    "    \"R2 DIFF\": R2_DIFF,\n",
    "    \"MSE\": MSE_VALUES,\n",
    "    \"MSE CV\": MSE_CV,\n",
    "    \"RMSE\": RMSE_VALUES,\n",
    "    \"RMSE CV\": RMSE_CV\n",
    "}\n",
    "\n",
    "df_performance = pd.DataFrame(data).sort_values(by=[\"R2 DIFF\",\"R^2\"], ascending= [True, True])\n",
    "\n",
    "print (df_performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8433b50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Define weights for your 'Best Logical Model' criteria\n",
    "# We want high R^2, low RMSE, and low R2 DIFF (stability)\n",
    "weights = {\n",
    "    'R^2': 0.4,       # Predictive power\n",
    "    'MSE': 0.3,\n",
    "    'RMSE': 0.3,     # Magnitude of error\n",
    "    'R2 DIFF': 0.3    # Robustness/Generalization\n",
    "}\n",
    "\n",
    "# 2. Create a Ranking Score (Lower is better)\n",
    "# .rank(ascending=False) means highest value gets rank 1\n",
    "# .rank(ascending=True) means lowest value gets rank 1\n",
    "df_performance['Score'] = (\n",
    "    df_performance['R^2'].rank(ascending=False) * weights['R^2'] +\n",
    "    df_performance['MSE'].rank(ascending=True) * weights['MSE'] +\n",
    "    df_performance['RMSE'].rank(ascending=True) * weights['RMSE']  +\n",
    "    df_performance['R2 DIFF'].rank(ascending=True) * weights['R2 DIFF']\n",
    ")\n",
    "\n",
    "# 3. Extract the winner\n",
    "best_logical_model = df_performance.loc[df_performance['Score'].idxmin()]\n",
    "\n",
    "print(f\"The Best Logical Model is: {best_logical_model['Model']}\")\n",
    "print(f\"--- Reason: Balanced score across R^2 ({best_logical_model['R^2']:.4f}) \"\n",
    "      f\"and Stability (DIFF: {best_logical_model['R2 DIFF']:.4f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ea13b4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
